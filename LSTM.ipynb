{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf  # Version 1.0.0 (some previous versions are used in past commits)\n",
    "from sklearn import metrics\n",
    "import random\n",
    "from random import randint\n",
    "import time\n",
    "import os\n",
    "from config import FLAGS\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful Constants\n",
    "\n",
    "# Output classes to learn how to classify\n",
    "LABELS = FLAGS.LABELS\n",
    "DATASET_PATH_train = FLAGS.train_path\n",
    "DATASET_PATH_test = FLAGS.test_path\n",
    "\n",
    "X_train_path = DATASET_PATH_train + \"X_train.txt\"\n",
    "X_test_path = DATASET_PATH_test + \"X_test.txt\"\n",
    "\n",
    "y_train_path = DATASET_PATH_train + \"Y_train.txt\"\n",
    "y_test_path = DATASET_PATH_test + \"Y_test.txt\"\n",
    "\n",
    "n_steps = FLAGS.n_frames # 32 timesteps per series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the networks inputs\n",
    "\n",
    "def load_X(X_path):\n",
    "    file = open(X_path, 'r')\n",
    "    X_ =np.array(\n",
    "        [elem for elem in [\n",
    "            row.replace('\\n', '').split(' ') for row in file\n",
    "        ]], \n",
    "        dtype=np.float32\n",
    "    )\n",
    "\n",
    "    file.close()\n",
    "    blocks = int(len(X_) / n_steps)\n",
    "    \n",
    "\n",
    "    X_ = np.array(np.split(X_,blocks))\n",
    "    return X_ \n",
    "\n",
    "# Load the networks outputs\n",
    "\n",
    "def load_y(y_path):\n",
    "    file = open(y_path, 'r')\n",
    "    y_ = np.array(\n",
    "        [elem for elem in [\n",
    "            row.replace('  ', ' ').strip().split(' ') for row in file\n",
    "        ]], \n",
    "        dtype=np.int32\n",
    "    )\n",
    "    file.close()\n",
    "    \n",
    "    # for 0-based indexing \n",
    "    return y_ - 1\n",
    "\n",
    "X_train = load_X(X_train_path)\n",
    "X_test = load_X(X_test_path)\n",
    "#print X_test\n",
    "\n",
    "y_train = load_y(y_train_path)\n",
    "y_test = load_y(y_test_path)\n",
    "# proof that it actually works for the skeptical: replace labelled classes with random classes to train on\n",
    "#for i in range(len(y_train)):\n",
    "#    y_train[i] = randint(0, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(X shape, y shape, every X's mean, every X's standard deviation)\n",
      "((10057, 1, 36), (3499, 1), 108.98817, 64.57691)\n",
      "\n",
      "The dataset has not been preprocessed, is not normalised etc\n"
     ]
    }
   ],
   "source": [
    "# Input Data \n",
    "\n",
    "training_data_count = len(X_train)  # 4519 training series (with 50% overlap between each serie)\n",
    "test_data_count = len(X_test)  # 1197 test series\n",
    "n_input = FLAGS.n_features  # num input parameters per timestep\n",
    "\n",
    "n_hidden = FLAGS.n_hiddens # Hidden layer num of features\n",
    "n_classes = FLAGS.n_outputs \n",
    "\n",
    "#updated for learning-rate decay\n",
    "# calculated as: decayed_learning_rate = learning_rate * decay_rate ^ (global_step / decay_steps)\n",
    "decaying_learning_rate = FLAGS.decaying_learning_rate\n",
    "learning_rate = FLAGS.lr #used if decaying_learning_rate set to False\n",
    "init_learning_rate = FLAGS.init_lr\n",
    "decay_rate = FLAGS.decay_rate #the base of the exponential in the decay\n",
    "decay_steps = FLAGS.decay_steps #used in decay every 60000 steps with a base of 0.96\n",
    "\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "lambda_loss_amount = FLAGS.lambda_loss_amount\n",
    "\n",
    "training_iters = training_data_count*100 #FLAGS.epochs   # Loop 300 times on the dataset, ie 300 epochs\n",
    "batch_size = FLAGS.batch_size\n",
    "display_iter = FLAGS.display_iter  # To show test set accuracy during training\n",
    "\n",
    "print(\"(X shape, y shape, every X's mean, every X's standard deviation)\")\n",
    "print(X_train.shape, y_test.shape, np.mean(X_test), np.std(X_test))\n",
    "print(\"\\nThe dataset has not been preprocessed, is not normalised etc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([128.052,  29.805, 128.052,  52.987, 111.494,  56.299,  98.247,\n",
       "        23.182, 118.117,   0.   , 144.61 ,  52.987, 157.857,  29.805,\n",
       "       141.299,   3.312, 114.805, 135.779, 111.494, 195.39 , 108.182,\n",
       "       251.688, 137.987, 135.779, 141.299, 198.701, 141.299, 255.   ,\n",
       "       124.74 ,  23.182, 131.364,  26.494, 118.117,  29.805, 137.987,\n",
       "        29.805], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(X shape, y shape, every X's mean, every X's standard deviation)\n",
      "((10057, 1, 36), (3499, 1))\n",
      "\n",
      "The dataset has not been preprocessed, is not normalised etc\n"
     ]
    }
   ],
   "source": [
    "# Input Data \n",
    "\n",
    "training_data_count = len(X_train)  # 4519 training series (with 50% overlap between each serie)\n",
    "test_data_count = len(X_test)  # 1197 test series\n",
    "n_input = FLAGS.n_features  # num input parameters per timestep\n",
    "\n",
    "n_hidden = FLAGS.n_hiddens # Hidden layer num of features\n",
    "n_classes = FLAGS.n_outputs \n",
    "\n",
    "#updated for learning-rate decay\n",
    "# calculated as: decayed_learning_rate = learning_rate * decay_rate ^ (global_step / decay_steps)\n",
    "decaying_learning_rate = FLAGS.decaying_learning_rate\n",
    "learning_rate = FLAGS.lr #used if decaying_learning_rate set to False\n",
    "init_learning_rate = FLAGS.init_lr\n",
    "decay_rate = FLAGS.decay_rate #the base of the exponential in the decay\n",
    "decay_steps = FLAGS.decay_steps #used in decay every 60000 steps with a base of 0.96\n",
    "\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "lambda_loss_amount = FLAGS.lambda_loss_amount\n",
    "\n",
    "training_iters = training_data_count*100 #FLAGS.epochs   # Loop 300 times on the dataset, ie 300 epochs\n",
    "batch_size = FLAGS.batch_size\n",
    "display_iter = FLAGS.display_iter  # To show test set accuracy during training\n",
    "\n",
    "print(\"(X shape, y shape, every X's mean, every X's standard deviation)\")\n",
    "print(X_train.shape, y_test.shape)#, np.mean(X_test), np.std(X_test))\n",
    "print(\"\\nThe dataset has not been preprocessed, is not normalised etc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_RNN(_X, _weights, _biases):\n",
    "    # model architecture based on \"guillaume-chevalier\" and \"aymericdamien\" under the MIT license.\n",
    "\n",
    "    _X = tf.transpose(_X, [1, 0, 2])  # permute n_steps and batch_size\n",
    "    _X = tf.reshape(_X, [-1, n_input])   \n",
    "    # Rectifies Linear Unit activation function used\n",
    "    _X = tf.nn.relu(tf.matmul(_X, _weights['hidden']) + _biases['hidden'])\n",
    "    # Split data because rnn cell needs a list of inputs for the RNN inner loop\n",
    "    _X = tf.split(_X, n_steps, 0) \n",
    "\n",
    "    # Define two stacked LSTM cells (two recurrent layers deep) with tensorflow\n",
    "    lstm_cell_1 = tf.contrib.rnn.BasicLSTMCell(n_hidden, forget_bias=1.0, state_is_tuple=True)\n",
    "    lstm_cell_2 = tf.contrib.rnn.BasicLSTMCell(n_hidden, forget_bias=1.0, state_is_tuple=True)\n",
    "    lstm_cells = tf.contrib.rnn.MultiRNNCell([lstm_cell_1, lstm_cell_2], state_is_tuple=True)\n",
    "    outputs, states = tf.contrib.rnn.static_rnn(lstm_cells, _X, dtype=tf.float32)\n",
    "\n",
    "    # A single output is produced, in style of \"many to one\" classifier, refer to http://karpathy.github.io/2015/05/21/rnn-effectiveness/ for details\n",
    "    lstm_last_output = outputs[-1]\n",
    "    \n",
    "    # Linear activation\n",
    "    return tf.matmul(lstm_last_output, _weights['out']) + _biases['out']\n",
    "\n",
    "\n",
    "def extract_batch_size(_train, _labels, _unsampled, batch_size):\n",
    "    # Fetch a \"batch_size\" amount of data and labels from \"(X|y)_train\" data. \n",
    "    # Elements of each batch are chosen randomly, without replacement, from X_train with corresponding label from Y_train\n",
    "    # unsampled_indices keeps track of sampled data ensuring non-replacement. Resets when remaining datapoints < batch_size    \n",
    "    \n",
    "    shape = list(_train.shape)\n",
    "    shape[0] = batch_size\n",
    "    batch_s = np.empty(shape)\n",
    "    batch_labels = np.empty((batch_size,1)) \n",
    "\n",
    "    for i in list(range(batch_size)):\n",
    "        # Loop index\n",
    "        # index = random sample from _unsampled (indices)\n",
    "        index = random.choice(_unsampled)\n",
    "#        print(_unsampled)\n",
    "#        print(index)\n",
    "#        print(_train[0][0])\n",
    "        batch_s[i] = _train[index] \n",
    "        batch_labels[i] = _labels[index]\n",
    "        _unsampled.remove(index)\n",
    "\n",
    "\n",
    "    return batch_s, batch_labels, _unsampled\n",
    "\n",
    "\n",
    "def one_hot(y_):\n",
    "    # One hot encoding of the network outputs\n",
    "    # e.g.: [[5], [0], [3]] --> [[0, 0, 0, 0, 0, 1], [1, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0]]\n",
    "    \n",
    "    y_ = y_.reshape(len(y_))\n",
    "    n_values = int(np.max(y_)) + 1\n",
    "    return np.eye(n_values)[np.array(y_, dtype=np.int32)]  # Returns FLOATS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-8-54c3d276d670>:21: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Graph input/output\n",
    "x = tf.placeholder(tf.float32, [None, n_steps, n_input])\n",
    "y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "\n",
    "# Graph weights\n",
    "weights = {\n",
    "    'hidden': tf.Variable(tf.random_normal([n_input, n_hidden])), # Hidden layer weights\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden, n_classes], mean=1.0))\n",
    "}\n",
    "biases = {\n",
    "    'hidden': tf.Variable(tf.random_normal([n_hidden])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "\n",
    "pred = LSTM_RNN(x, weights, biases)\n",
    "\n",
    "# Loss, optimizer and evaluation\n",
    "l2 = lambda_loss_amount * sum(\n",
    "    tf.nn.l2_loss(tf_var) for tf_var in tf.trainable_variables()\n",
    ") # L2 loss prevents this overkill neural network to overfit the data\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=pred)) + l2 # Softmax loss\n",
    "if decaying_learning_rate:\n",
    "    learning_rate = tf.train.exponential_decay(init_learning_rate, global_step*batch_size, decay_steps, decay_rate, staircase=True)\n",
    "\n",
    "\n",
    "#decayed_learning_rate = learning_rate * decay_rate ^ (global_step / decay_steps) #exponentially decayed learning rate\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost,global_step=global_step) # Adam Optimizer\n",
    "\n",
    "correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter #80:  Learning rate = 0.005000:   Batch Loss = 3.342341, Accuracy = 0.262499988079\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 3.16299676895, Accuracy = 0.135467275977\n",
      "Iter #640:  Learning rate = 0.005000:   Batch Loss = 2.760847, Accuracy = 0.487500011921\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 2.7448451519, Accuracy = 0.490997433662\n",
      "Iter #1280:  Learning rate = 0.005000:   Batch Loss = 2.530897, Accuracy = 0.512499988079\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 2.49742126465, Accuracy = 0.491283237934\n",
      "Iter #1920:  Learning rate = 0.005000:   Batch Loss = 2.365692, Accuracy = 0.449999988079\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 2.3015806675, Accuracy = 0.491283237934\n",
      "Iter #2560:  Learning rate = 0.005000:   Batch Loss = 2.236192, Accuracy = 0.487500011921\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 2.22711610794, Accuracy = 0.491283237934\n",
      "Iter #3200:  Learning rate = 0.005000:   Batch Loss = 1.952652, Accuracy = 0.600000023842\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 2.0646648407, Accuracy = 0.491283237934\n",
      "Iter #3840:  Learning rate = 0.005000:   Batch Loss = 1.883366, Accuracy = 0.6875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 2.04230475426, Accuracy = 0.418119460344\n",
      "Iter #4480:  Learning rate = 0.005000:   Batch Loss = 1.956976, Accuracy = 0.587499976158\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.89841604233, Accuracy = 0.672192037106\n",
      "Iter #5120:  Learning rate = 0.005000:   Batch Loss = 1.800531, Accuracy = 0.675000011921\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.80461716652, Accuracy = 0.672192037106\n",
      "Iter #5760:  Learning rate = 0.005000:   Batch Loss = 1.702580, Accuracy = 0.737500011921\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.75622391701, Accuracy = 0.672192037106\n",
      "Iter #6400:  Learning rate = 0.005000:   Batch Loss = 1.854053, Accuracy = 0.574999988079\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.89318013191, Accuracy = 0.472134888172\n",
      "Iter #7040:  Learning rate = 0.005000:   Batch Loss = 1.725255, Accuracy = 0.612500011921\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.89017605782, Accuracy = 0.591026008129\n",
      "Iter #7680:  Learning rate = 0.005000:   Batch Loss = 1.823612, Accuracy = 0.612500011921\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.83111858368, Accuracy = 0.672192037106\n",
      "Iter #8320:  Learning rate = 0.005000:   Batch Loss = 1.959980, Accuracy = 0.637499988079\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.998685956, Accuracy = 0.491283237934\n",
      "Iter #8960:  Learning rate = 0.005000:   Batch Loss = 1.930318, Accuracy = 0.46250000596\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.93112707138, Accuracy = 0.491283237934\n",
      "Iter #9600:  Learning rate = 0.005000:   Batch Loss = 1.879464, Accuracy = 0.47499999404\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.88426852226, Accuracy = 0.491283237934\n",
      "Iter #10240:  Learning rate = 0.005000:   Batch Loss = 1.826035, Accuracy = 0.524999976158\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.83387565613, Accuracy = 0.491283237934\n",
      "Iter #10880:  Learning rate = 0.005000:   Batch Loss = 1.774245, Accuracy = 0.537500023842\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.80721187592, Accuracy = 0.491283237934\n",
      "Iter #11520:  Learning rate = 0.005000:   Batch Loss = 1.746082, Accuracy = 0.512499988079\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.78325486183, Accuracy = 0.491283237934\n",
      "Iter #12160:  Learning rate = 0.005000:   Batch Loss = 1.796174, Accuracy = 0.4375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.75395512581, Accuracy = 0.491283237934\n",
      "Iter #12800:  Learning rate = 0.005000:   Batch Loss = 2.055418, Accuracy = 0.425000011921\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.59969210625, Accuracy = 0.668476700783\n",
      "Iter #13440:  Learning rate = 0.005000:   Batch Loss = 1.740309, Accuracy = 0.46250000596\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.75901961327, Accuracy = 0.491283237934\n",
      "Iter #14080:  Learning rate = 0.005000:   Batch Loss = 1.658926, Accuracy = 0.524999976158\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.71181130409, Accuracy = 0.491283237934\n",
      "Iter #14720:  Learning rate = 0.005000:   Batch Loss = 1.660756, Accuracy = 0.512499988079\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.68826341629, Accuracy = 0.491283237934\n",
      "Iter #15360:  Learning rate = 0.005000:   Batch Loss = 1.640379, Accuracy = 0.524999976158\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.69556307793, Accuracy = 0.491283237934\n",
      "Iter #16000:  Learning rate = 0.005000:   Batch Loss = 1.658191, Accuracy = 0.46250000596\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.65444016457, Accuracy = 0.491283237934\n",
      "Iter #16640:  Learning rate = 0.005000:   Batch Loss = 1.619183, Accuracy = 0.512499988079\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.64157056808, Accuracy = 0.491283237934\n",
      "Iter #17280:  Learning rate = 0.005000:   Batch Loss = 1.638937, Accuracy = 0.487500011921\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.62765550613, Accuracy = 0.491283237934\n",
      "Iter #17920:  Learning rate = 0.005000:   Batch Loss = 1.615109, Accuracy = 0.4375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.62040114403, Accuracy = 0.491283237934\n",
      "Iter #18560:  Learning rate = 0.005000:   Batch Loss = 1.586017, Accuracy = 0.47499999404\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 2.40711975098, Accuracy = 0.286939114332\n",
      "Iter #19200:  Learning rate = 0.005000:   Batch Loss = 1.716238, Accuracy = 0.46250000596\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.73657202721, Accuracy = 0.491283237934\n",
      "Iter #19840:  Learning rate = 0.005000:   Batch Loss = 1.684688, Accuracy = 0.46250000596\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.69400322437, Accuracy = 0.491283237934\n",
      "Iter #20480:  Learning rate = 0.005000:   Batch Loss = 1.609093, Accuracy = 0.487500011921\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.61000657082, Accuracy = 0.491283237934\n",
      "Iter #21120:  Learning rate = 0.005000:   Batch Loss = 1.569236, Accuracy = 0.512499988079\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.58965539932, Accuracy = 0.491283237934\n",
      "Iter #21760:  Learning rate = 0.005000:   Batch Loss = 1.621830, Accuracy = 0.387499988079\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.56628799438, Accuracy = 0.491283237934\n",
      "Iter #22400:  Learning rate = 0.005000:   Batch Loss = 1.570779, Accuracy = 0.47499999404\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.56627178192, Accuracy = 0.491283237934\n",
      "Iter #23040:  Learning rate = 0.005000:   Batch Loss = 1.513775, Accuracy = 0.574999988079\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.54846036434, Accuracy = 0.491283237934\n",
      "Iter #23680:  Learning rate = 0.005000:   Batch Loss = 1.477418, Accuracy = 0.524999976158\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.55448532104, Accuracy = 0.491283237934\n",
      "Iter #24320:  Learning rate = 0.005000:   Batch Loss = 1.534605, Accuracy = 0.449999988079\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.54464697838, Accuracy = 0.491283237934\n",
      "Iter #24960:  Learning rate = 0.005000:   Batch Loss = 1.493618, Accuracy = 0.487500011921\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.51755714417, Accuracy = 0.491283237934\n",
      "Iter #25600:  Learning rate = 0.005000:   Batch Loss = 1.535346, Accuracy = 0.5\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.5188447237, Accuracy = 0.491283237934\n",
      "Iter #26240:  Learning rate = 0.005000:   Batch Loss = 1.491347, Accuracy = 0.487500011921\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.50126898289, Accuracy = 0.491283237934\n",
      "Iter #26880:  Learning rate = 0.005000:   Batch Loss = 1.443919, Accuracy = 0.5\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.50724124908, Accuracy = 0.384681344032\n",
      "Iter #27520:  Learning rate = 0.005000:   Batch Loss = 1.536561, Accuracy = 0.425000011921\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.48369789124, Accuracy = 0.491283237934\n",
      "Iter #28160:  Learning rate = 0.005000:   Batch Loss = 1.444480, Accuracy = 0.587499976158\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.49258136749, Accuracy = 0.491283237934\n",
      "Iter #28800:  Learning rate = 0.005000:   Batch Loss = 1.422947, Accuracy = 0.574999988079\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.46812605858, Accuracy = 0.491283237934\n",
      "Iter #29440:  Learning rate = 0.005000:   Batch Loss = 1.493570, Accuracy = 0.41249999404\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.46035480499, Accuracy = 0.491283237934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter #30080:  Learning rate = 0.005000:   Batch Loss = 1.464785, Accuracy = 0.47499999404\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.45908212662, Accuracy = 0.491283237934\n",
      "Iter #30720:  Learning rate = 0.005000:   Batch Loss = 1.454368, Accuracy = 0.4375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.44972658157, Accuracy = 0.491283237934\n",
      "Iter #31360:  Learning rate = 0.005000:   Batch Loss = 1.423979, Accuracy = 0.5\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.4440574646, Accuracy = 0.491283237934\n",
      "Iter #32000:  Learning rate = 0.005000:   Batch Loss = 1.372294, Accuracy = 0.600000023842\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.45660066605, Accuracy = 0.491283237934\n",
      "Iter #32640:  Learning rate = 0.005000:   Batch Loss = 1.388097, Accuracy = 0.537500023842\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.42940270901, Accuracy = 0.491283237934\n",
      "Iter #33280:  Learning rate = 0.005000:   Batch Loss = 1.388076, Accuracy = 0.600000023842\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.4270581007, Accuracy = 0.491283237934\n",
      "Iter #33920:  Learning rate = 0.005000:   Batch Loss = 1.370807, Accuracy = 0.524999976158\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.4111084938, Accuracy = 0.491283237934\n",
      "Iter #34560:  Learning rate = 0.005000:   Batch Loss = 1.402565, Accuracy = 0.46250000596\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.40782022476, Accuracy = 0.491283237934\n",
      "Iter #35200:  Learning rate = 0.005000:   Batch Loss = 1.353237, Accuracy = 0.425000011921\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.41863632202, Accuracy = 0.384681344032\n",
      "Iter #35840:  Learning rate = 0.005000:   Batch Loss = 1.383028, Accuracy = 0.487500011921\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.39168119431, Accuracy = 0.491283237934\n",
      "Iter #36480:  Learning rate = 0.005000:   Batch Loss = 1.414511, Accuracy = 0.487500011921\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.39897429943, Accuracy = 0.491283237934\n",
      "Iter #37120:  Learning rate = 0.005000:   Batch Loss = 1.395918, Accuracy = 0.46250000596\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.39997339249, Accuracy = 0.491283237934\n",
      "Iter #37760:  Learning rate = 0.005000:   Batch Loss = 1.378673, Accuracy = 0.524999976158\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.37482130527, Accuracy = 0.491283237934\n",
      "Iter #38400:  Learning rate = 0.005000:   Batch Loss = 1.348277, Accuracy = 0.550000011921\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.36405968666, Accuracy = 0.491283237934\n",
      "Iter #39040:  Learning rate = 0.005000:   Batch Loss = 1.341387, Accuracy = 0.46250000596\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.36982285976, Accuracy = 0.491283237934\n",
      "Iter #39680:  Learning rate = 0.005000:   Batch Loss = 1.343859, Accuracy = 0.524999976158\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.37182021141, Accuracy = 0.491283237934\n",
      "Iter #40320:  Learning rate = 0.005000:   Batch Loss = 1.397986, Accuracy = 0.4375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.36913669109, Accuracy = 0.491283237934\n",
      "Iter #40960:  Learning rate = 0.005000:   Batch Loss = 1.356740, Accuracy = 0.47499999404\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.35797429085, Accuracy = 0.491283237934\n",
      "Iter #41600:  Learning rate = 0.005000:   Batch Loss = 1.314309, Accuracy = 0.537500023842\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.35098588467, Accuracy = 0.491283237934\n",
      "Iter #42240:  Learning rate = 0.005000:   Batch Loss = 1.242670, Accuracy = 0.625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.33640241623, Accuracy = 0.491283237934\n",
      "Iter #42880:  Learning rate = 0.005000:   Batch Loss = 1.337815, Accuracy = 0.47499999404\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.32487773895, Accuracy = 0.491283237934\n",
      "Iter #43520:  Learning rate = 0.005000:   Batch Loss = 1.324972, Accuracy = 0.5\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.32118415833, Accuracy = 0.491283237934\n",
      "Iter #44160:  Learning rate = 0.005000:   Batch Loss = 1.368716, Accuracy = 0.40000000596\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3166655302, Accuracy = 0.491283237934\n",
      "Iter #44800:  Learning rate = 0.005000:   Batch Loss = 1.281049, Accuracy = 0.487500011921\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.31262254715, Accuracy = 0.491283237934\n",
      "Iter #45440:  Learning rate = 0.005000:   Batch Loss = 1.319820, Accuracy = 0.487500011921\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.30468893051, Accuracy = 0.491283237934\n",
      "Iter #46080:  Learning rate = 0.005000:   Batch Loss = 1.263361, Accuracy = 0.550000011921\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.30269193649, Accuracy = 0.491283237934\n",
      "Iter #46720:  Learning rate = 0.005000:   Batch Loss = 1.267246, Accuracy = 0.524999976158\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.29809629917, Accuracy = 0.491283237934\n",
      "Iter #47360:  Learning rate = 0.005000:   Batch Loss = 1.361916, Accuracy = 0.425000011921\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.29987370968, Accuracy = 0.491283237934\n",
      "Iter #48000:  Learning rate = 0.005000:   Batch Loss = 1.350189, Accuracy = 0.46250000596\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.31136882305, Accuracy = 0.491283237934\n",
      "Iter #48640:  Learning rate = 0.005000:   Batch Loss = 1.307441, Accuracy = 0.46250000596\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.28629076481, Accuracy = 0.491283237934\n",
      "Iter #49280:  Learning rate = 0.005000:   Batch Loss = 1.307389, Accuracy = 0.387499988079\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.284943223, Accuracy = 0.491283237934\n",
      "Iter #49920:  Learning rate = 0.005000:   Batch Loss = 1.305135, Accuracy = 0.425000011921\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.28552532196, Accuracy = 0.491283237934\n",
      "Iter #50560:  Learning rate = 0.005000:   Batch Loss = 1.284306, Accuracy = 0.47499999404\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.27362966537, Accuracy = 0.491283237934\n",
      "Iter #51200:  Learning rate = 0.005000:   Batch Loss = 1.236916, Accuracy = 0.487500011921\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.26963472366, Accuracy = 0.491283237934\n",
      "Iter #51840:  Learning rate = 0.005000:   Batch Loss = 1.235288, Accuracy = 0.524999976158\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.26780045033, Accuracy = 0.491283237934\n",
      "Iter #52480:  Learning rate = 0.005000:   Batch Loss = 1.220898, Accuracy = 0.537500023842\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.26425611973, Accuracy = 0.491283237934\n",
      "Iter #53120:  Learning rate = 0.005000:   Batch Loss = 1.291942, Accuracy = 0.449999988079\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.25807571411, Accuracy = 0.491283237934\n",
      "Iter #53760:  Learning rate = 0.005000:   Batch Loss = 1.225861, Accuracy = 0.600000023842\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.25780117512, Accuracy = 0.491283237934\n",
      "Iter #54400:  Learning rate = 0.005000:   Batch Loss = 1.266101, Accuracy = 0.47499999404\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.25652766228, Accuracy = 0.491283237934\n",
      "Iter #55040:  Learning rate = 0.005000:   Batch Loss = 1.292822, Accuracy = 0.46250000596\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.24630367756, Accuracy = 0.491283237934\n",
      "Iter #55680:  Learning rate = 0.005000:   Batch Loss = 1.235104, Accuracy = 0.537500023842\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.24006354809, Accuracy = 0.491283237934\n",
      "Iter #56320:  Learning rate = 0.005000:   Batch Loss = 1.185254, Accuracy = 0.512499988079\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.24002945423, Accuracy = 0.491283237934\n",
      "Iter #56960:  Learning rate = 0.005000:   Batch Loss = 1.264368, Accuracy = 0.5\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.23917114735, Accuracy = 0.491283237934\n",
      "Iter #57600:  Learning rate = 0.005000:   Batch Loss = 1.202289, Accuracy = 0.5\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.23863589764, Accuracy = 0.491283237934\n",
      "Iter #58240:  Learning rate = 0.005000:   Batch Loss = 1.174125, Accuracy = 0.537500023842\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.23909044266, Accuracy = 0.491283237934\n",
      "Iter #58880:  Learning rate = 0.005000:   Batch Loss = 1.223602, Accuracy = 0.487500011921\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.22975111008, Accuracy = 0.491283237934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter #59520:  Learning rate = 0.005000:   Batch Loss = 1.203667, Accuracy = 0.550000011921\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.21786665916, Accuracy = 0.491283237934\n",
      "Iter #60160:  Learning rate = 0.005000:   Batch Loss = 1.619515, Accuracy = 0.25\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.56224131584, Accuracy = 0.287224918604\n",
      "Iter #60800:  Learning rate = 0.005000:   Batch Loss = 1.312478, Accuracy = 0.5\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.34117949009, Accuracy = 0.491283237934\n",
      "Iter #61440:  Learning rate = 0.005000:   Batch Loss = 1.210857, Accuracy = 0.550000011921\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.26893770695, Accuracy = 0.491283237934\n",
      "Iter #62080:  Learning rate = 0.005000:   Batch Loss = 1.236948, Accuracy = 0.487500011921\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.26032435894, Accuracy = 0.491283237934\n",
      "Iter #62720:  Learning rate = 0.005000:   Batch Loss = 1.246767, Accuracy = 0.425000011921\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.23426127434, Accuracy = 0.491283237934\n",
      "Iter #63360:  Learning rate = 0.005000:   Batch Loss = 1.247797, Accuracy = 0.41249999404\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.22996258736, Accuracy = 0.491283237934\n",
      "Iter #64000:  Learning rate = 0.005000:   Batch Loss = 1.183048, Accuracy = 0.487500011921\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.22316324711, Accuracy = 0.491283237934\n",
      "Iter #64640:  Learning rate = 0.005000:   Batch Loss = 1.202984, Accuracy = 0.5625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.21784114838, Accuracy = 0.491283237934\n",
      "Iter #65280:  Learning rate = 0.005000:   Batch Loss = 1.183103, Accuracy = 0.550000011921\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.21945130825, Accuracy = 0.491283237934\n",
      "Iter #65920:  Learning rate = 0.005000:   Batch Loss = 1.214656, Accuracy = 0.46250000596\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.22651743889, Accuracy = 0.491283237934\n",
      "Iter #66560:  Learning rate = 0.005000:   Batch Loss = 1.149389, Accuracy = 0.550000011921\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.21538388729, Accuracy = 0.491283237934\n",
      "Iter #67200:  Learning rate = 0.005000:   Batch Loss = 1.248719, Accuracy = 0.375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.20580053329, Accuracy = 0.491283237934\n",
      "Iter #67840:  Learning rate = 0.005000:   Batch Loss = 1.220142, Accuracy = 0.46250000596\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.20206737518, Accuracy = 0.491283237934\n",
      "Iter #68480:  Learning rate = 0.005000:   Batch Loss = 1.198125, Accuracy = 0.5625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.19956576824, Accuracy = 0.491283237934\n",
      "Iter #69120:  Learning rate = 0.005000:   Batch Loss = 1.227886, Accuracy = 0.425000011921\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.20067489147, Accuracy = 0.491283237934\n",
      "Iter #69760:  Learning rate = 0.005000:   Batch Loss = 1.196430, Accuracy = 0.487500011921\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.20131993294, Accuracy = 0.491283237934\n",
      "Iter #70400:  Learning rate = 0.005000:   Batch Loss = 1.213787, Accuracy = 0.5\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.18850553036, Accuracy = 0.491283237934\n",
      "Iter #71040:  Learning rate = 0.005000:   Batch Loss = 1.103649, Accuracy = 0.637499988079\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.18733882904, Accuracy = 0.491283237934\n",
      "Iter #71680:  Learning rate = 0.005000:   Batch Loss = 1.192869, Accuracy = 0.487500011921\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.19034349918, Accuracy = 0.491283237934\n",
      "Iter #72320:  Learning rate = 0.005000:   Batch Loss = 1.146414, Accuracy = 0.537500023842\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.19443500042, Accuracy = 0.491283237934\n",
      "Iter #72960:  Learning rate = 0.005000:   Batch Loss = 1.193408, Accuracy = 0.46250000596\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.19018101692, Accuracy = 0.491283237934\n",
      "Iter #73600:  Learning rate = 0.005000:   Batch Loss = 1.144882, Accuracy = 0.487500011921\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.18419873714, Accuracy = 0.491283237934\n",
      "Iter #74240:  Learning rate = 0.005000:   Batch Loss = 1.136618, Accuracy = 0.47499999404\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.18058550358, Accuracy = 0.491283237934\n",
      "Iter #74880:  Learning rate = 0.005000:   Batch Loss = 1.125774, Accuracy = 0.600000023842\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.18029975891, Accuracy = 0.491283237934\n",
      "Iter #75520:  Learning rate = 0.005000:   Batch Loss = 1.212858, Accuracy = 0.41249999404\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1730774641, Accuracy = 0.491283237934\n",
      "Iter #76160:  Learning rate = 0.005000:   Batch Loss = 1.153492, Accuracy = 0.550000011921\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.17497086525, Accuracy = 0.491283237934\n",
      "Iter #76800:  Learning rate = 0.005000:   Batch Loss = 1.080786, Accuracy = 0.5625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.17087626457, Accuracy = 0.491283237934\n",
      "Iter #77440:  Learning rate = 0.005000:   Batch Loss = 1.165744, Accuracy = 0.449999988079\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.16284108162, Accuracy = 0.491283237934\n",
      "Iter #78080:  Learning rate = 0.005000:   Batch Loss = 1.177703, Accuracy = 0.524999976158\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.15978467464, Accuracy = 0.491283237934\n",
      "Iter #78720:  Learning rate = 0.005000:   Batch Loss = 1.121889, Accuracy = 0.537500023842\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.15723788738, Accuracy = 0.491283237934\n",
      "Iter #79360:  Learning rate = 0.005000:   Batch Loss = 1.180500, Accuracy = 0.4375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.16638898849, Accuracy = 0.491283237934\n",
      "Iter #80000:  Learning rate = 0.005000:   Batch Loss = 1.160171, Accuracy = 0.5\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.17040205002, Accuracy = 0.491283237934\n",
      "Iter #80640:  Learning rate = 0.005000:   Batch Loss = 1.196198, Accuracy = 0.41249999404\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.16319096088, Accuracy = 0.491283237934\n",
      "Iter #81280:  Learning rate = 0.005000:   Batch Loss = 1.104491, Accuracy = 0.512499988079\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.15640735626, Accuracy = 0.491283237934\n",
      "Iter #81920:  Learning rate = 0.005000:   Batch Loss = 1.129756, Accuracy = 0.537500023842\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.15454018116, Accuracy = 0.491283237934\n",
      "Iter #82560:  Learning rate = 0.005000:   Batch Loss = 1.130499, Accuracy = 0.524999976158\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.15168511868, Accuracy = 0.491283237934\n",
      "Iter #83200:  Learning rate = 0.005000:   Batch Loss = 1.128131, Accuracy = 0.512499988079\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.15026521683, Accuracy = 0.491283237934\n",
      "Iter #83840:  Learning rate = 0.005000:   Batch Loss = 1.155593, Accuracy = 0.512499988079\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.14932262897, Accuracy = 0.491283237934\n",
      "Iter #84480:  Learning rate = 0.005000:   Batch Loss = 1.100721, Accuracy = 0.487500011921\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.14460468292, Accuracy = 0.491283237934\n",
      "Iter #85120:  Learning rate = 0.005000:   Batch Loss = 1.089098, Accuracy = 0.600000023842\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.14092111588, Accuracy = 0.491283237934\n",
      "Iter #85760:  Learning rate = 0.005000:   Batch Loss = 1.068533, Accuracy = 0.5625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.13801133633, Accuracy = 0.491283237934\n",
      "Iter #86400:  Learning rate = 0.005000:   Batch Loss = 1.131230, Accuracy = 0.5\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.13582456112, Accuracy = 0.491283237934\n",
      "Iter #87040:  Learning rate = 0.005000:   Batch Loss = 1.094598, Accuracy = 0.512499988079\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.13428032398, Accuracy = 0.491283237934\n",
      "Iter #87680:  Learning rate = 0.005000:   Batch Loss = 1.106852, Accuracy = 0.5\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.13269090652, Accuracy = 0.491283237934\n",
      "Iter #88320:  Learning rate = 0.005000:   Batch Loss = 1.104837, Accuracy = 0.537500023842\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.13289034367, Accuracy = 0.491283237934\n",
      "Iter #88960:  Learning rate = 0.005000:   Batch Loss = 1.173251, Accuracy = 0.46250000596\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.13139247894, Accuracy = 0.491283237934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter #89600:  Learning rate = 0.005000:   Batch Loss = 1.093002, Accuracy = 0.47499999404\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.13090109825, Accuracy = 0.491283237934\n",
      "Iter #90240:  Learning rate = 0.005000:   Batch Loss = 1.097141, Accuracy = 0.47499999404\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.12813448906, Accuracy = 0.491283237934\n",
      "Iter #90880:  Learning rate = 0.005000:   Batch Loss = 1.108520, Accuracy = 0.5\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.12638103962, Accuracy = 0.491283237934\n",
      "Iter #91520:  Learning rate = 0.005000:   Batch Loss = 1.142424, Accuracy = 0.512499988079\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.12593567371, Accuracy = 0.491283237934\n",
      "Iter #92160:  Learning rate = 0.005000:   Batch Loss = 1.137514, Accuracy = 0.47499999404\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1208409071, Accuracy = 0.491283237934\n",
      "Iter #92800:  Learning rate = 0.005000:   Batch Loss = 1.088289, Accuracy = 0.524999976158\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.11918210983, Accuracy = 0.491283237934\n",
      "Iter #93440:  Learning rate = 0.005000:   Batch Loss = 1.055093, Accuracy = 0.574999988079\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.12040579319, Accuracy = 0.491283237934\n",
      "Iter #94080:  Learning rate = 0.005000:   Batch Loss = 1.031406, Accuracy = 0.600000023842\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.11594212055, Accuracy = 0.491283237934\n",
      "Iter #94720:  Learning rate = 0.005000:   Batch Loss = 1.077605, Accuracy = 0.5625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.11310899258, Accuracy = 0.491283237934\n",
      "Iter #95360:  Learning rate = 0.005000:   Batch Loss = 1.109840, Accuracy = 0.47499999404\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.11233925819, Accuracy = 0.491283237934\n",
      "Iter #96000:  Learning rate = 0.005000:   Batch Loss = 1.177545, Accuracy = 0.375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.11236250401, Accuracy = 0.491283237934\n",
      "Iter #96640:  Learning rate = 0.005000:   Batch Loss = 1.089960, Accuracy = 0.550000011921\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.11141455173, Accuracy = 0.491283237934\n",
      "Iter #97280:  Learning rate = 0.005000:   Batch Loss = 1.079397, Accuracy = 0.512499988079\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1091452837, Accuracy = 0.491283237934\n",
      "Iter #97920:  Learning rate = 0.005000:   Batch Loss = 1.083650, Accuracy = 0.487500011921\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1068520546, Accuracy = 0.491283237934\n",
      "Iter #98560:  Learning rate = 0.005000:   Batch Loss = 1.064043, Accuracy = 0.600000023842\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.10534894466, Accuracy = 0.491283237934\n",
      "Iter #99200:  Learning rate = 0.005000:   Batch Loss = 1.074253, Accuracy = 0.524999976158\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.100943923, Accuracy = 0.491283237934\n",
      "Iter #99840:  Learning rate = 0.005000:   Batch Loss = 1.017754, Accuracy = 0.574999988079\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0987149477, Accuracy = 0.491283237934\n",
      "Iter #100480:  Learning rate = 0.004800:   Batch Loss = 1.096944, Accuracy = 0.5\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.10359334946, Accuracy = 0.491283237934\n",
      "Iter #101120:  Learning rate = 0.004800:   Batch Loss = 1.108989, Accuracy = 0.550000011921\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.11484491825, Accuracy = 0.491283237934\n",
      "Iter #101760:  Learning rate = 0.004800:   Batch Loss = 1.164057, Accuracy = 0.425000011921\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.09989833832, Accuracy = 0.491283237934\n",
      "Iter #102400:  Learning rate = 0.004800:   Batch Loss = 1.075384, Accuracy = 0.487500011921\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.09776639938, Accuracy = 0.491283237934\n",
      "Iter #103040:  Learning rate = 0.004800:   Batch Loss = 1.115000, Accuracy = 0.46250000596\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.09274089336, Accuracy = 0.491283237934\n",
      "Iter #103680:  Learning rate = 0.004800:   Batch Loss = 1.118664, Accuracy = 0.40000000596\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.09177350998, Accuracy = 0.491283237934\n",
      "Iter #104320:  Learning rate = 0.004800:   Batch Loss = 1.013918, Accuracy = 0.625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.08969342709, Accuracy = 0.491283237934\n",
      "Iter #104960:  Learning rate = 0.004800:   Batch Loss = 1.141349, Accuracy = 0.41249999404\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.09020113945, Accuracy = 0.491283237934\n",
      "Iter #105600:  Learning rate = 0.004800:   Batch Loss = 1.100328, Accuracy = 0.487500011921\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.08952832222, Accuracy = 0.491283237934\n",
      "Iter #106240:  Learning rate = 0.004800:   Batch Loss = 1.032303, Accuracy = 0.5625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.09015870094, Accuracy = 0.491283237934\n",
      "Iter #106880:  Learning rate = 0.004800:   Batch Loss = 1.116028, Accuracy = 0.425000011921\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.08699965477, Accuracy = 0.491283237934\n",
      "Iter #107520:  Learning rate = 0.004800:   Batch Loss = 1.033549, Accuracy = 0.537500023842\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.08501553535, Accuracy = 0.491283237934\n",
      "Iter #108160:  Learning rate = 0.004800:   Batch Loss = 1.112156, Accuracy = 0.47499999404\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0875300169, Accuracy = 0.491283237934\n",
      "Iter #108800:  Learning rate = 0.004800:   Batch Loss = 1.069916, Accuracy = 0.47499999404\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.08208584785, Accuracy = 0.491283237934\n",
      "Iter #109440:  Learning rate = 0.004800:   Batch Loss = 1.060777, Accuracy = 0.47499999404\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.07784116268, Accuracy = 0.491283237934\n",
      "Iter #110080:  Learning rate = 0.004800:   Batch Loss = 1.041929, Accuracy = 0.550000011921\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.07746815681, Accuracy = 0.491283237934\n",
      "Iter #110720:  Learning rate = 0.004800:   Batch Loss = 1.035220, Accuracy = 0.5625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.07677400112, Accuracy = 0.491283237934\n",
      "Iter #111360:  Learning rate = 0.004800:   Batch Loss = 1.053703, Accuracy = 0.47499999404\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.07678496838, Accuracy = 0.491283237934\n",
      "Iter #112000:  Learning rate = 0.004800:   Batch Loss = 1.051833, Accuracy = 0.487500011921\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.07500720024, Accuracy = 0.491283237934\n",
      "Iter #112640:  Learning rate = 0.004800:   Batch Loss = 1.044356, Accuracy = 0.537500023842\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.07476878166, Accuracy = 0.491283237934\n",
      "Iter #113280:  Learning rate = 0.004800:   Batch Loss = 1.041183, Accuracy = 0.512499988079\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.07529342175, Accuracy = 0.491283237934\n",
      "Iter #113920:  Learning rate = 0.004800:   Batch Loss = 1.071098, Accuracy = 0.5\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0754545927, Accuracy = 0.491283237934\n",
      "Iter #114560:  Learning rate = 0.004800:   Batch Loss = 1.122627, Accuracy = 0.40000000596\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.07615327835, Accuracy = 0.491283237934\n",
      "Iter #115200:  Learning rate = 0.004800:   Batch Loss = 1.045754, Accuracy = 0.487500011921\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.07493627071, Accuracy = 0.491283237934\n",
      "Iter #115840:  Learning rate = 0.004800:   Batch Loss = 1.054677, Accuracy = 0.512499988079\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0697824955, Accuracy = 0.491283237934\n",
      "Iter #116480:  Learning rate = 0.004800:   Batch Loss = 1.050681, Accuracy = 0.47499999404\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.06500661373, Accuracy = 0.491283237934\n",
      "Iter #117120:  Learning rate = 0.004800:   Batch Loss = 1.084993, Accuracy = 0.449999988079\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.06186497211, Accuracy = 0.491283237934\n",
      "Iter #117760:  Learning rate = 0.004800:   Batch Loss = 1.062248, Accuracy = 0.5\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0615657568, Accuracy = 0.491283237934\n",
      "Iter #118400:  Learning rate = 0.004800:   Batch Loss = 1.025446, Accuracy = 0.512499988079\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0636767149, Accuracy = 0.491283237934\n",
      "Iter #119040:  Learning rate = 0.004800:   Batch Loss = 1.136338, Accuracy = 0.34999999404\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.06259131432, Accuracy = 0.491283237934\n",
      "Iter #119680:  Learning rate = 0.004800:   Batch Loss = 0.990083, Accuracy = 0.587499976158\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.05908250809, Accuracy = 0.491283237934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter #120320:  Learning rate = 0.004800:   Batch Loss = 1.024338, Accuracy = 0.5\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.05986571312, Accuracy = 0.491283237934\n",
      "Iter #120960:  Learning rate = 0.004800:   Batch Loss = 1.057267, Accuracy = 0.5\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0599937439, Accuracy = 0.491283237934\n",
      "Iter #121600:  Learning rate = 0.004800:   Batch Loss = 1.081466, Accuracy = 0.46250000596\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.05611491203, Accuracy = 0.491283237934\n",
      "Iter #122240:  Learning rate = 0.004800:   Batch Loss = 1.024236, Accuracy = 0.512499988079\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0548324585, Accuracy = 0.491283237934\n",
      "Iter #122880:  Learning rate = 0.004800:   Batch Loss = 1.062671, Accuracy = 0.512499988079\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.05591070652, Accuracy = 0.491283237934\n",
      "Iter #123520:  Learning rate = 0.004800:   Batch Loss = 1.051125, Accuracy = 0.537500023842\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.05493986607, Accuracy = 0.491283237934\n",
      "Iter #124160:  Learning rate = 0.004800:   Batch Loss = 1.053417, Accuracy = 0.487500011921\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.05073952675, Accuracy = 0.491283237934\n",
      "Iter #124800:  Learning rate = 0.004800:   Batch Loss = 1.012138, Accuracy = 0.47499999404\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.05097770691, Accuracy = 0.491283237934\n",
      "Iter #125440:  Learning rate = 0.004800:   Batch Loss = 1.043980, Accuracy = 0.5\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.05079185963, Accuracy = 0.491283237934\n",
      "Iter #126080:  Learning rate = 0.004800:   Batch Loss = 1.074776, Accuracy = 0.40000000596\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.04920184612, Accuracy = 0.491283237934\n",
      "Iter #126720:  Learning rate = 0.004800:   Batch Loss = 1.070476, Accuracy = 0.449999988079\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.04991781712, Accuracy = 0.491283237934\n",
      "Iter #127360:  Learning rate = 0.004800:   Batch Loss = 1.070824, Accuracy = 0.449999988079\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.05325591564, Accuracy = 0.491283237934\n",
      "Iter #128000:  Learning rate = 0.004800:   Batch Loss = 0.977348, Accuracy = 0.537500023842\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.05623078346, Accuracy = 0.491283237934\n",
      "Iter #128640:  Learning rate = 0.004800:   Batch Loss = 1.083425, Accuracy = 0.47499999404\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.05579948425, Accuracy = 0.491283237934\n",
      "Iter #129280:  Learning rate = 0.004800:   Batch Loss = 1.032900, Accuracy = 0.550000011921\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.04860341549, Accuracy = 0.491283237934\n",
      "Iter #129920:  Learning rate = 0.004800:   Batch Loss = 1.029919, Accuracy = 0.512499988079\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.04424679279, Accuracy = 0.491283237934\n",
      "Iter #130560:  Learning rate = 0.004800:   Batch Loss = 0.963208, Accuracy = 0.612500011921\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.04157364368, Accuracy = 0.491283237934\n",
      "Iter #131200:  Learning rate = 0.004800:   Batch Loss = 0.999894, Accuracy = 0.587499976158\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.04068422318, Accuracy = 0.491283237934\n",
      "Iter #131840:  Learning rate = 0.004800:   Batch Loss = 0.970058, Accuracy = 0.612500011921\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.04132008553, Accuracy = 0.491283237934\n",
      "Iter #132480:  Learning rate = 0.004800:   Batch Loss = 0.986628, Accuracy = 0.5625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.04506921768, Accuracy = 0.491283237934\n",
      "Iter #133120:  Learning rate = 0.004800:   Batch Loss = 1.020722, Accuracy = 0.512499988079\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.04545414448, Accuracy = 0.491283237934\n",
      "Iter #133760:  Learning rate = 0.004800:   Batch Loss = 1.009841, Accuracy = 0.524999976158\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.04202389717, Accuracy = 0.491283237934\n",
      "Iter #134400:  Learning rate = 0.004800:   Batch Loss = 1.072529, Accuracy = 0.375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.04164135456, Accuracy = 0.491283237934\n",
      "Iter #135040:  Learning rate = 0.004800:   Batch Loss = 0.996976, Accuracy = 0.574999988079\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.04087424278, Accuracy = 0.491283237934\n",
      "Iter #135680:  Learning rate = 0.004800:   Batch Loss = 0.998048, Accuracy = 0.46250000596\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0386070013, Accuracy = 0.491283237934\n",
      "Iter #136320:  Learning rate = 0.004800:   Batch Loss = 0.952918, Accuracy = 0.587499976158\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.03455710411, Accuracy = 0.491283237934\n",
      "Iter #136960:  Learning rate = 0.004800:   Batch Loss = 1.028363, Accuracy = 0.5\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.03571033478, Accuracy = 0.491283237934\n",
      "Iter #137600:  Learning rate = 0.004800:   Batch Loss = 0.998151, Accuracy = 0.537500023842\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.03320419788, Accuracy = 0.491283237934\n",
      "Iter #138240:  Learning rate = 0.004800:   Batch Loss = 1.014101, Accuracy = 0.512499988079\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.03127336502, Accuracy = 0.491283237934\n",
      "Iter #138880:  Learning rate = 0.004800:   Batch Loss = 1.030332, Accuracy = 0.5\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.03537642956, Accuracy = 0.491283237934\n",
      "Iter #139520:  Learning rate = 0.004800:   Batch Loss = 1.009417, Accuracy = 0.537500023842\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.03958368301, Accuracy = 0.491283237934\n",
      "Iter #140160:  Learning rate = 0.004800:   Batch Loss = 1.013982, Accuracy = 0.574999988079\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.03436303139, Accuracy = 0.491283237934\n",
      "Iter #140800:  Learning rate = 0.004800:   Batch Loss = 1.034612, Accuracy = 0.512499988079\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.03404402733, Accuracy = 0.491283237934\n",
      "Iter #141440:  Learning rate = 0.004800:   Batch Loss = 1.010848, Accuracy = 0.487500011921\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.02986550331, Accuracy = 0.491283237934\n",
      "Iter #142080:  Learning rate = 0.004800:   Batch Loss = 1.079347, Accuracy = 0.41249999404\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.02557587624, Accuracy = 0.491283237934\n",
      "Iter #142720:  Learning rate = 0.004800:   Batch Loss = 1.034836, Accuracy = 0.46250000596\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.02466416359, Accuracy = 0.491283237934\n",
      "Iter #143360:  Learning rate = 0.004800:   Batch Loss = 1.029318, Accuracy = 0.47499999404\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.02551269531, Accuracy = 0.491283237934\n",
      "Iter #144000:  Learning rate = 0.004800:   Batch Loss = 1.061816, Accuracy = 0.33750000596\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.02894461155, Accuracy = 0.491283237934\n",
      "Iter #144640:  Learning rate = 0.004800:   Batch Loss = 1.064044, Accuracy = 0.4375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.03217053413, Accuracy = 0.491283237934\n",
      "Iter #145280:  Learning rate = 0.004800:   Batch Loss = 0.993044, Accuracy = 0.5\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.03313612938, Accuracy = 0.491283237934\n",
      "Iter #145920:  Learning rate = 0.004800:   Batch Loss = 1.064815, Accuracy = 0.375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.02724599838, Accuracy = 0.491283237934\n",
      "Iter #146560:  Learning rate = 0.004800:   Batch Loss = 0.942013, Accuracy = 0.600000023842\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.02467060089, Accuracy = 0.491283237934\n",
      "Iter #147200:  Learning rate = 0.004800:   Batch Loss = 1.069104, Accuracy = 0.387499988079\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.02583265305, Accuracy = 0.491283237934\n",
      "Iter #147840:  Learning rate = 0.004800:   Batch Loss = 0.978735, Accuracy = 0.625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.02715551853, Accuracy = 0.491283237934\n",
      "Iter #148480:  Learning rate = 0.004800:   Batch Loss = 1.023157, Accuracy = 0.487500011921\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0266712904, Accuracy = 0.491283237934\n",
      "Iter #149120:  Learning rate = 0.004800:   Batch Loss = 1.014149, Accuracy = 0.512499988079\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.02545452118, Accuracy = 0.491283237934\n",
      "Iter #149760:  Learning rate = 0.004800:   Batch Loss = 0.989977, Accuracy = 0.550000011921\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.02090978622, Accuracy = 0.491283237934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter #150400:  Learning rate = 0.004800:   Batch Loss = 0.938053, Accuracy = 0.537500023842\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.01886081696, Accuracy = 0.491283237934\n",
      "Iter #151040:  Learning rate = 0.004800:   Batch Loss = 0.969474, Accuracy = 0.612500011921\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.01986718178, Accuracy = 0.491283237934\n",
      "Iter #151680:  Learning rate = 0.004800:   Batch Loss = 0.984289, Accuracy = 0.537500023842\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.02393424511, Accuracy = 0.491283237934\n",
      "Iter #152320:  Learning rate = 0.004800:   Batch Loss = 0.975448, Accuracy = 0.537500023842\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0228651762, Accuracy = 0.491283237934\n",
      "Iter #152960:  Learning rate = 0.004800:   Batch Loss = 1.008953, Accuracy = 0.47499999404\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.02160477638, Accuracy = 0.491283237934\n",
      "Iter #153600:  Learning rate = 0.004800:   Batch Loss = 1.009225, Accuracy = 0.46250000596\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.01765334606, Accuracy = 0.491283237934\n",
      "Iter #154240:  Learning rate = 0.004800:   Batch Loss = 1.017912, Accuracy = 0.46250000596\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.01609754562, Accuracy = 0.491283237934\n",
      "Iter #154880:  Learning rate = 0.004800:   Batch Loss = 0.986523, Accuracy = 0.512499988079\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.01600754261, Accuracy = 0.491283237934\n",
      "Iter #155520:  Learning rate = 0.004800:   Batch Loss = 0.981607, Accuracy = 0.5\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.01407313347, Accuracy = 0.491283237934\n",
      "Iter #156160:  Learning rate = 0.004800:   Batch Loss = 1.011525, Accuracy = 0.512499988079\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.01349580288, Accuracy = 0.491283237934\n",
      "Iter #156800:  Learning rate = 0.004800:   Batch Loss = 1.045380, Accuracy = 0.387499988079\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.01429128647, Accuracy = 0.491283237934\n",
      "Iter #157440:  Learning rate = 0.004800:   Batch Loss = 0.982781, Accuracy = 0.537500023842\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.01633572578, Accuracy = 0.491283237934\n",
      "Iter #158080:  Learning rate = 0.004800:   Batch Loss = 1.019850, Accuracy = 0.5\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.01690268517, Accuracy = 0.491283237934\n",
      "Iter #158720:  Learning rate = 0.004800:   Batch Loss = 0.908193, Accuracy = 0.587499976158\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.01563918591, Accuracy = 0.491283237934\n",
      "Iter #159360:  Learning rate = 0.004800:   Batch Loss = 3.312028, Accuracy = 0.27500000596\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 3.17449498177, Accuracy = 0.2043440938\n",
      "Iter #160000:  Learning rate = 0.004800:   Batch Loss = 1.674748, Accuracy = 0.300000011921\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.66201293468, Accuracy = 0.286939114332\n",
      "Iter #160640:  Learning rate = 0.004800:   Batch Loss = 1.681557, Accuracy = 0.25\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.6611866951, Accuracy = 0.286939114332\n",
      "Iter #161280:  Learning rate = 0.004800:   Batch Loss = 1.707166, Accuracy = 0.21250000596\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.6644616127, Accuracy = 0.286939114332\n",
      "Iter #161920:  Learning rate = 0.004800:   Batch Loss = 1.691606, Accuracy = 0.28749999404\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.76886236668, Accuracy = 0.2043440938\n",
      "Iter #162560:  Learning rate = 0.004800:   Batch Loss = 1.717758, Accuracy = 0.22499999404\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7095181942, Accuracy = 0.2043440938\n",
      "Iter #163200:  Learning rate = 0.004800:   Batch Loss = 1.670533, Accuracy = 0.16249999404\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.66614842415, Accuracy = 0.286939114332\n",
      "Iter #163840:  Learning rate = 0.004800:   Batch Loss = 1.649344, Accuracy = 0.23749999702\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.66715884209, Accuracy = 0.286939114332\n",
      "Iter #164480:  Learning rate = 0.004800:   Batch Loss = 1.693269, Accuracy = 0.22499999404\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.66748988628, Accuracy = 0.2043440938\n",
      "Iter #165120:  Learning rate = 0.004800:   Batch Loss = 1.654875, Accuracy = 0.1875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.66509008408, Accuracy = 0.286939114332\n",
      "Iter #165760:  Learning rate = 0.004800:   Batch Loss = 1.675237, Accuracy = 0.22499999404\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.66212391853, Accuracy = 0.286939114332\n",
      "Iter #166400:  Learning rate = 0.004800:   Batch Loss = 1.664789, Accuracy = 0.20000000298\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.66163086891, Accuracy = 0.286939114332\n",
      "Iter #167040:  Learning rate = 0.004800:   Batch Loss = 1.737407, Accuracy = 0.22499999404\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.74314332008, Accuracy = 0.2043440938\n",
      "Iter #167680:  Learning rate = 0.004800:   Batch Loss = 1.667279, Accuracy = 0.300000011921\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.70471167564, Accuracy = 0.2043440938\n",
      "Iter #168320:  Learning rate = 0.004800:   Batch Loss = 1.618586, Accuracy = 0.375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.66820836067, Accuracy = 0.2043440938\n",
      "Iter #168960:  Learning rate = 0.004800:   Batch Loss = 1.626126, Accuracy = 0.262499988079\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.65738117695, Accuracy = 0.286939114332\n",
      "Iter #169600:  Learning rate = 0.004800:   Batch Loss = 1.653741, Accuracy = 0.15000000596\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.65522229671, Accuracy = 0.286939114332\n",
      "Iter #170240:  Learning rate = 0.004800:   Batch Loss = 1.621923, Accuracy = 0.23749999702\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.65848755836, Accuracy = 0.2043440938\n",
      "Iter #170880:  Learning rate = 0.004800:   Batch Loss = 1.672001, Accuracy = 0.21250000596\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.65944433212, Accuracy = 0.2043440938\n",
      "Iter #171520:  Learning rate = 0.004800:   Batch Loss = 1.673830, Accuracy = 0.3125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.65739881992, Accuracy = 0.2043440938\n",
      "Iter #172160:  Learning rate = 0.004800:   Batch Loss = 1.662704, Accuracy = 0.25\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.66046071053, Accuracy = 0.2043440938\n",
      "Iter #172800:  Learning rate = 0.004800:   Batch Loss = 1.636153, Accuracy = 0.262499988079\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.65933060646, Accuracy = 0.2043440938\n",
      "Iter #173440:  Learning rate = 0.004800:   Batch Loss = 1.623417, Accuracy = 0.362500011921\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.65550529957, Accuracy = 0.2043440938\n",
      "Iter #174080:  Learning rate = 0.004800:   Batch Loss = 1.644871, Accuracy = 0.23749999702\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.65489137173, Accuracy = 0.2043440938\n",
      "Iter #174720:  Learning rate = 0.004800:   Batch Loss = 1.701981, Accuracy = 0.17499999702\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.65200591087, Accuracy = 0.286939114332\n",
      "Iter #175360:  Learning rate = 0.004800:   Batch Loss = 1.607702, Accuracy = 0.21250000596\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.65390861034, Accuracy = 0.2043440938\n",
      "Iter #176000:  Learning rate = 0.004800:   Batch Loss = 1.655761, Accuracy = 0.300000011921\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.66162824631, Accuracy = 0.2043440938\n",
      "Iter #176640:  Learning rate = 0.004800:   Batch Loss = 1.625567, Accuracy = 0.262499988079\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.66151809692, Accuracy = 0.2043440938\n",
      "Iter #177280:  Learning rate = 0.004800:   Batch Loss = 1.686564, Accuracy = 0.23749999702\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.65460205078, Accuracy = 0.2043440938\n",
      "Iter #177920:  Learning rate = 0.004800:   Batch Loss = 1.628520, Accuracy = 0.34999999404\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.65193712711, Accuracy = 0.2043440938\n",
      "Iter #178560:  Learning rate = 0.004800:   Batch Loss = 1.675565, Accuracy = 0.16249999404\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.65000963211, Accuracy = 0.2043440938\n",
      "Iter #179200:  Learning rate = 0.004800:   Batch Loss = 1.641721, Accuracy = 0.262499988079\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.64944434166, Accuracy = 0.2043440938\n",
      "Iter #179840:  Learning rate = 0.004800:   Batch Loss = 1.609775, Accuracy = 0.375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.64654040337, Accuracy = 0.286939114332\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter #180480:  Learning rate = 0.004800:   Batch Loss = 1.646268, Accuracy = 0.25\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.64465987682, Accuracy = 0.286939114332\n",
      "Iter #181120:  Learning rate = 0.004800:   Batch Loss = 1.651081, Accuracy = 0.17499999702\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.64789605141, Accuracy = 0.2043440938\n",
      "Iter #181760:  Learning rate = 0.004800:   Batch Loss = 1.724210, Accuracy = 0.17499999702\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.66808760166, Accuracy = 0.2043440938\n",
      "Iter #182400:  Learning rate = 0.004800:   Batch Loss = 1.583638, Accuracy = 0.34999999404\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.64734518528, Accuracy = 0.2043440938\n",
      "Iter #183040:  Learning rate = 0.004800:   Batch Loss = 1.656510, Accuracy = 0.28749999404\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.64708983898, Accuracy = 0.2043440938\n",
      "Iter #183680:  Learning rate = 0.004800:   Batch Loss = 1.668323, Accuracy = 0.21250000596\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.64681696892, Accuracy = 0.2043440938\n",
      "Iter #184320:  Learning rate = 0.004800:   Batch Loss = 1.607740, Accuracy = 0.25\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.64506876469, Accuracy = 0.286939114332\n",
      "Iter #184960:  Learning rate = 0.004800:   Batch Loss = 1.661197, Accuracy = 0.17499999702\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.64867067337, Accuracy = 0.2043440938\n",
      "Iter #185600:  Learning rate = 0.004800:   Batch Loss = 1.680568, Accuracy = 0.22499999404\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.64407002926, Accuracy = 0.286939114332\n",
      "Iter #186240:  Learning rate = 0.004800:   Batch Loss = 1.613894, Accuracy = 0.20000000298\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.64431738853, Accuracy = 0.2043440938\n",
      "Iter #186880:  Learning rate = 0.004800:   Batch Loss = 1.632255, Accuracy = 0.300000011921\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.6494166851, Accuracy = 0.2043440938\n",
      "Iter #187520:  Learning rate = 0.004800:   Batch Loss = 1.642613, Accuracy = 0.262499988079\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.64705967903, Accuracy = 0.2043440938\n",
      "Iter #188160:  Learning rate = 0.004800:   Batch Loss = 1.645313, Accuracy = 0.22499999404\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.64332234859, Accuracy = 0.2043440938\n",
      "Iter #188800:  Learning rate = 0.004800:   Batch Loss = 1.636808, Accuracy = 0.324999988079\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.64375686646, Accuracy = 0.2043440938\n",
      "Iter #189440:  Learning rate = 0.004800:   Batch Loss = 1.648106, Accuracy = 0.21250000596\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.64728927612, Accuracy = 0.2043440938\n",
      "Iter #190080:  Learning rate = 0.004800:   Batch Loss = 1.660358, Accuracy = 0.20000000298\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.63898217678, Accuracy = 0.286939114332\n",
      "Iter #190720:  Learning rate = 0.004800:   Batch Loss = 1.628205, Accuracy = 0.27500000596\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.64167571068, Accuracy = 0.2043440938\n",
      "Iter #191360:  Learning rate = 0.004800:   Batch Loss = 1.662389, Accuracy = 0.300000011921\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.64645349979, Accuracy = 0.2043440938\n",
      "Iter #192000:  Learning rate = 0.004800:   Batch Loss = 1.637828, Accuracy = 0.25\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.64358580112, Accuracy = 0.2043440938\n",
      "Iter #192640:  Learning rate = 0.004800:   Batch Loss = 1.647222, Accuracy = 0.25\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.63700377941, Accuracy = 0.286939114332\n",
      "Iter #193280:  Learning rate = 0.004800:   Batch Loss = 1.620257, Accuracy = 0.28749999404\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.64014983177, Accuracy = 0.2043440938\n",
      "Iter #193920:  Learning rate = 0.004800:   Batch Loss = 1.715704, Accuracy = 0.16249999404\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.63783097267, Accuracy = 0.286939114332\n",
      "Iter #194560:  Learning rate = 0.004800:   Batch Loss = 1.653686, Accuracy = 0.23749999702\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.64172923565, Accuracy = 0.2043440938\n",
      "Iter #195200:  Learning rate = 0.004800:   Batch Loss = 1.610373, Accuracy = 0.27500000596\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.64234745502, Accuracy = 0.2043440938\n",
      "Iter #195840:  Learning rate = 0.004800:   Batch Loss = 1.634853, Accuracy = 0.262499988079\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.64123821259, Accuracy = 0.2043440938\n",
      "Iter #196480:  Learning rate = 0.004800:   Batch Loss = 1.610208, Accuracy = 0.27500000596\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.63609826565, Accuracy = 0.286939114332\n",
      "Iter #197120:  Learning rate = 0.004800:   Batch Loss = 1.609336, Accuracy = 0.3125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.6340521574, Accuracy = 0.286939114332\n",
      "Iter #197760:  Learning rate = 0.004800:   Batch Loss = 1.670846, Accuracy = 0.20000000298\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.63757860661, Accuracy = 0.2043440938\n",
      "Iter #198400:  Learning rate = 0.004800:   Batch Loss = 1.636768, Accuracy = 0.3125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.63793992996, Accuracy = 0.2043440938\n",
      "Iter #199040:  Learning rate = 0.004800:   Batch Loss = 1.670591, Accuracy = 0.21250000596\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.63466334343, Accuracy = 0.2043440938\n",
      "Iter #199680:  Learning rate = 0.004800:   Batch Loss = 1.647825, Accuracy = 0.22499999404\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.63574421406, Accuracy = 0.2043440938\n",
      "Iter #200320:  Learning rate = 0.004608:   Batch Loss = 1.659422, Accuracy = 0.27500000596\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.63304257393, Accuracy = 0.286939114332\n",
      "Iter #200960:  Learning rate = 0.004608:   Batch Loss = 1.583833, Accuracy = 0.300000011921\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.63428902626, Accuracy = 0.2043440938\n",
      "Iter #201600:  Learning rate = 0.004608:   Batch Loss = 1.631902, Accuracy = 0.300000011921\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.63770198822, Accuracy = 0.2043440938\n",
      "Iter #202240:  Learning rate = 0.004608:   Batch Loss = 1.605952, Accuracy = 0.28749999404\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.64095211029, Accuracy = 0.2043440938\n",
      "Iter #202880:  Learning rate = 0.004608:   Batch Loss = 1.625652, Accuracy = 0.23749999702\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.63375544548, Accuracy = 0.2043440938\n",
      "Iter #203520:  Learning rate = 0.004608:   Batch Loss = 1.580697, Accuracy = 0.28749999404\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.63273775578, Accuracy = 0.286939114332\n",
      "Iter #204160:  Learning rate = 0.004608:   Batch Loss = 1.616228, Accuracy = 0.387499988079\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.63463592529, Accuracy = 0.2043440938\n",
      "Iter #204800:  Learning rate = 0.004608:   Batch Loss = 1.576584, Accuracy = 0.300000011921\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.64561975002, Accuracy = 0.2043440938\n",
      "Iter #205440:  Learning rate = 0.004608:   Batch Loss = 1.585320, Accuracy = 0.3125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.64530599117, Accuracy = 0.2043440938\n",
      "Iter #206080:  Learning rate = 0.004608:   Batch Loss = 1.625362, Accuracy = 0.22499999404\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.63982152939, Accuracy = 0.2043440938\n",
      "Iter #206720:  Learning rate = 0.004608:   Batch Loss = 1.615648, Accuracy = 0.27500000596\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.63006711006, Accuracy = 0.286939114332\n",
      "Iter #207360:  Learning rate = 0.004608:   Batch Loss = 1.641616, Accuracy = 0.21250000596\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.62668955326, Accuracy = 0.286939114332\n",
      "Iter #208000:  Learning rate = 0.004608:   Batch Loss = 1.632983, Accuracy = 0.262499988079\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.62632250786, Accuracy = 0.286939114332\n",
      "Iter #208640:  Learning rate = 0.004608:   Batch Loss = 1.604516, Accuracy = 0.27500000596\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.62656891346, Accuracy = 0.286939114332\n",
      "Iter #209280:  Learning rate = 0.004608:   Batch Loss = 1.566666, Accuracy = 0.23749999702\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.63035237789, Accuracy = 0.2043440938\n",
      "Iter #209920:  Learning rate = 0.004608:   Batch Loss = 1.636673, Accuracy = 0.21250000596\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.638854146, Accuracy = 0.2043440938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter #210560:  Learning rate = 0.004608:   Batch Loss = 1.607846, Accuracy = 0.27500000596\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.63725030422, Accuracy = 0.2043440938\n",
      "Iter #211200:  Learning rate = 0.004608:   Batch Loss = 1.668512, Accuracy = 0.22499999404\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.62717080116, Accuracy = 0.286939114332\n",
      "Iter #211840:  Learning rate = 0.004608:   Batch Loss = 1.628625, Accuracy = 0.23749999702\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.62622630596, Accuracy = 0.286939114332\n",
      "Iter #212480:  Learning rate = 0.004608:   Batch Loss = 1.637464, Accuracy = 0.28749999404\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.62728619576, Accuracy = 0.286939114332\n",
      "Iter #213120:  Learning rate = 0.004608:   Batch Loss = 1.599149, Accuracy = 0.21250000596\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.62877655029, Accuracy = 0.2043440938\n",
      "Iter #213760:  Learning rate = 0.004608:   Batch Loss = 1.629432, Accuracy = 0.27500000596\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.6310261488, Accuracy = 0.2043440938\n",
      "Iter #214400:  Learning rate = 0.004608:   Batch Loss = 1.603577, Accuracy = 0.262499988079\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.63366472721, Accuracy = 0.2043440938\n",
      "Iter #215040:  Learning rate = 0.004608:   Batch Loss = 1.625002, Accuracy = 0.3125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.63064539433, Accuracy = 0.2043440938\n",
      "Iter #215680:  Learning rate = 0.004608:   Batch Loss = 1.640890, Accuracy = 0.27500000596\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.63189256191, Accuracy = 0.2043440938\n",
      "Iter #216320:  Learning rate = 0.004608:   Batch Loss = 1.689857, Accuracy = 0.17499999702\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.63037121296, Accuracy = 0.2043440938\n",
      "Iter #216960:  Learning rate = 0.004608:   Batch Loss = 1.635680, Accuracy = 0.23749999702\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.62901043892, Accuracy = 0.2043440938\n",
      "Iter #217600:  Learning rate = 0.004608:   Batch Loss = 1.597979, Accuracy = 0.28749999404\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.63094234467, Accuracy = 0.2043440938\n",
      "Iter #218240:  Learning rate = 0.004608:   Batch Loss = 1.603317, Accuracy = 0.262499988079\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.62873446941, Accuracy = 0.2043440938\n",
      "Iter #218880:  Learning rate = 0.004608:   Batch Loss = 1.639965, Accuracy = 0.28749999404\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.62499260902, Accuracy = 0.286939114332\n",
      "Iter #219520:  Learning rate = 0.004608:   Batch Loss = 1.586080, Accuracy = 0.27500000596\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.62474107742, Accuracy = 0.2043440938\n",
      "Iter #220160:  Learning rate = 0.004608:   Batch Loss = 1.631683, Accuracy = 0.25\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.62447071075, Accuracy = 0.2043440938\n",
      "Iter #220800:  Learning rate = 0.004608:   Batch Loss = 1.595688, Accuracy = 0.21250000596\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.62450015545, Accuracy = 0.2043440938\n",
      "Iter #221440:  Learning rate = 0.004608:   Batch Loss = 1.647099, Accuracy = 0.21250000596\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.62370026112, Accuracy = 0.286939114332\n",
      "Iter #222080:  Learning rate = 0.004608:   Batch Loss = 1.678121, Accuracy = 0.21250000596\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.6262472868, Accuracy = 0.2043440938\n",
      "Iter #222720:  Learning rate = 0.004608:   Batch Loss = 1.595295, Accuracy = 0.28749999404\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.62659788132, Accuracy = 0.2043440938\n",
      "Iter #223360:  Learning rate = 0.004608:   Batch Loss = 1.617872, Accuracy = 0.23749999702\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.62487757206, Accuracy = 0.2043440938\n",
      "Iter #224000:  Learning rate = 0.004608:   Batch Loss = 1.607256, Accuracy = 0.23749999702\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.6247677803, Accuracy = 0.2043440938\n",
      "Iter #224640:  Learning rate = 0.004608:   Batch Loss = 1.616238, Accuracy = 0.262499988079\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.62326848507, Accuracy = 0.2043440938\n",
      "Iter #225280:  Learning rate = 0.004608:   Batch Loss = 1.630681, Accuracy = 0.27500000596\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.62436664104, Accuracy = 0.2043440938\n",
      "Iter #225920:  Learning rate = 0.004608:   Batch Loss = 1.647422, Accuracy = 0.20000000298\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.6263076067, Accuracy = 0.2043440938\n",
      "Iter #226560:  Learning rate = 0.004608:   Batch Loss = 1.636863, Accuracy = 0.21250000596\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.62436461449, Accuracy = 0.2043440938\n",
      "Iter #227200:  Learning rate = 0.004608:   Batch Loss = 1.650985, Accuracy = 0.1875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.6252682209, Accuracy = 0.2043440938\n",
      "Iter #227840:  Learning rate = 0.004608:   Batch Loss = 1.663402, Accuracy = 0.25\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.62526500225, Accuracy = 0.2043440938\n",
      "Iter #228480:  Learning rate = 0.004608:   Batch Loss = 1.584973, Accuracy = 0.262499988079\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.61958813667, Accuracy = 0.286939114332\n",
      "Iter #229120:  Learning rate = 0.004608:   Batch Loss = 1.628648, Accuracy = 0.262499988079\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.620408535, Accuracy = 0.286939114332\n",
      "Iter #229760:  Learning rate = 0.004608:   Batch Loss = 1.653961, Accuracy = 0.20000000298\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.62326824665, Accuracy = 0.2043440938\n",
      "Iter #230400:  Learning rate = 0.004608:   Batch Loss = 1.610005, Accuracy = 0.25\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.62124538422, Accuracy = 0.2043440938\n",
      "Iter #231040:  Learning rate = 0.004608:   Batch Loss = 1.624222, Accuracy = 0.23749999702\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.62039136887, Accuracy = 0.286939114332\n",
      "Iter #231680:  Learning rate = 0.004608:   Batch Loss = 1.598793, Accuracy = 0.300000011921\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.62367939949, Accuracy = 0.2043440938\n",
      "Iter #232320:  Learning rate = 0.004608:   Batch Loss = 1.596887, Accuracy = 0.3125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.62855684757, Accuracy = 0.2043440938\n",
      "Iter #232960:  Learning rate = 0.004608:   Batch Loss = 1.622435, Accuracy = 0.25\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.63005280495, Accuracy = 0.2043440938\n",
      "Iter #233600:  Learning rate = 0.004608:   Batch Loss = 1.607263, Accuracy = 0.262499988079\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.62371397018, Accuracy = 0.2043440938\n",
      "Iter #234240:  Learning rate = 0.004608:   Batch Loss = 1.591942, Accuracy = 0.25\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.61837792397, Accuracy = 0.286939114332\n",
      "Iter #234880:  Learning rate = 0.004608:   Batch Loss = 1.605071, Accuracy = 0.25\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.61702108383, Accuracy = 0.286939114332\n",
      "Iter #235520:  Learning rate = 0.004608:   Batch Loss = 1.642052, Accuracy = 0.20000000298\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.61777722836, Accuracy = 0.286939114332\n",
      "Iter #236160:  Learning rate = 0.004608:   Batch Loss = 1.650421, Accuracy = 0.21250000596\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.61918795109, Accuracy = 0.2043440938\n",
      "Iter #236800:  Learning rate = 0.004608:   Batch Loss = 1.608179, Accuracy = 0.27500000596\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.62265014648, Accuracy = 0.2043440938\n",
      "Iter #237440:  Learning rate = 0.004608:   Batch Loss = 1.630589, Accuracy = 0.27500000596\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.62948739529, Accuracy = 0.2043440938\n",
      "Iter #238080:  Learning rate = 0.004608:   Batch Loss = 1.616339, Accuracy = 0.28749999404\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.63099205494, Accuracy = 0.2043440938\n",
      "Iter #238720:  Learning rate = 0.004608:   Batch Loss = 1.577656, Accuracy = 0.324999988079\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.6229442358, Accuracy = 0.2043440938\n",
      "Iter #239360:  Learning rate = 0.004608:   Batch Loss = 1.580493, Accuracy = 0.3125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.61520075798, Accuracy = 0.286939114332\n",
      "Iter #240000:  Learning rate = 0.004608:   Batch Loss = 1.596817, Accuracy = 0.27500000596\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.61434018612, Accuracy = 0.286939114332\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter #240640:  Learning rate = 0.004608:   Batch Loss = 1.616629, Accuracy = 0.27500000596\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.6191380024, Accuracy = 0.2043440938\n",
      "Iter #241280:  Learning rate = 0.004608:   Batch Loss = 1.633999, Accuracy = 0.21250000596\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.62334501743, Accuracy = 0.2043440938\n",
      "Iter #241920:  Learning rate = 0.004608:   Batch Loss = 1.591284, Accuracy = 0.262499988079\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.62534713745, Accuracy = 0.2043440938\n",
      "Iter #242560:  Learning rate = 0.004608:   Batch Loss = 1.587418, Accuracy = 0.324999988079\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.62008738518, Accuracy = 0.2043440938\n",
      "Iter #243200:  Learning rate = 0.004608:   Batch Loss = 1.591637, Accuracy = 0.300000011921\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.61794734001, Accuracy = 0.2043440938\n",
      "Iter #243840:  Learning rate = 0.004608:   Batch Loss = 1.592257, Accuracy = 0.28749999404\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.61860525608, Accuracy = 0.2043440938\n",
      "Iter #244480:  Learning rate = 0.004608:   Batch Loss = 1.602610, Accuracy = 0.17499999702\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.61748337746, Accuracy = 0.2043440938\n",
      "Iter #245120:  Learning rate = 0.004608:   Batch Loss = 1.620244, Accuracy = 0.300000011921\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.61624097824, Accuracy = 0.2043440938\n",
      "Iter #245760:  Learning rate = 0.004608:   Batch Loss = 1.611065, Accuracy = 0.28749999404\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.61855959892, Accuracy = 0.2043440938\n",
      "Iter #246400:  Learning rate = 0.004608:   Batch Loss = 1.614087, Accuracy = 0.20000000298\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.61864411831, Accuracy = 0.2043440938\n",
      "Iter #247040:  Learning rate = 0.004608:   Batch Loss = 1.610142, Accuracy = 0.300000011921\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.61619877815, Accuracy = 0.2043440938\n",
      "Iter #247680:  Learning rate = 0.004608:   Batch Loss = 1.612941, Accuracy = 0.3125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.61322391033, Accuracy = 0.286939114332\n",
      "Iter #248320:  Learning rate = 0.004608:   Batch Loss = 1.611006, Accuracy = 0.21250000596\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.61170053482, Accuracy = 0.286939114332\n",
      "Iter #248960:  Learning rate = 0.004608:   Batch Loss = 1.622345, Accuracy = 0.25\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.61238753796, Accuracy = 0.286939114332\n",
      "Iter #249600:  Learning rate = 0.004608:   Batch Loss = 1.609529, Accuracy = 0.3125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.61524260044, Accuracy = 0.2043440938\n",
      "Iter #250240:  Learning rate = 0.004608:   Batch Loss = 1.609663, Accuracy = 0.25\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.61606311798, Accuracy = 0.2043440938\n",
      "Iter #250880:  Learning rate = 0.004608:   Batch Loss = 1.606113, Accuracy = 0.23749999702\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.61654949188, Accuracy = 0.2043440938\n",
      "Iter #251520:  Learning rate = 0.004608:   Batch Loss = 1.646548, Accuracy = 0.15000000596\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.61656951904, Accuracy = 0.2043440938\n",
      "Iter #252160:  Learning rate = 0.004608:   Batch Loss = 1.610618, Accuracy = 0.20000000298\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.61562144756, Accuracy = 0.2043440938\n",
      "Iter #252800:  Learning rate = 0.004608:   Batch Loss = 1.554093, Accuracy = 0.324999988079\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.61500883102, Accuracy = 0.2043440938\n",
      "Iter #253440:  Learning rate = 0.004608:   Batch Loss = 1.617017, Accuracy = 0.25\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.61538255215, Accuracy = 0.2043440938\n",
      "Iter #254080:  Learning rate = 0.004608:   Batch Loss = 1.597334, Accuracy = 0.27500000596\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.61468553543, Accuracy = 0.2043440938\n",
      "Iter #254720:  Learning rate = 0.004608:   Batch Loss = 1.593190, Accuracy = 0.22499999404\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.61495184898, Accuracy = 0.2043440938\n",
      "Iter #255360:  Learning rate = 0.004608:   Batch Loss = 1.554035, Accuracy = 0.324999988079\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.61518871784, Accuracy = 0.2043440938\n",
      "Iter #256000:  Learning rate = 0.004608:   Batch Loss = 1.615026, Accuracy = 0.22499999404\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.61404407024, Accuracy = 0.2043440938\n",
      "Iter #256640:  Learning rate = 0.004608:   Batch Loss = 1.571135, Accuracy = 0.324999988079\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.61335873604, Accuracy = 0.2043440938\n",
      "Iter #257280:  Learning rate = 0.004608:   Batch Loss = 1.624757, Accuracy = 0.25\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.61363220215, Accuracy = 0.2043440938\n",
      "Iter #257920:  Learning rate = 0.004608:   Batch Loss = 1.589378, Accuracy = 0.262499988079\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.61362171173, Accuracy = 0.2043440938\n",
      "Iter #258560:  Learning rate = 0.004608:   Batch Loss = 1.654932, Accuracy = 0.23749999702\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.61386847496, Accuracy = 0.2043440938\n",
      "Iter #259200:  Learning rate = 0.004608:   Batch Loss = 1.589535, Accuracy = 0.25\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.61399424076, Accuracy = 0.2043440938\n",
      "Iter #259840:  Learning rate = 0.004608:   Batch Loss = 1.606684, Accuracy = 0.21250000596\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.61424338818, Accuracy = 0.2043440938\n",
      "Iter #260480:  Learning rate = 0.004608:   Batch Loss = 1.645563, Accuracy = 0.23749999702\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.61387348175, Accuracy = 0.2043440938\n",
      "Iter #261120:  Learning rate = 0.004608:   Batch Loss = 1.592635, Accuracy = 0.300000011921\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.61382389069, Accuracy = 0.2043440938\n",
      "Iter #261760:  Learning rate = 0.004608:   Batch Loss = 1.617303, Accuracy = 0.25\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.61447203159, Accuracy = 0.2043440938\n",
      "Iter #262400:  Learning rate = 0.004608:   Batch Loss = 1.570383, Accuracy = 0.28749999404\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.61241698265, Accuracy = 0.2043440938\n",
      "Iter #263040:  Learning rate = 0.004608:   Batch Loss = 1.636750, Accuracy = 0.27500000596\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.61247169971, Accuracy = 0.2043440938\n",
      "Iter #263680:  Learning rate = 0.004608:   Batch Loss = 1.617842, Accuracy = 0.23749999702\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.6121712923, Accuracy = 0.2043440938\n",
      "Iter #264320:  Learning rate = 0.004608:   Batch Loss = 1.609132, Accuracy = 0.262499988079\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.61240589619, Accuracy = 0.2043440938\n",
      "Iter #264960:  Learning rate = 0.004608:   Batch Loss = 1.657861, Accuracy = 0.1875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.61278808117, Accuracy = 0.2043440938\n",
      "Iter #265600:  Learning rate = 0.004608:   Batch Loss = 1.586105, Accuracy = 0.324999988079\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.61243867874, Accuracy = 0.2043440938\n",
      "Iter #266240:  Learning rate = 0.004608:   Batch Loss = 1.644535, Accuracy = 0.25\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.61211121082, Accuracy = 0.2043440938\n",
      "Iter #266880:  Learning rate = 0.004608:   Batch Loss = 1.589476, Accuracy = 0.25\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.61022984982, Accuracy = 0.286939114332\n",
      "Iter #267520:  Learning rate = 0.004608:   Batch Loss = 1.641672, Accuracy = 0.15000000596\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.60902714729, Accuracy = 0.286939114332\n",
      "Iter #268160:  Learning rate = 0.004608:   Batch Loss = 1.603674, Accuracy = 0.22499999404\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.60909366608, Accuracy = 0.286939114332\n",
      "Iter #268800:  Learning rate = 0.004608:   Batch Loss = 1.625577, Accuracy = 0.25\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.61203789711, Accuracy = 0.2043440938\n",
      "Iter #269440:  Learning rate = 0.004608:   Batch Loss = 1.571088, Accuracy = 0.28749999404\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.61139965057, Accuracy = 0.2043440938\n",
      "Iter #270080:  Learning rate = 0.004608:   Batch Loss = 1.630553, Accuracy = 0.23749999702\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.61081898212, Accuracy = 0.2043440938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter #270720:  Learning rate = 0.004608:   Batch Loss = 1.563896, Accuracy = 0.3125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.61094212532, Accuracy = 0.2043440938\n",
      "Iter #271360:  Learning rate = 0.004608:   Batch Loss = 1.577343, Accuracy = 0.262499988079\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.61129355431, Accuracy = 0.2043440938\n",
      "Iter #272000:  Learning rate = 0.004608:   Batch Loss = 1.607161, Accuracy = 0.27500000596\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.60991728306, Accuracy = 0.2043440938\n",
      "Iter #272640:  Learning rate = 0.004608:   Batch Loss = 1.614900, Accuracy = 0.25\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.61346828938, Accuracy = 0.2043440938\n",
      "Iter #273280:  Learning rate = 0.004608:   Batch Loss = 1.569150, Accuracy = 0.27500000596\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.61282861233, Accuracy = 0.2043440938\n",
      "Iter #273920:  Learning rate = 0.004608:   Batch Loss = 1.531588, Accuracy = 0.33750000596\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.61166238785, Accuracy = 0.2043440938\n",
      "Iter #274560:  Learning rate = 0.004608:   Batch Loss = 1.595372, Accuracy = 0.3125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.61057221889, Accuracy = 0.2043440938\n",
      "Iter #275200:  Learning rate = 0.004608:   Batch Loss = 1.605180, Accuracy = 0.28749999404\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.61030578613, Accuracy = 0.2043440938\n",
      "Iter #275840:  Learning rate = 0.004608:   Batch Loss = 1.573781, Accuracy = 0.375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.61025547981, Accuracy = 0.2043440938\n",
      "Iter #276480:  Learning rate = 0.004608:   Batch Loss = 1.649112, Accuracy = 0.17499999702\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.61006689072, Accuracy = 0.2043440938\n",
      "Iter #277120:  Learning rate = 0.004608:   Batch Loss = 1.588182, Accuracy = 0.27500000596\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.60862636566, Accuracy = 0.2043440938\n",
      "Iter #277760:  Learning rate = 0.004608:   Batch Loss = 1.652011, Accuracy = 0.1875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.60811316967, Accuracy = 0.2043440938\n",
      "Iter #278400:  Learning rate = 0.004608:   Batch Loss = 1.616761, Accuracy = 0.27500000596\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.60776543617, Accuracy = 0.2043440938\n",
      "Iter #279040:  Learning rate = 0.004608:   Batch Loss = 1.578924, Accuracy = 0.27500000596\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.60856318474, Accuracy = 0.2043440938\n",
      "Iter #279680:  Learning rate = 0.004608:   Batch Loss = 1.607985, Accuracy = 0.25\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.60883080959, Accuracy = 0.2043440938\n",
      "Iter #280320:  Learning rate = 0.004608:   Batch Loss = 1.575318, Accuracy = 0.28749999404\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.60835134983, Accuracy = 0.2043440938\n",
      "Iter #280960:  Learning rate = 0.004608:   Batch Loss = 1.659838, Accuracy = 0.27500000596\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.60891616344, Accuracy = 0.2043440938\n",
      "Iter #281600:  Learning rate = 0.004608:   Batch Loss = 1.591556, Accuracy = 0.25\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.61011886597, Accuracy = 0.2043440938\n",
      "Iter #282240:  Learning rate = 0.004608:   Batch Loss = 1.578379, Accuracy = 0.27500000596\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.61077237129, Accuracy = 0.2043440938\n",
      "Iter #282880:  Learning rate = 0.004608:   Batch Loss = 1.615956, Accuracy = 0.262499988079\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.61038780212, Accuracy = 0.2043440938\n",
      "Iter #283520:  Learning rate = 0.004608:   Batch Loss = 1.596337, Accuracy = 0.25\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.60913097858, Accuracy = 0.2043440938\n",
      "Iter #284160:  Learning rate = 0.004608:   Batch Loss = 1.619140, Accuracy = 0.20000000298\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.60984706879, Accuracy = 0.2043440938\n",
      "Iter #284800:  Learning rate = 0.004608:   Batch Loss = 1.568442, Accuracy = 0.3125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.60867393017, Accuracy = 0.2043440938\n",
      "Iter #285440:  Learning rate = 0.004608:   Batch Loss = 1.623675, Accuracy = 0.20000000298\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.60721707344, Accuracy = 0.2043440938\n",
      "Iter #286080:  Learning rate = 0.004608:   Batch Loss = 1.621722, Accuracy = 0.1875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.60474824905, Accuracy = 0.286939114332\n",
      "Iter #286720:  Learning rate = 0.004608:   Batch Loss = 1.565879, Accuracy = 0.300000011921\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.60454761982, Accuracy = 0.286939114332\n",
      "Iter #287360:  Learning rate = 0.004608:   Batch Loss = 1.598573, Accuracy = 0.28749999404\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.60575771332, Accuracy = 0.2043440938\n",
      "Iter #288000:  Learning rate = 0.004608:   Batch Loss = 1.627843, Accuracy = 0.16249999404\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.60676026344, Accuracy = 0.2043440938\n",
      "Iter #288640:  Learning rate = 0.004608:   Batch Loss = 1.631306, Accuracy = 0.25\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.60563349724, Accuracy = 0.2043440938\n",
      "Iter #289280:  Learning rate = 0.004608:   Batch Loss = 1.582961, Accuracy = 0.27500000596\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.60417330265, Accuracy = 0.286939114332\n",
      "Iter #289920:  Learning rate = 0.004608:   Batch Loss = 1.583835, Accuracy = 0.23749999702\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.60534262657, Accuracy = 0.2043440938\n",
      "Iter #290560:  Learning rate = 0.004608:   Batch Loss = 1.575386, Accuracy = 0.300000011921\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.60640931129, Accuracy = 0.2043440938\n",
      "Iter #291200:  Learning rate = 0.004608:   Batch Loss = 1.563479, Accuracy = 0.27500000596\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.60815310478, Accuracy = 0.2043440938\n",
      "Iter #291840:  Learning rate = 0.004608:   Batch Loss = 1.591922, Accuracy = 0.22499999404\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.60688400269, Accuracy = 0.2043440938\n",
      "Iter #292480:  Learning rate = 0.004608:   Batch Loss = 1.610443, Accuracy = 0.25\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.60697519779, Accuracy = 0.2043440938\n",
      "Iter #293120:  Learning rate = 0.004608:   Batch Loss = 1.584219, Accuracy = 0.27500000596\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.60784864426, Accuracy = 0.2043440938\n",
      "Iter #293760:  Learning rate = 0.004608:   Batch Loss = 1.615330, Accuracy = 0.25\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.60769605637, Accuracy = 0.2043440938\n",
      "Iter #294400:  Learning rate = 0.004608:   Batch Loss = 1.571391, Accuracy = 0.300000011921\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.60823893547, Accuracy = 0.2043440938\n",
      "Iter #295040:  Learning rate = 0.004608:   Batch Loss = 1.566968, Accuracy = 0.27500000596\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.60784304142, Accuracy = 0.2043440938\n",
      "Iter #295680:  Learning rate = 0.004608:   Batch Loss = 1.581883, Accuracy = 0.27500000596\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.60692405701, Accuracy = 0.2043440938\n",
      "Iter #296320:  Learning rate = 0.004608:   Batch Loss = 1.595964, Accuracy = 0.262499988079\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.6061848402, Accuracy = 0.2043440938\n",
      "Iter #296960:  Learning rate = 0.004608:   Batch Loss = 1.611694, Accuracy = 0.25\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.60511386395, Accuracy = 0.2043440938\n",
      "Iter #297600:  Learning rate = 0.004608:   Batch Loss = 1.608034, Accuracy = 0.25\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.60384273529, Accuracy = 0.286939114332\n",
      "Iter #298240:  Learning rate = 0.004608:   Batch Loss = 1.555635, Accuracy = 0.262499988079\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.60385477543, Accuracy = 0.286939114332\n",
      "Iter #298880:  Learning rate = 0.004608:   Batch Loss = 1.586264, Accuracy = 0.25\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.60450649261, Accuracy = 0.2043440938\n",
      "Iter #299520:  Learning rate = 0.004608:   Batch Loss = 1.652864, Accuracy = 0.22499999404\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.60420012474, Accuracy = 0.2043440938\n",
      "Iter #300160:  Learning rate = 0.004424:   Batch Loss = 1.637831, Accuracy = 0.1875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.6034348011, Accuracy = 0.2043440938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter #300800:  Learning rate = 0.004424:   Batch Loss = 1.599100, Accuracy = 0.22499999404\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.60276675224, Accuracy = 0.286939114332\n",
      "Iter #301440:  Learning rate = 0.004424:   Batch Loss = 1.603058, Accuracy = 0.21250000596\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.60250866413, Accuracy = 0.286939114332\n",
      "Iter #302080:  Learning rate = 0.004424:   Batch Loss = 1.587569, Accuracy = 0.27500000596\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.60316073895, Accuracy = 0.2043440938\n",
      "Iter #302720:  Learning rate = 0.004424:   Batch Loss = 1.575625, Accuracy = 0.25\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.60293161869, Accuracy = 0.286939114332\n",
      "Iter #303360:  Learning rate = 0.004424:   Batch Loss = 1.640352, Accuracy = 0.15000000596\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.60309886932, Accuracy = 0.286939114332\n",
      "Iter #304000:  Learning rate = 0.004424:   Batch Loss = 1.607997, Accuracy = 0.28749999404\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.60493886471, Accuracy = 0.2043440938\n",
      "Iter #304640:  Learning rate = 0.004424:   Batch Loss = 1.558002, Accuracy = 0.300000011921\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.60679805279, Accuracy = 0.2043440938\n",
      "Iter #305280:  Learning rate = 0.004424:   Batch Loss = 1.602263, Accuracy = 0.23749999702\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.60700035095, Accuracy = 0.2043440938\n",
      "Iter #305920:  Learning rate = 0.004424:   Batch Loss = 1.577719, Accuracy = 0.262499988079\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.60549032688, Accuracy = 0.2043440938\n",
      "Iter #306560:  Learning rate = 0.004424:   Batch Loss = 1.579330, Accuracy = 0.25\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.60342919827, Accuracy = 0.2043440938\n",
      "Iter #307200:  Learning rate = 0.004424:   Batch Loss = 1.601230, Accuracy = 0.28749999404\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.60398340225, Accuracy = 0.2043440938\n",
      "Iter #307840:  Learning rate = 0.004424:   Batch Loss = 1.564089, Accuracy = 0.25\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.60334610939, Accuracy = 0.2043440938\n",
      "Iter #308480:  Learning rate = 0.004424:   Batch Loss = 1.601724, Accuracy = 0.300000011921\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.60268867016, Accuracy = 0.2043440938\n",
      "Iter #309120:  Learning rate = 0.004424:   Batch Loss = 1.625476, Accuracy = 0.25\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.60358011723, Accuracy = 0.2043440938\n",
      "Iter #309760:  Learning rate = 0.004424:   Batch Loss = 1.602165, Accuracy = 0.300000011921\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.60393249989, Accuracy = 0.2043440938\n",
      "Iter #310400:  Learning rate = 0.004424:   Batch Loss = 1.598890, Accuracy = 0.25\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.60305440426, Accuracy = 0.2043440938\n",
      "Iter #311040:  Learning rate = 0.004424:   Batch Loss = 1.610361, Accuracy = 0.27500000596\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.6031306982, Accuracy = 0.2043440938\n",
      "Iter #311680:  Learning rate = 0.004424:   Batch Loss = 1.605537, Accuracy = 0.300000011921\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.60283327103, Accuracy = 0.2043440938\n",
      "Iter #312320:  Learning rate = 0.004424:   Batch Loss = 1.615552, Accuracy = 0.28749999404\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.60248291492, Accuracy = 0.2043440938\n",
      "Iter #312960:  Learning rate = 0.004424:   Batch Loss = 1.628522, Accuracy = 0.23749999702\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.60327768326, Accuracy = 0.2043440938\n",
      "Iter #313600:  Learning rate = 0.004424:   Batch Loss = 1.530804, Accuracy = 0.33750000596\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.60439097881, Accuracy = 0.2043440938\n",
      "Iter #314240:  Learning rate = 0.004424:   Batch Loss = 1.573803, Accuracy = 0.27500000596\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.60445642471, Accuracy = 0.2043440938\n",
      "Iter #314880:  Learning rate = 0.004424:   Batch Loss = 1.616447, Accuracy = 0.262499988079\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.60469317436, Accuracy = 0.2043440938\n",
      "Iter #315520:  Learning rate = 0.004424:   Batch Loss = 1.577384, Accuracy = 0.262499988079\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.60421812534, Accuracy = 0.2043440938\n",
      "Iter #316160:  Learning rate = 0.004424:   Batch Loss = 1.585524, Accuracy = 0.25\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.60418307781, Accuracy = 0.2043440938\n",
      "Iter #316800:  Learning rate = 0.004424:   Batch Loss = 1.573478, Accuracy = 0.23749999702\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.60394454002, Accuracy = 0.2043440938\n",
      "Iter #317440:  Learning rate = 0.004424:   Batch Loss = 1.598143, Accuracy = 0.20000000298\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.6036632061, Accuracy = 0.2043440938\n",
      "Iter #318080:  Learning rate = 0.004424:   Batch Loss = 1.581985, Accuracy = 0.23749999702\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.60302519798, Accuracy = 0.2043440938\n",
      "Iter #318720:  Learning rate = 0.004424:   Batch Loss = 1.627671, Accuracy = 0.23749999702\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.60236358643, Accuracy = 0.2043440938\n",
      "Iter #319360:  Learning rate = 0.004424:   Batch Loss = 1.623196, Accuracy = 0.16249999404\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.6016150713, Accuracy = 0.2043440938\n",
      "Iter #320000:  Learning rate = 0.004424:   Batch Loss = 1.596112, Accuracy = 0.20000000298\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.60081124306, Accuracy = 0.2043440938\n",
      "Iter #320640:  Learning rate = 0.004424:   Batch Loss = 1.604759, Accuracy = 0.23749999702\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.60136079788, Accuracy = 0.2043440938\n",
      "Iter #321280:  Learning rate = 0.004424:   Batch Loss = 1.637937, Accuracy = 0.16249999404\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.60254478455, Accuracy = 0.2043440938\n",
      "Iter #321920:  Learning rate = 0.004424:   Batch Loss = 1.637106, Accuracy = 0.20000000298\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.60336720943, Accuracy = 0.2043440938\n",
      "Iter #322560:  Learning rate = 0.004424:   Batch Loss = 1.605914, Accuracy = 0.21250000596\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.60362100601, Accuracy = 0.2043440938\n",
      "Iter #323200:  Learning rate = 0.004424:   Batch Loss = 1.594090, Accuracy = 0.324999988079\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.60332107544, Accuracy = 0.2043440938\n",
      "Iter #323840:  Learning rate = 0.004424:   Batch Loss = 1.564988, Accuracy = 0.324999988079\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.60349178314, Accuracy = 0.2043440938\n",
      "Iter #324480:  Learning rate = 0.004424:   Batch Loss = 1.588653, Accuracy = 0.300000011921\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.60340130329, Accuracy = 0.2043440938\n",
      "Iter #325120:  Learning rate = 0.004424:   Batch Loss = 1.612812, Accuracy = 0.20000000298\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.60353040695, Accuracy = 0.2043440938\n",
      "Iter #325760:  Learning rate = 0.004424:   Batch Loss = 1.598133, Accuracy = 0.20000000298\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.60438871384, Accuracy = 0.2043440938\n",
      "Iter #326400:  Learning rate = 0.004424:   Batch Loss = 1.605983, Accuracy = 0.20000000298\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.60326600075, Accuracy = 0.2043440938\n",
      "Iter #327040:  Learning rate = 0.004424:   Batch Loss = 1.593736, Accuracy = 0.25\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.60106432438, Accuracy = 0.2043440938\n",
      "Iter #327680:  Learning rate = 0.004424:   Batch Loss = 1.564155, Accuracy = 0.262499988079\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.60031330585, Accuracy = 0.2043440938\n",
      "Iter #328320:  Learning rate = 0.004424:   Batch Loss = 1.598426, Accuracy = 0.3125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.60021626949, Accuracy = 0.2043440938\n",
      "Iter #328960:  Learning rate = 0.004424:   Batch Loss = 1.586828, Accuracy = 0.3125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.60096144676, Accuracy = 0.2043440938\n",
      "Iter #329600:  Learning rate = 0.004424:   Batch Loss = 1.608300, Accuracy = 0.20000000298\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.60040187836, Accuracy = 0.2043440938\n",
      "Iter #330240:  Learning rate = 0.004424:   Batch Loss = 1.549652, Accuracy = 0.25\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.59871160984, Accuracy = 0.286939114332\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter #330880:  Learning rate = 0.004424:   Batch Loss = 1.575627, Accuracy = 0.22499999404\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.59830844402, Accuracy = 0.286939114332\n",
      "Iter #331520:  Learning rate = 0.004424:   Batch Loss = 1.590247, Accuracy = 0.23749999702\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.59831297398, Accuracy = 0.286939114332\n",
      "Iter #332160:  Learning rate = 0.004424:   Batch Loss = 1.603975, Accuracy = 0.27500000596\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.59981560707, Accuracy = 0.2043440938\n",
      "Iter #332800:  Learning rate = 0.004424:   Batch Loss = 1.559366, Accuracy = 0.300000011921\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.6007219553, Accuracy = 0.2043440938\n",
      "Iter #333440:  Learning rate = 0.004424:   Batch Loss = 1.584087, Accuracy = 0.25\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.60077893734, Accuracy = 0.2043440938\n",
      "Iter #334080:  Learning rate = 0.004424:   Batch Loss = 1.623885, Accuracy = 0.20000000298\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.6009349823, Accuracy = 0.2043440938\n",
      "Iter #334720:  Learning rate = 0.004424:   Batch Loss = 1.540505, Accuracy = 0.324999988079\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.60036420822, Accuracy = 0.2043440938\n",
      "Iter #335360:  Learning rate = 0.004424:   Batch Loss = 1.605215, Accuracy = 0.22499999404\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.60042655468, Accuracy = 0.2043440938\n",
      "Iter #336000:  Learning rate = 0.004424:   Batch Loss = 1.640387, Accuracy = 0.13750000298\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.60047626495, Accuracy = 0.2043440938\n",
      "Iter #336640:  Learning rate = 0.004424:   Batch Loss = 1.561998, Accuracy = 0.28749999404\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.60008752346, Accuracy = 0.2043440938\n",
      "Iter #337280:  Learning rate = 0.004424:   Batch Loss = 1.570380, Accuracy = 0.25\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.59900403023, Accuracy = 0.2043440938\n",
      "Iter #337920:  Learning rate = 0.004424:   Batch Loss = 1.552641, Accuracy = 0.27500000596\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.59863996506, Accuracy = 0.2043440938\n",
      "Iter #338560:  Learning rate = 0.004424:   Batch Loss = 1.584562, Accuracy = 0.23749999702\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.59875750542, Accuracy = 0.2043440938\n",
      "Iter #339200:  Learning rate = 0.004424:   Batch Loss = 1.609455, Accuracy = 0.22499999404\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.59861540794, Accuracy = 0.2043440938\n",
      "Iter #339840:  Learning rate = 0.004424:   Batch Loss = 1.613890, Accuracy = 0.21250000596\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.59897553921, Accuracy = 0.2043440938\n",
      "Iter #340480:  Learning rate = 0.004424:   Batch Loss = 1.576964, Accuracy = 0.25\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.59949696064, Accuracy = 0.2043440938\n",
      "Iter #341120:  Learning rate = 0.004424:   Batch Loss = 1.527650, Accuracy = 0.262499988079\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.59939670563, Accuracy = 0.2043440938\n",
      "Iter #341760:  Learning rate = 0.004424:   Batch Loss = 1.607266, Accuracy = 0.25\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.59875512123, Accuracy = 0.2043440938\n",
      "Iter #342400:  Learning rate = 0.004424:   Batch Loss = 1.621154, Accuracy = 0.262499988079\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.59822368622, Accuracy = 0.286939114332\n",
      "Iter #343040:  Learning rate = 0.004424:   Batch Loss = 1.636834, Accuracy = 0.21250000596\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.59815275669, Accuracy = 0.286939114332\n",
      "Iter #343680:  Learning rate = 0.004424:   Batch Loss = 1.604907, Accuracy = 0.25\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.59773242474, Accuracy = 0.286939114332\n",
      "Iter #344320:  Learning rate = 0.004424:   Batch Loss = 1.595268, Accuracy = 0.21250000596\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.59777021408, Accuracy = 0.286939114332\n",
      "Iter #344960:  Learning rate = 0.004424:   Batch Loss = 1.583255, Accuracy = 0.27500000596\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.59799921513, Accuracy = 0.2043440938\n",
      "Iter #345600:  Learning rate = 0.004424:   Batch Loss = 1.645775, Accuracy = 0.22499999404\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.59865534306, Accuracy = 0.2043440938\n",
      "Iter #346240:  Learning rate = 0.004424:   Batch Loss = 1.611230, Accuracy = 0.1875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.59938943386, Accuracy = 0.2043440938\n",
      "Iter #346880:  Learning rate = 0.004424:   Batch Loss = 1.596233, Accuracy = 0.21250000596\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.59887588024, Accuracy = 0.2043440938\n",
      "Iter #347520:  Learning rate = 0.004424:   Batch Loss = 1.587982, Accuracy = 0.262499988079\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.59983682632, Accuracy = 0.2043440938\n",
      "Iter #348160:  Learning rate = 0.004424:   Batch Loss = 1.609664, Accuracy = 0.21250000596\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.60012996197, Accuracy = 0.2043440938\n",
      "Iter #348800:  Learning rate = 0.004424:   Batch Loss = 1.573846, Accuracy = 0.25\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.6005282402, Accuracy = 0.2043440938\n",
      "Iter #349440:  Learning rate = 0.004424:   Batch Loss = 1.622868, Accuracy = 0.21250000596\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.6010696888, Accuracy = 0.2043440938\n",
      "Iter #350080:  Learning rate = 0.004424:   Batch Loss = 1.595068, Accuracy = 0.20000000298\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.60091018677, Accuracy = 0.2043440938\n",
      "Iter #350720:  Learning rate = 0.004424:   Batch Loss = 1.603013, Accuracy = 0.20000000298\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.60067152977, Accuracy = 0.2043440938\n",
      "Iter #351360:  Learning rate = 0.004424:   Batch Loss = 1.539329, Accuracy = 0.362500011921\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.60012888908, Accuracy = 0.2043440938\n",
      "Iter #352000:  Learning rate = 0.004424:   Batch Loss = 1.598596, Accuracy = 0.33750000596\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.60096001625, Accuracy = 0.2043440938\n",
      "Iter #352640:  Learning rate = 0.004424:   Batch Loss = 1.584563, Accuracy = 0.3125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.60239601135, Accuracy = 0.2043440938\n",
      "Iter #353280:  Learning rate = 0.004424:   Batch Loss = 1.599207, Accuracy = 0.25\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.60349667072, Accuracy = 0.2043440938\n",
      "Iter #353920:  Learning rate = 0.004424:   Batch Loss = 1.626497, Accuracy = 0.21250000596\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.60270166397, Accuracy = 0.2043440938\n",
      "Iter #354560:  Learning rate = 0.004424:   Batch Loss = 1.575937, Accuracy = 0.25\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.60056829453, Accuracy = 0.2043440938\n",
      "Iter #355200:  Learning rate = 0.004424:   Batch Loss = 1.588655, Accuracy = 0.22499999404\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.59788632393, Accuracy = 0.2043440938\n",
      "Iter #355840:  Learning rate = 0.004424:   Batch Loss = 1.629677, Accuracy = 0.23749999702\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.59576058388, Accuracy = 0.286939114332\n",
      "Iter #356480:  Learning rate = 0.004424:   Batch Loss = 1.620836, Accuracy = 0.15000000596\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.59434783459, Accuracy = 0.286939114332\n",
      "Iter #357120:  Learning rate = 0.004424:   Batch Loss = 1.595950, Accuracy = 0.25\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.5940130949, Accuracy = 0.286939114332\n",
      "Iter #357760:  Learning rate = 0.004424:   Batch Loss = 1.592727, Accuracy = 0.27500000596\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.59731221199, Accuracy = 0.2043440938\n",
      "Iter #358400:  Learning rate = 0.004424:   Batch Loss = 1.583997, Accuracy = 0.28749999404\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.59887766838, Accuracy = 0.2043440938\n",
      "Iter #359040:  Learning rate = 0.004424:   Batch Loss = 1.550735, Accuracy = 0.28749999404\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.59896421432, Accuracy = 0.2043440938\n",
      "Iter #359680:  Learning rate = 0.004424:   Batch Loss = 1.574550, Accuracy = 0.25\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.5983954668, Accuracy = 0.2043440938\n",
      "Iter #360320:  Learning rate = 0.004424:   Batch Loss = 1.569360, Accuracy = 0.34999999404\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.59857213497, Accuracy = 0.2043440938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter #360960:  Learning rate = 0.004424:   Batch Loss = 1.575771, Accuracy = 0.300000011921\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.59871888161, Accuracy = 0.2043440938\n",
      "Iter #361600:  Learning rate = 0.004424:   Batch Loss = 1.575096, Accuracy = 0.3125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.59942173958, Accuracy = 0.2043440938\n",
      "Iter #362240:  Learning rate = 0.004424:   Batch Loss = 1.613983, Accuracy = 0.15000000596\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.59941101074, Accuracy = 0.2043440938\n",
      "Iter #362880:  Learning rate = 0.004424:   Batch Loss = 1.580585, Accuracy = 0.28749999404\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.59870243073, Accuracy = 0.2043440938\n",
      "Iter #363520:  Learning rate = 0.004424:   Batch Loss = 1.577906, Accuracy = 0.28749999404\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.59815704823, Accuracy = 0.2043440938\n",
      "Iter #364160:  Learning rate = 0.004424:   Batch Loss = 1.552472, Accuracy = 0.324999988079\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.59734988213, Accuracy = 0.2043440938\n",
      "Iter #364800:  Learning rate = 0.004424:   Batch Loss = 1.574854, Accuracy = 0.27500000596\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.59609949589, Accuracy = 0.286939114332\n",
      "Iter #365440:  Learning rate = 0.004424:   Batch Loss = 1.602043, Accuracy = 0.27500000596\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.5954554081, Accuracy = 0.286939114332\n",
      "Iter #366080:  Learning rate = 0.004424:   Batch Loss = 1.607600, Accuracy = 0.13750000298\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.59593975544, Accuracy = 0.286939114332\n",
      "Iter #366720:  Learning rate = 0.004424:   Batch Loss = 1.613445, Accuracy = 0.25\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.59663617611, Accuracy = 0.2043440938\n",
      "Iter #367360:  Learning rate = 0.004424:   Batch Loss = 1.596567, Accuracy = 0.300000011921\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.59714102745, Accuracy = 0.2043440938\n",
      "Iter #368000:  Learning rate = 0.004424:   Batch Loss = 1.550925, Accuracy = 0.27500000596\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.59892833233, Accuracy = 0.2043440938\n",
      "Iter #368640:  Learning rate = 0.004424:   Batch Loss = 1.586885, Accuracy = 0.22499999404\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.59997355938, Accuracy = 0.2043440938\n",
      "Iter #369280:  Learning rate = 0.004424:   Batch Loss = 1.625511, Accuracy = 0.22499999404\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.60033428669, Accuracy = 0.2043440938\n",
      "Iter #369920:  Learning rate = 0.004424:   Batch Loss = 1.578856, Accuracy = 0.262499988079\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.59831511974, Accuracy = 0.2043440938\n",
      "Iter #370560:  Learning rate = 0.004424:   Batch Loss = 1.578425, Accuracy = 0.28749999404\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.59777867794, Accuracy = 0.2043440938\n",
      "Iter #371200:  Learning rate = 0.004424:   Batch Loss = 1.555276, Accuracy = 0.27500000596\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.59778952599, Accuracy = 0.2043440938\n",
      "Iter #371840:  Learning rate = 0.004424:   Batch Loss = 1.585760, Accuracy = 0.300000011921\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.59747207165, Accuracy = 0.2043440938\n",
      "Iter #372480:  Learning rate = 0.004424:   Batch Loss = 1.562004, Accuracy = 0.25\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.59751915932, Accuracy = 0.2043440938\n",
      "Iter #373120:  Learning rate = 0.004424:   Batch Loss = 1.619952, Accuracy = 0.22499999404\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.59606909752, Accuracy = 0.286939114332\n",
      "Iter #373760:  Learning rate = 0.004424:   Batch Loss = 1.599925, Accuracy = 0.27500000596\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.59678649902, Accuracy = 0.2043440938\n",
      "Iter #374400:  Learning rate = 0.004424:   Batch Loss = 1.567008, Accuracy = 0.324999988079\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.59889149666, Accuracy = 0.2043440938\n",
      "Iter #375040:  Learning rate = 0.004424:   Batch Loss = 1.585515, Accuracy = 0.23749999702\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.60029637814, Accuracy = 0.2043440938\n",
      "Iter #375680:  Learning rate = 0.004424:   Batch Loss = 1.571078, Accuracy = 0.28749999404\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.59851908684, Accuracy = 0.2043440938\n",
      "Iter #376320:  Learning rate = 0.004424:   Batch Loss = 1.636001, Accuracy = 0.16249999404\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.59650635719, Accuracy = 0.2043440938\n",
      "Iter #376960:  Learning rate = 0.004424:   Batch Loss = 1.601561, Accuracy = 0.21250000596\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.5953168869, Accuracy = 0.286939114332\n",
      "Iter #377600:  Learning rate = 0.004424:   Batch Loss = 1.594794, Accuracy = 0.262499988079\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.59580647945, Accuracy = 0.2043440938\n",
      "Iter #378240:  Learning rate = 0.004424:   Batch Loss = 1.588726, Accuracy = 0.22499999404\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.59715175629, Accuracy = 0.2043440938\n",
      "Iter #378880:  Learning rate = 0.004424:   Batch Loss = 1.590392, Accuracy = 0.300000011921\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.5995978117, Accuracy = 0.2043440938\n",
      "Iter #379520:  Learning rate = 0.004424:   Batch Loss = 1.595499, Accuracy = 0.27500000596\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.60073530674, Accuracy = 0.2043440938\n",
      "Iter #380160:  Learning rate = 0.004424:   Batch Loss = 1.616864, Accuracy = 0.28749999404\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.59964239597, Accuracy = 0.2043440938\n",
      "Iter #380800:  Learning rate = 0.004424:   Batch Loss = 1.568267, Accuracy = 0.324999988079\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.60120522976, Accuracy = 0.2043440938\n",
      "Iter #381440:  Learning rate = 0.004424:   Batch Loss = 1.594180, Accuracy = 0.23749999702\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.5995657444, Accuracy = 0.2043440938\n",
      "Iter #382080:  Learning rate = 0.004424:   Batch Loss = 1.581405, Accuracy = 0.25\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.59700536728, Accuracy = 0.2043440938\n",
      "Iter #382720:  Learning rate = 0.004424:   Batch Loss = 1.569932, Accuracy = 0.300000011921\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.595469594, Accuracy = 0.2043440938\n",
      "Iter #383360:  Learning rate = 0.004424:   Batch Loss = 1.585494, Accuracy = 0.28749999404\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.59654903412, Accuracy = 0.2043440938\n",
      "Iter #384000:  Learning rate = 0.004424:   Batch Loss = 1.586574, Accuracy = 0.27500000596\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.59636843204, Accuracy = 0.2043440938\n",
      "Iter #384640:  Learning rate = 0.004424:   Batch Loss = 1.614440, Accuracy = 0.28749999404\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.59664285183, Accuracy = 0.2043440938\n",
      "Iter #385280:  Learning rate = 0.004424:   Batch Loss = 1.579414, Accuracy = 0.262499988079\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.59760344028, Accuracy = 0.2043440938\n",
      "Iter #385920:  Learning rate = 0.004424:   Batch Loss = 1.594431, Accuracy = 0.23749999702\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.59857428074, Accuracy = 0.2043440938\n",
      "Iter #386560:  Learning rate = 0.004424:   Batch Loss = 1.553868, Accuracy = 0.324999988079\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.59915196896, Accuracy = 0.2043440938\n",
      "Iter #387200:  Learning rate = 0.004424:   Batch Loss = 1.555866, Accuracy = 0.25\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.59950101376, Accuracy = 0.2043440938\n",
      "Iter #387840:  Learning rate = 0.004424:   Batch Loss = 1.611679, Accuracy = 0.23749999702\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.59878575802, Accuracy = 0.2043440938\n",
      "Iter #388480:  Learning rate = 0.004424:   Batch Loss = 1.608233, Accuracy = 0.23749999702\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.59772753716, Accuracy = 0.2043440938\n",
      "Iter #389120:  Learning rate = 0.004424:   Batch Loss = 1.643803, Accuracy = 0.22499999404\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.59678113461, Accuracy = 0.2043440938\n",
      "Iter #389760:  Learning rate = 0.004424:   Batch Loss = 1.578568, Accuracy = 0.25\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.59544873238, Accuracy = 0.2043440938\n",
      "Iter #390400:  Learning rate = 0.004424:   Batch Loss = 1.570576, Accuracy = 0.5625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.59096741676, Accuracy = 0.491283237934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter #391040:  Learning rate = 0.004424:   Batch Loss = 1.488852, Accuracy = 0.537500023842\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.47128283978, Accuracy = 0.491283237934\n",
      "Iter #391680:  Learning rate = 0.004424:   Batch Loss = 1.102067, Accuracy = 0.5625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.08555936813, Accuracy = 0.491283237934\n",
      "Iter #392320:  Learning rate = 0.004424:   Batch Loss = 0.963459, Accuracy = 0.524999976158\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.998297810555, Accuracy = 0.491283237934\n",
      "Iter #392960:  Learning rate = 0.004424:   Batch Loss = 1.269506, Accuracy = 0.5\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.981078922749, Accuracy = 0.510145783424\n",
      "Iter #393600:  Learning rate = 0.004424:   Batch Loss = 1.061931, Accuracy = 0.425000011921\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.04636001587, Accuracy = 0.448699623346\n",
      "Iter #394240:  Learning rate = 0.004424:   Batch Loss = 0.805875, Accuracy = 0.662500023842\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.853736579418, Accuracy = 0.671620488167\n",
      "Iter #394880:  Learning rate = 0.004424:   Batch Loss = 0.770730, Accuracy = 0.612500011921\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.765959620476, Accuracy = 0.671048879623\n",
      "Iter #395520:  Learning rate = 0.004424:   Batch Loss = 0.669062, Accuracy = 0.699999988079\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.74747800827, Accuracy = 0.672192037106\n",
      "Iter #396160:  Learning rate = 0.004424:   Batch Loss = 0.919681, Accuracy = 0.649999976158\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.742373943329, Accuracy = 0.670763075352\n",
      "Iter #396800:  Learning rate = 0.004424:   Batch Loss = 0.714572, Accuracy = 0.637499988079\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.741339325905, Accuracy = 0.671048879623\n",
      "Iter #397440:  Learning rate = 0.004424:   Batch Loss = 0.665020, Accuracy = 0.662500023842\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.762544095516, Accuracy = 0.665047168732\n",
      "Iter #398080:  Learning rate = 0.004424:   Batch Loss = 0.821538, Accuracy = 0.600000023842\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.731764614582, Accuracy = 0.671048879623\n",
      "Iter #398720:  Learning rate = 0.004424:   Batch Loss = 0.679564, Accuracy = 0.712499976158\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.737213909626, Accuracy = 0.671620488167\n",
      "Iter #399360:  Learning rate = 0.004424:   Batch Loss = 0.703709, Accuracy = 0.712499976158\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.725995540619, Accuracy = 0.672192037106\n",
      "Iter #400000:  Learning rate = 0.004247:   Batch Loss = 0.564599, Accuracy = 0.800000011921\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.722476482391, Accuracy = 0.672192037106\n",
      "Iter #400640:  Learning rate = 0.004247:   Batch Loss = 0.675274, Accuracy = 0.75\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.722479701042, Accuracy = 0.672192037106\n",
      "Iter #401280:  Learning rate = 0.004247:   Batch Loss = 0.613975, Accuracy = 0.699999988079\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.720737874508, Accuracy = 0.672192037106\n",
      "Iter #401920:  Learning rate = 0.004247:   Batch Loss = 0.701569, Accuracy = 0.649999976158\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.723375022411, Accuracy = 0.672192037106\n",
      "Iter #402560:  Learning rate = 0.004247:   Batch Loss = 0.654055, Accuracy = 0.649999976158\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.722133636475, Accuracy = 0.672192037106\n",
      "Iter #403200:  Learning rate = 0.004247:   Batch Loss = 0.774577, Accuracy = 0.574999988079\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.719224750996, Accuracy = 0.672192037106\n",
      "Iter #403840:  Learning rate = 0.004247:   Batch Loss = 0.600979, Accuracy = 0.787500023842\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.721823513508, Accuracy = 0.672192037106\n",
      "Iter #404480:  Learning rate = 0.004247:   Batch Loss = 0.690198, Accuracy = 0.675000011921\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.719095110893, Accuracy = 0.672192037106\n",
      "Iter #405120:  Learning rate = 0.004247:   Batch Loss = 0.621665, Accuracy = 0.699999988079\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.715710759163, Accuracy = 0.672192037106\n",
      "Iter #405760:  Learning rate = 0.004247:   Batch Loss = 0.729733, Accuracy = 0.662500023842\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.715468108654, Accuracy = 0.672192037106\n",
      "Iter #406400:  Learning rate = 0.004247:   Batch Loss = 0.693721, Accuracy = 0.675000011921\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.718046009541, Accuracy = 0.672192037106\n",
      "Iter #407040:  Learning rate = 0.004247:   Batch Loss = 0.661434, Accuracy = 0.625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.724053800106, Accuracy = 0.672192037106\n",
      "Iter #407680:  Learning rate = 0.004247:   Batch Loss = 0.698905, Accuracy = 0.662500023842\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.716213107109, Accuracy = 0.672192037106\n",
      "Iter #408320:  Learning rate = 0.004247:   Batch Loss = 0.646887, Accuracy = 0.725000023842\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.722110211849, Accuracy = 0.672192037106\n",
      "Iter #408960:  Learning rate = 0.004247:   Batch Loss = 0.685865, Accuracy = 0.637499988079\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.71508038044, Accuracy = 0.672192037106\n",
      "Iter #409600:  Learning rate = 0.004247:   Batch Loss = 0.649773, Accuracy = 0.675000011921\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.712848246098, Accuracy = 0.672192037106\n",
      "Iter #410240:  Learning rate = 0.004247:   Batch Loss = 0.602370, Accuracy = 0.699999988079\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.712451338768, Accuracy = 0.672192037106\n",
      "Iter #410880:  Learning rate = 0.004247:   Batch Loss = 0.649515, Accuracy = 0.675000011921\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.716145098209, Accuracy = 0.672192037106\n",
      "Iter #411520:  Learning rate = 0.004247:   Batch Loss = 0.748060, Accuracy = 0.625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.71160787344, Accuracy = 0.672192037106\n",
      "Iter #412160:  Learning rate = 0.004247:   Batch Loss = 0.570651, Accuracy = 0.725000023842\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.711234509945, Accuracy = 0.672192037106\n",
      "Iter #412800:  Learning rate = 0.004247:   Batch Loss = 0.634707, Accuracy = 0.725000023842\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.713967442513, Accuracy = 0.672192037106\n",
      "Iter #413440:  Learning rate = 0.004247:   Batch Loss = 0.680671, Accuracy = 0.675000011921\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.714008748531, Accuracy = 0.672192037106\n",
      "Iter #414080:  Learning rate = 0.004247:   Batch Loss = 0.695428, Accuracy = 0.637499988079\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.711342990398, Accuracy = 0.672192037106\n",
      "Iter #414720:  Learning rate = 0.004247:   Batch Loss = 0.622990, Accuracy = 0.725000023842\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.710943639278, Accuracy = 0.672192037106\n",
      "Iter #415360:  Learning rate = 0.004247:   Batch Loss = 0.729836, Accuracy = 0.587499976158\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.709521353245, Accuracy = 0.672192037106\n",
      "Iter #416000:  Learning rate = 0.004247:   Batch Loss = 0.589938, Accuracy = 0.774999976158\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.710292935371, Accuracy = 0.672192037106\n",
      "Iter #416640:  Learning rate = 0.004247:   Batch Loss = 0.633789, Accuracy = 0.675000011921\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.712207138538, Accuracy = 0.672192037106\n",
      "Iter #417280:  Learning rate = 0.004247:   Batch Loss = 0.802372, Accuracy = 0.600000023842\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.709773123264, Accuracy = 0.672192037106\n",
      "Iter #417920:  Learning rate = 0.004247:   Batch Loss = 0.536294, Accuracy = 0.774999976158\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.709702551365, Accuracy = 0.672192037106\n",
      "Iter #418560:  Learning rate = 0.004247:   Batch Loss = 0.711540, Accuracy = 0.637499988079\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.708737909794, Accuracy = 0.672192037106\n",
      "Iter #419200:  Learning rate = 0.004247:   Batch Loss = 0.609572, Accuracy = 0.6875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.708033680916, Accuracy = 0.672192037106\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter #419840:  Learning rate = 0.004247:   Batch Loss = 0.751084, Accuracy = 0.600000023842\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.708946347237, Accuracy = 0.672192037106\n",
      "Iter #420480:  Learning rate = 0.004247:   Batch Loss = 0.646756, Accuracy = 0.6875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.708987236023, Accuracy = 0.672192037106\n",
      "Iter #421120:  Learning rate = 0.004247:   Batch Loss = 0.653166, Accuracy = 0.725000023842\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.706189513206, Accuracy = 0.672192037106\n",
      "Iter #421760:  Learning rate = 0.004247:   Batch Loss = 0.584656, Accuracy = 0.699999988079\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.706775188446, Accuracy = 0.672192037106\n",
      "Iter #422400:  Learning rate = 0.004247:   Batch Loss = 0.721230, Accuracy = 0.625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.709795951843, Accuracy = 0.672192037106\n",
      "Iter #423040:  Learning rate = 0.004247:   Batch Loss = 0.766632, Accuracy = 0.600000023842\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.706446528435, Accuracy = 0.672192037106\n",
      "Iter #423680:  Learning rate = 0.004247:   Batch Loss = 0.719012, Accuracy = 0.649999976158\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.704594492912, Accuracy = 0.672192037106\n",
      "Iter #424320:  Learning rate = 0.004247:   Batch Loss = 0.670078, Accuracy = 0.675000011921\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.704354345798, Accuracy = 0.672192037106\n",
      "Iter #424960:  Learning rate = 0.004247:   Batch Loss = 0.682546, Accuracy = 0.6875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.707185387611, Accuracy = 0.672192037106\n",
      "Iter #425600:  Learning rate = 0.004247:   Batch Loss = 0.611794, Accuracy = 0.762499988079\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.709331274033, Accuracy = 0.672192037106\n",
      "Iter #426240:  Learning rate = 0.004247:   Batch Loss = 0.721224, Accuracy = 0.637499988079\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.704883813858, Accuracy = 0.672192037106\n",
      "Iter #426880:  Learning rate = 0.004247:   Batch Loss = 0.562216, Accuracy = 0.762499988079\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.708312451839, Accuracy = 0.672192037106\n",
      "Iter #427520:  Learning rate = 0.004247:   Batch Loss = 0.653020, Accuracy = 0.725000023842\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.706728279591, Accuracy = 0.672192037106\n",
      "Iter #428160:  Learning rate = 0.004247:   Batch Loss = 0.654265, Accuracy = 0.675000011921\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.703467190266, Accuracy = 0.672192037106\n",
      "Iter #428800:  Learning rate = 0.004247:   Batch Loss = 0.758397, Accuracy = 0.699999988079\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.704396665096, Accuracy = 0.672192037106\n",
      "Iter #429440:  Learning rate = 0.004247:   Batch Loss = 0.657875, Accuracy = 0.675000011921\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.703668296337, Accuracy = 0.672192037106\n",
      "Iter #430080:  Learning rate = 0.004247:   Batch Loss = 0.577906, Accuracy = 0.774999976158\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.703576385975, Accuracy = 0.672192037106\n",
      "Iter #430720:  Learning rate = 0.004247:   Batch Loss = 0.628951, Accuracy = 0.699999988079\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.707547605038, Accuracy = 0.672192037106\n",
      "Iter #431360:  Learning rate = 0.004247:   Batch Loss = 0.686773, Accuracy = 0.612500011921\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.705669462681, Accuracy = 0.672192037106\n",
      "Iter #432000:  Learning rate = 0.004247:   Batch Loss = 0.656547, Accuracy = 0.662500023842\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.703121900558, Accuracy = 0.672192037106\n",
      "Iter #432640:  Learning rate = 0.004247:   Batch Loss = 0.645226, Accuracy = 0.737500011921\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.705619454384, Accuracy = 0.672192037106\n",
      "Iter #433280:  Learning rate = 0.004247:   Batch Loss = 0.621164, Accuracy = 0.712499976158\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.70312833786, Accuracy = 0.672192037106\n",
      "Iter #433920:  Learning rate = 0.004247:   Batch Loss = 0.626432, Accuracy = 0.637499988079\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.701316714287, Accuracy = 0.672192037106\n",
      "Iter #434560:  Learning rate = 0.004247:   Batch Loss = 0.657972, Accuracy = 0.649999976158\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.702374994755, Accuracy = 0.672192037106\n",
      "Iter #435200:  Learning rate = 0.004247:   Batch Loss = 0.652953, Accuracy = 0.712499976158\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.702108740807, Accuracy = 0.672192037106\n",
      "Iter #435840:  Learning rate = 0.004247:   Batch Loss = 0.711685, Accuracy = 0.612500011921\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.701211571693, Accuracy = 0.672192037106\n",
      "Iter #436480:  Learning rate = 0.004247:   Batch Loss = 0.631205, Accuracy = 0.712499976158\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.702940285206, Accuracy = 0.672192037106\n",
      "Iter #437120:  Learning rate = 0.004247:   Batch Loss = 0.613906, Accuracy = 0.699999988079\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.701704859734, Accuracy = 0.672192037106\n",
      "Iter #437760:  Learning rate = 0.004247:   Batch Loss = 0.681240, Accuracy = 0.75\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.704952538013, Accuracy = 0.672192037106\n",
      "Iter #438400:  Learning rate = 0.004247:   Batch Loss = 0.667318, Accuracy = 0.662500023842\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.702064096928, Accuracy = 0.672192037106\n",
      "Iter #439040:  Learning rate = 0.004247:   Batch Loss = 0.665183, Accuracy = 0.712499976158\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.703807771206, Accuracy = 0.672192037106\n",
      "Iter #439680:  Learning rate = 0.004247:   Batch Loss = 0.647529, Accuracy = 0.675000011921\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.702023208141, Accuracy = 0.672192037106\n",
      "Iter #440320:  Learning rate = 0.004247:   Batch Loss = 0.641353, Accuracy = 0.6875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.699896216393, Accuracy = 0.672192037106\n",
      "Iter #440960:  Learning rate = 0.004247:   Batch Loss = 0.649762, Accuracy = 0.699999988079\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.701998353004, Accuracy = 0.672192037106\n",
      "Iter #441600:  Learning rate = 0.004247:   Batch Loss = 0.642072, Accuracy = 0.637499988079\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.702917695045, Accuracy = 0.672192037106\n",
      "Iter #442240:  Learning rate = 0.004247:   Batch Loss = 0.742611, Accuracy = 0.574999988079\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.702268242836, Accuracy = 0.672192037106\n",
      "Iter #442880:  Learning rate = 0.004247:   Batch Loss = 0.754641, Accuracy = 0.625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.703908324242, Accuracy = 0.672192037106\n",
      "Iter #443520:  Learning rate = 0.004247:   Batch Loss = 0.646486, Accuracy = 0.699999988079\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.700722455978, Accuracy = 0.672192037106\n",
      "Iter #444160:  Learning rate = 0.004247:   Batch Loss = 0.621434, Accuracy = 0.712499976158\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.700298190117, Accuracy = 0.672192037106\n",
      "Iter #444800:  Learning rate = 0.004247:   Batch Loss = 0.740973, Accuracy = 0.649999976158\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.700720489025, Accuracy = 0.672192037106\n",
      "Iter #445440:  Learning rate = 0.004247:   Batch Loss = 0.723356, Accuracy = 0.6875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.700996339321, Accuracy = 0.672192037106\n",
      "Iter #446080:  Learning rate = 0.004247:   Batch Loss = 0.654882, Accuracy = 0.699999988079\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.699728608131, Accuracy = 0.672192037106\n",
      "Iter #446720:  Learning rate = 0.004247:   Batch Loss = 0.651123, Accuracy = 0.675000011921\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.699930548668, Accuracy = 0.672192037106\n",
      "Iter #447360:  Learning rate = 0.004247:   Batch Loss = 0.566832, Accuracy = 0.699999988079\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.69970023632, Accuracy = 0.672192037106\n",
      "Iter #448000:  Learning rate = 0.004247:   Batch Loss = 0.735620, Accuracy = 0.625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.699809193611, Accuracy = 0.672192037106\n",
      "Iter #448640:  Learning rate = 0.004247:   Batch Loss = 0.634025, Accuracy = 0.6875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.700665235519, Accuracy = 0.672192037106\n",
      "Iter #449280:  Learning rate = 0.004247:   Batch Loss = 0.690729, Accuracy = 0.737500011921\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.701145291328, Accuracy = 0.672192037106\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter #449920:  Learning rate = 0.004247:   Batch Loss = 0.693657, Accuracy = 0.662500023842\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.699248671532, Accuracy = 0.672192037106\n",
      "Iter #450560:  Learning rate = 0.004247:   Batch Loss = 0.626118, Accuracy = 0.6875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.699100732803, Accuracy = 0.672192037106\n",
      "Iter #451200:  Learning rate = 0.004247:   Batch Loss = 0.641130, Accuracy = 0.712499976158\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.701268494129, Accuracy = 0.672192037106\n",
      "Iter #451840:  Learning rate = 0.004247:   Batch Loss = 0.573556, Accuracy = 0.762499988079\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.702680945396, Accuracy = 0.672192037106\n",
      "Iter #452480:  Learning rate = 0.004247:   Batch Loss = 0.657548, Accuracy = 0.662500023842\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.700158059597, Accuracy = 0.672192037106\n",
      "Iter #453120:  Learning rate = 0.004247:   Batch Loss = 0.710870, Accuracy = 0.625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.6987657547, Accuracy = 0.672192037106\n",
      "Iter #453760:  Learning rate = 0.004247:   Batch Loss = 0.754966, Accuracy = 0.637499988079\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.699579954147, Accuracy = 0.672192037106\n",
      "Iter #454400:  Learning rate = 0.004247:   Batch Loss = 0.744861, Accuracy = 0.612500011921\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.700637042522, Accuracy = 0.672192037106\n",
      "Iter #455040:  Learning rate = 0.004247:   Batch Loss = 0.714725, Accuracy = 0.600000023842\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.700081825256, Accuracy = 0.672192037106\n",
      "Iter #455680:  Learning rate = 0.004247:   Batch Loss = 0.634953, Accuracy = 0.712499976158\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.699423015118, Accuracy = 0.672192037106\n",
      "Iter #456320:  Learning rate = 0.004247:   Batch Loss = 0.672323, Accuracy = 0.699999988079\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.699747145176, Accuracy = 0.672192037106\n",
      "Iter #456960:  Learning rate = 0.004247:   Batch Loss = 0.733618, Accuracy = 0.637499988079\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.698623001575, Accuracy = 0.672192037106\n",
      "Iter #457600:  Learning rate = 0.004247:   Batch Loss = 0.664481, Accuracy = 0.675000011921\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.698276937008, Accuracy = 0.672192037106\n",
      "Iter #458240:  Learning rate = 0.004247:   Batch Loss = 0.637727, Accuracy = 0.6875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.696909189224, Accuracy = 0.672192037106\n",
      "Iter #458880:  Learning rate = 0.004247:   Batch Loss = 0.633788, Accuracy = 0.712499976158\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.695283293724, Accuracy = 0.672192037106\n",
      "Iter #459520:  Learning rate = 0.004247:   Batch Loss = 0.628541, Accuracy = 0.675000011921\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.685107588768, Accuracy = 0.672192037106\n",
      "Iter #460160:  Learning rate = 0.004247:   Batch Loss = 0.569369, Accuracy = 0.725000023842\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.649113714695, Accuracy = 0.671048879623\n",
      "Iter #460800:  Learning rate = 0.004247:   Batch Loss = 0.557082, Accuracy = 0.8125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.577010214329, Accuracy = 0.668762505054\n",
      "Iter #461440:  Learning rate = 0.004247:   Batch Loss = 0.493558, Accuracy = 0.75\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.467656195164, Accuracy = 0.844527006149\n",
      "Iter #462080:  Learning rate = 0.004247:   Batch Loss = 0.795757, Accuracy = 0.737500011921\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.571380972862, Accuracy = 0.789368391037\n",
      "Iter #462720:  Learning rate = 0.004247:   Batch Loss = 0.852599, Accuracy = 0.47499999404\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.699910700321, Accuracy = 0.796513319016\n",
      "Iter #463360:  Learning rate = 0.004247:   Batch Loss = 0.610577, Accuracy = 0.699999988079\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.685847699642, Accuracy = 0.663046598434\n",
      "Iter #464000:  Learning rate = 0.004247:   Batch Loss = 0.838568, Accuracy = 0.649999976158\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.577069103718, Accuracy = 0.803658187389\n",
      "Iter #464640:  Learning rate = 0.004247:   Batch Loss = 0.692556, Accuracy = 0.787500023842\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.678556501865, Accuracy = 0.674764215946\n",
      "Iter #465280:  Learning rate = 0.004247:   Batch Loss = 0.576089, Accuracy = 0.824999988079\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.581114709377, Accuracy = 0.834524154663\n",
      "Iter #465920:  Learning rate = 0.004247:   Batch Loss = 0.446310, Accuracy = 0.850000023842\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.479193240404, Accuracy = 0.846241772175\n",
      "Iter #466560:  Learning rate = 0.004247:   Batch Loss = 0.467048, Accuracy = 0.875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.423257052898, Accuracy = 0.844527006149\n",
      "Iter #467200:  Learning rate = 0.004247:   Batch Loss = 0.422101, Accuracy = 0.8125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.403876334429, Accuracy = 0.844241201878\n",
      "Iter #467840:  Learning rate = 0.004247:   Batch Loss = 0.485671, Accuracy = 0.712499976158\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.380289405584, Accuracy = 0.844241201878\n",
      "Iter #468480:  Learning rate = 0.004247:   Batch Loss = 0.498041, Accuracy = 0.725000023842\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.448410362005, Accuracy = 0.844241201878\n",
      "Iter #469120:  Learning rate = 0.004247:   Batch Loss = 0.374148, Accuracy = 0.887499988079\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.356730401516, Accuracy = 0.844812810421\n",
      "Iter #469760:  Learning rate = 0.004247:   Batch Loss = 0.403594, Accuracy = 0.862500011921\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.395783215761, Accuracy = 0.838239490986\n",
      "Iter #470400:  Learning rate = 0.004247:   Batch Loss = 0.396731, Accuracy = 0.837499976158\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.437932640314, Accuracy = 0.813375234604\n",
      "Iter #471040:  Learning rate = 0.004247:   Batch Loss = 0.317842, Accuracy = 0.875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.338688254356, Accuracy = 0.852529287338\n",
      "Iter #471680:  Learning rate = 0.004247:   Batch Loss = 0.342054, Accuracy = 0.837499976158\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.338249206543, Accuracy = 0.820234358311\n",
      "Iter #472320:  Learning rate = 0.004247:   Batch Loss = 0.349529, Accuracy = 0.800000011921\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3327473104, Accuracy = 0.852529287338\n",
      "Iter #472960:  Learning rate = 0.004247:   Batch Loss = 0.385108, Accuracy = 0.962499976158\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.323229014874, Accuracy = 0.955987453461\n",
      "Iter #473600:  Learning rate = 0.004247:   Batch Loss = 0.327464, Accuracy = 0.975000023842\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.308197319508, Accuracy = 0.98685336113\n",
      "Iter #474240:  Learning rate = 0.004247:   Batch Loss = 0.267188, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.282141089439, Accuracy = 0.98685336113\n",
      "Iter #474880:  Learning rate = 0.004247:   Batch Loss = 0.395985, Accuracy = 0.925000011921\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.29495254159, Accuracy = 0.97056299448\n",
      "Iter #475520:  Learning rate = 0.004247:   Batch Loss = 0.267695, Accuracy = 0.975000023842\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.286245673895, Accuracy = 0.939411282539\n",
      "Iter #476160:  Learning rate = 0.004247:   Batch Loss = 0.240824, Accuracy = 0.962499976158\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.295556426048, Accuracy = 0.936553299427\n",
      "Iter #476800:  Learning rate = 0.004247:   Batch Loss = 0.220165, Accuracy = 0.975000023842\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.234907463193, Accuracy = 0.976850509644\n",
      "Iter #477440:  Learning rate = 0.004247:   Batch Loss = 0.230757, Accuracy = 0.975000023842\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.444844126701, Accuracy = 0.852529287338\n",
      "Iter #478080:  Learning rate = 0.004247:   Batch Loss = 0.276788, Accuracy = 0.949999988079\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.380085617304, Accuracy = 0.904829978943\n",
      "Iter #478720:  Learning rate = 0.004247:   Batch Loss = 0.274392, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.314456254244, Accuracy = 0.936839103699\n",
      "Iter #479360:  Learning rate = 0.004247:   Batch Loss = 0.298561, Accuracy = 0.9375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.239795684814, Accuracy = 0.971420407295\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter #480000:  Learning rate = 0.004247:   Batch Loss = 0.188147, Accuracy = 0.987500011921\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.194064974785, Accuracy = 0.994855701923\n",
      "Iter #480640:  Learning rate = 0.004247:   Batch Loss = 0.170691, Accuracy = 0.987500011921\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.176931291819, Accuracy = 0.996570467949\n",
      "Iter #481280:  Learning rate = 0.004247:   Batch Loss = 0.198105, Accuracy = 0.975000023842\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.172564104199, Accuracy = 0.994855701923\n",
      "Iter #481920:  Learning rate = 0.004247:   Batch Loss = 0.183857, Accuracy = 0.987500011921\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.162499368191, Accuracy = 0.999142587185\n",
      "Iter #482560:  Learning rate = 0.004247:   Batch Loss = 0.158692, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.172534614801, Accuracy = 0.992283523083\n",
      "Iter #483200:  Learning rate = 0.004247:   Batch Loss = 0.168628, Accuracy = 0.987500011921\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.175193920732, Accuracy = 0.986281812191\n",
      "Iter #483840:  Learning rate = 0.004247:   Batch Loss = 0.150553, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.149953484535, Accuracy = 0.997713625431\n",
      "Iter #484480:  Learning rate = 0.004247:   Batch Loss = 0.197021, Accuracy = 0.962499976158\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.149316430092, Accuracy = 0.997999429703\n",
      "Iter #485120:  Learning rate = 0.004247:   Batch Loss = 0.141874, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.144232496619, Accuracy = 0.999142587185\n",
      "Iter #485760:  Learning rate = 0.004247:   Batch Loss = 0.148056, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.141602411866, Accuracy = 0.999428391457\n",
      "Iter #486400:  Learning rate = 0.004247:   Batch Loss = 0.141711, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.144801035523, Accuracy = 0.996570467949\n",
      "Iter #487040:  Learning rate = 0.004247:   Batch Loss = 0.138830, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.138207837939, Accuracy = 0.999428391457\n",
      "Iter #487680:  Learning rate = 0.004247:   Batch Loss = 0.141525, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.139976516366, Accuracy = 0.998285233974\n",
      "Iter #488320:  Learning rate = 0.004247:   Batch Loss = 0.135814, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.136493772268, Accuracy = 0.998285233974\n",
      "Iter #488960:  Learning rate = 0.004247:   Batch Loss = 0.131111, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.133781567216, Accuracy = 0.999428391457\n",
      "Iter #489600:  Learning rate = 0.004247:   Batch Loss = 0.136363, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.132642060518, Accuracy = 0.999428391457\n",
      "Iter #490240:  Learning rate = 0.004247:   Batch Loss = 0.127667, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.131271630526, Accuracy = 0.999428391457\n",
      "Iter #490880:  Learning rate = 0.004247:   Batch Loss = 0.133593, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.130443647504, Accuracy = 0.999428391457\n",
      "Iter #491520:  Learning rate = 0.004247:   Batch Loss = 0.175953, Accuracy = 0.975000023842\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.130427390337, Accuracy = 0.998571038246\n",
      "Iter #492160:  Learning rate = 0.004247:   Batch Loss = 0.147445, Accuracy = 0.987500011921\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.139750987291, Accuracy = 0.994569897652\n",
      "Iter #492800:  Learning rate = 0.004247:   Batch Loss = 0.123982, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.128522977233, Accuracy = 0.999142587185\n",
      "Iter #493440:  Learning rate = 0.004247:   Batch Loss = 0.126212, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.126607999206, Accuracy = 0.999714195728\n",
      "Iter #494080:  Learning rate = 0.004247:   Batch Loss = 0.123201, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.127329036593, Accuracy = 0.999142587185\n",
      "Iter #494720:  Learning rate = 0.004247:   Batch Loss = 0.125264, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.132563099265, Accuracy = 0.997713625431\n",
      "Iter #495360:  Learning rate = 0.004247:   Batch Loss = 0.146181, Accuracy = 0.987500011921\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.178225159645, Accuracy = 0.977422118187\n",
      "Iter #496000:  Learning rate = 0.004247:   Batch Loss = 0.130992, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.471353203058, Accuracy = 0.896256089211\n",
      "Iter #496640:  Learning rate = 0.004247:   Batch Loss = 0.396762, Accuracy = 0.912500023842\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.391060352325, Accuracy = 0.90511572361\n",
      "Iter #497280:  Learning rate = 0.004247:   Batch Loss = 0.491578, Accuracy = 0.8125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.19370496273, Accuracy = 0.998285233974\n",
      "Iter #497920:  Learning rate = 0.004247:   Batch Loss = 0.180991, Accuracy = 0.987500011921\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.168918922544, Accuracy = 0.993998289108\n",
      "Iter #498560:  Learning rate = 0.004247:   Batch Loss = 0.211016, Accuracy = 0.987500011921\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.210132554173, Accuracy = 0.978565275669\n",
      "Iter #499200:  Learning rate = 0.004247:   Batch Loss = 0.146675, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.160512328148, Accuracy = 0.994569897652\n",
      "Iter #499840:  Learning rate = 0.004247:   Batch Loss = 0.133407, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.156269297004, Accuracy = 0.992283523083\n",
      "Iter #500480:  Learning rate = 0.004077:   Batch Loss = 0.127433, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.137151941657, Accuracy = 0.997999429703\n",
      "Iter #501120:  Learning rate = 0.004077:   Batch Loss = 0.166969, Accuracy = 0.987500011921\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.13914257288, Accuracy = 0.996284663677\n",
      "Iter #501760:  Learning rate = 0.004077:   Batch Loss = 0.123612, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.12612529099, Accuracy = 0.999428391457\n",
      "Iter #502400:  Learning rate = 0.004077:   Batch Loss = 0.122768, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.12459564209, Accuracy = 0.999142587185\n",
      "Iter #503040:  Learning rate = 0.004077:   Batch Loss = 0.120154, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.125966265798, Accuracy = 0.997427821159\n",
      "Iter #503680:  Learning rate = 0.004077:   Batch Loss = 0.124936, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.124586343765, Accuracy = 0.999428391457\n",
      "Iter #504320:  Learning rate = 0.004077:   Batch Loss = 0.122685, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.122439786792, Accuracy = 0.999714195728\n",
      "Iter #504960:  Learning rate = 0.004077:   Batch Loss = 0.117349, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.120717652142, Accuracy = 0.999428391457\n",
      "Iter #505600:  Learning rate = 0.004077:   Batch Loss = 0.116239, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.118948496878, Accuracy = 0.999428391457\n",
      "Iter #506240:  Learning rate = 0.004077:   Batch Loss = 0.119200, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.120256103575, Accuracy = 0.999714195728\n",
      "Iter #506880:  Learning rate = 0.004077:   Batch Loss = 0.116214, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.117858886719, Accuracy = 0.999428391457\n",
      "Iter #507520:  Learning rate = 0.004077:   Batch Loss = 0.118703, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.117726631463, Accuracy = 0.999428391457\n",
      "Iter #508160:  Learning rate = 0.004077:   Batch Loss = 0.115810, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.116737082601, Accuracy = 0.999428391457\n",
      "Iter #508800:  Learning rate = 0.004077:   Batch Loss = 0.115310, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.115847602487, Accuracy = 0.999714195728\n",
      "Iter #509440:  Learning rate = 0.004077:   Batch Loss = 0.116507, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.11555057019, Accuracy = 0.999714195728\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter #510080:  Learning rate = 0.004077:   Batch Loss = 0.115629, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.122314319015, Accuracy = 0.999142587185\n",
      "Iter #510720:  Learning rate = 0.004077:   Batch Loss = 0.115081, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.117820978165, Accuracy = 0.999428391457\n",
      "Iter #511360:  Learning rate = 0.004077:   Batch Loss = 0.119164, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.115198351443, Accuracy = 0.999428391457\n",
      "Iter #512000:  Learning rate = 0.004077:   Batch Loss = 0.112646, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.11546766758, Accuracy = 0.999428391457\n",
      "Iter #512640:  Learning rate = 0.004077:   Batch Loss = 0.114622, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.11465343833, Accuracy = 0.999428391457\n",
      "Iter #513280:  Learning rate = 0.004077:   Batch Loss = 0.114142, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.114892020822, Accuracy = 0.999428391457\n",
      "Iter #513920:  Learning rate = 0.004077:   Batch Loss = 0.113352, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.114081636071, Accuracy = 0.999714195728\n",
      "Iter #514560:  Learning rate = 0.004077:   Batch Loss = 0.113213, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.124261796474, Accuracy = 0.995713055134\n",
      "Iter #515200:  Learning rate = 0.004077:   Batch Loss = 0.112085, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.115193121135, Accuracy = 0.999142587185\n",
      "Iter #515840:  Learning rate = 0.004077:   Batch Loss = 0.112209, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.117221340537, Accuracy = 0.999428391457\n",
      "Iter #516480:  Learning rate = 0.004077:   Batch Loss = 0.112779, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.116198867559, Accuracy = 0.997999429703\n",
      "Iter #517120:  Learning rate = 0.004077:   Batch Loss = 0.110321, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.11223334074, Accuracy = 0.999714195728\n",
      "Iter #517760:  Learning rate = 0.004077:   Batch Loss = 0.128457, Accuracy = 0.987500011921\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.112679108977, Accuracy = 0.999714195728\n",
      "Iter #518400:  Learning rate = 0.004077:   Batch Loss = 0.110190, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.111648008227, Accuracy = 0.999714195728\n",
      "Iter #519040:  Learning rate = 0.004077:   Batch Loss = 0.114781, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.111597709358, Accuracy = 0.999714195728\n",
      "Iter #519680:  Learning rate = 0.004077:   Batch Loss = 0.111065, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.111125215888, Accuracy = 0.999714195728\n",
      "Iter #520320:  Learning rate = 0.004077:   Batch Loss = 0.110473, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.114046737552, Accuracy = 0.997713625431\n",
      "Iter #520960:  Learning rate = 0.004077:   Batch Loss = 0.109105, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.109958909452, Accuracy = 1.0\n",
      "Iter #521600:  Learning rate = 0.004077:   Batch Loss = 0.110455, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.111535139382, Accuracy = 0.999428391457\n",
      "Iter #522240:  Learning rate = 0.004077:   Batch Loss = 0.108662, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.110264740884, Accuracy = 0.999428391457\n",
      "Iter #522880:  Learning rate = 0.004077:   Batch Loss = 0.108014, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.109393320978, Accuracy = 0.999714195728\n",
      "Iter #523520:  Learning rate = 0.004077:   Batch Loss = 0.108723, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.10910269618, Accuracy = 1.0\n",
      "Iter #524160:  Learning rate = 0.004077:   Batch Loss = 0.107047, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.108760967851, Accuracy = 1.0\n",
      "Iter #524800:  Learning rate = 0.004077:   Batch Loss = 0.108003, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.108453094959, Accuracy = 1.0\n",
      "Iter #525440:  Learning rate = 0.004077:   Batch Loss = 0.110079, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.108235768974, Accuracy = 1.0\n",
      "Iter #526080:  Learning rate = 0.004077:   Batch Loss = 0.107744, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.108295351267, Accuracy = 0.999714195728\n",
      "Iter #526720:  Learning rate = 0.004077:   Batch Loss = 0.106861, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.107914663851, Accuracy = 1.0\n",
      "Iter #527360:  Learning rate = 0.004077:   Batch Loss = 0.107881, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.107829071581, Accuracy = 1.0\n",
      "Iter #528000:  Learning rate = 0.004077:   Batch Loss = 0.106350, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.107926875353, Accuracy = 0.999714195728\n",
      "Iter #528640:  Learning rate = 0.004077:   Batch Loss = 0.107666, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.109150394797, Accuracy = 0.999142587185\n",
      "Iter #529280:  Learning rate = 0.004077:   Batch Loss = 0.105959, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.107474222779, Accuracy = 1.0\n",
      "Iter #529920:  Learning rate = 0.004077:   Batch Loss = 0.108105, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.107739530504, Accuracy = 0.999428391457\n",
      "Iter #530560:  Learning rate = 0.004077:   Batch Loss = 0.108194, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.109498627484, Accuracy = 0.999142587185\n",
      "Iter #531200:  Learning rate = 0.004077:   Batch Loss = 0.111908, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.108836345375, Accuracy = 0.999428391457\n",
      "Iter #531840:  Learning rate = 0.004077:   Batch Loss = 0.108370, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.111042775214, Accuracy = 0.997999429703\n",
      "Iter #532480:  Learning rate = 0.004077:   Batch Loss = 0.105801, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.112060703337, Accuracy = 0.998856842518\n",
      "Iter #533120:  Learning rate = 0.004077:   Batch Loss = 0.108879, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.11034552753, Accuracy = 1.0\n",
      "Iter #533760:  Learning rate = 0.004077:   Batch Loss = 0.109116, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.108561187983, Accuracy = 1.0\n",
      "Iter #534400:  Learning rate = 0.004077:   Batch Loss = 0.108508, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.111996248364, Accuracy = 0.999142587185\n",
      "Iter #535040:  Learning rate = 0.004077:   Batch Loss = 0.107504, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.111081287265, Accuracy = 0.999142587185\n",
      "Iter #535680:  Learning rate = 0.004077:   Batch Loss = 0.107577, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.110377386212, Accuracy = 0.999428391457\n",
      "Iter #536320:  Learning rate = 0.004077:   Batch Loss = 0.124464, Accuracy = 0.987500011921\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.107043869793, Accuracy = 1.0\n",
      "Iter #536960:  Learning rate = 0.004077:   Batch Loss = 0.110422, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.112843699753, Accuracy = 0.999142587185\n",
      "Iter #537600:  Learning rate = 0.004077:   Batch Loss = 0.109788, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.109513491392, Accuracy = 0.999714195728\n",
      "Iter #538240:  Learning rate = 0.004077:   Batch Loss = 0.109339, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.111766055226, Accuracy = 0.998571038246\n",
      "Iter #538880:  Learning rate = 0.004077:   Batch Loss = 0.106677, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.107128210366, Accuracy = 0.999714195728\n",
      "Iter #539520:  Learning rate = 0.004077:   Batch Loss = 0.105864, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.106407739222, Accuracy = 0.999714195728\n",
      "Iter #540160:  Learning rate = 0.004077:   Batch Loss = 0.103632, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.105627462268, Accuracy = 0.999714195728\n",
      "Iter #540800:  Learning rate = 0.004077:   Batch Loss = 0.105592, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.10526534915, Accuracy = 0.999714195728\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter #541440:  Learning rate = 0.004077:   Batch Loss = 0.105307, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.107658341527, Accuracy = 0.999428391457\n",
      "Iter #542080:  Learning rate = 0.004077:   Batch Loss = 0.112590, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.119848057628, Accuracy = 0.996570467949\n",
      "Iter #542720:  Learning rate = 0.004077:   Batch Loss = 0.108741, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.114899739623, Accuracy = 0.997427821159\n",
      "Iter #543360:  Learning rate = 0.004077:   Batch Loss = 0.147150, Accuracy = 0.975000023842\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.134359419346, Accuracy = 0.991140305996\n",
      "Iter #544000:  Learning rate = 0.004077:   Batch Loss = 0.108545, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.10972301662, Accuracy = 0.999714195728\n",
      "Iter #544640:  Learning rate = 0.004077:   Batch Loss = 0.111995, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.109621725976, Accuracy = 0.998856842518\n",
      "Iter #545280:  Learning rate = 0.004077:   Batch Loss = 0.105970, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.107509464025, Accuracy = 0.999714195728\n",
      "Iter #545920:  Learning rate = 0.004077:   Batch Loss = 0.106067, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.113811925054, Accuracy = 0.996856272221\n",
      "Iter #546560:  Learning rate = 0.004077:   Batch Loss = 0.109243, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.108781419694, Accuracy = 0.999428391457\n",
      "Iter #547200:  Learning rate = 0.004077:   Batch Loss = 0.115051, Accuracy = 0.987500011921\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.107434622943, Accuracy = 0.999714195728\n",
      "Iter #547840:  Learning rate = 0.004077:   Batch Loss = 0.106784, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.11510565877, Accuracy = 0.997142016888\n",
      "Iter #548480:  Learning rate = 0.004077:   Batch Loss = 0.111817, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.112793952227, Accuracy = 1.0\n",
      "Iter #549120:  Learning rate = 0.004077:   Batch Loss = 0.107681, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.111872732639, Accuracy = 0.998571038246\n",
      "Iter #549760:  Learning rate = 0.004077:   Batch Loss = 0.105989, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.106729194522, Accuracy = 1.0\n",
      "Iter #550400:  Learning rate = 0.004077:   Batch Loss = 0.104772, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.105471171439, Accuracy = 1.0\n",
      "Iter #551040:  Learning rate = 0.004077:   Batch Loss = 0.104625, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.105088233948, Accuracy = 1.0\n",
      "Iter #551680:  Learning rate = 0.004077:   Batch Loss = 0.103891, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.104458674788, Accuracy = 1.0\n",
      "Iter #552320:  Learning rate = 0.004077:   Batch Loss = 0.102200, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.105487853289, Accuracy = 0.999428391457\n",
      "Iter #552960:  Learning rate = 0.004077:   Batch Loss = 0.102860, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.103523142636, Accuracy = 1.0\n",
      "Iter #553600:  Learning rate = 0.004077:   Batch Loss = 0.102413, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.104690432549, Accuracy = 0.999428391457\n",
      "Iter #554240:  Learning rate = 0.004077:   Batch Loss = 0.104351, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.104364767671, Accuracy = 0.999714195728\n",
      "Iter #554880:  Learning rate = 0.004077:   Batch Loss = 0.101720, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.102539919317, Accuracy = 1.0\n",
      "Iter #555520:  Learning rate = 0.004077:   Batch Loss = 0.101133, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.102324903011, Accuracy = 1.0\n",
      "Iter #556160:  Learning rate = 0.004077:   Batch Loss = 0.101949, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.102235659957, Accuracy = 1.0\n",
      "Iter #556800:  Learning rate = 0.004077:   Batch Loss = 0.100858, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.102111682296, Accuracy = 1.0\n",
      "Iter #557440:  Learning rate = 0.004077:   Batch Loss = 0.100395, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.10194003582, Accuracy = 1.0\n",
      "Iter #558080:  Learning rate = 0.004077:   Batch Loss = 0.101055, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.101726084948, Accuracy = 1.0\n",
      "Iter #558720:  Learning rate = 0.004077:   Batch Loss = 0.101087, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.101521149278, Accuracy = 1.0\n",
      "Iter #559360:  Learning rate = 0.004077:   Batch Loss = 0.100853, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.101386904716, Accuracy = 1.0\n",
      "Iter #560000:  Learning rate = 0.004077:   Batch Loss = 0.101943, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.101292446256, Accuracy = 1.0\n",
      "Iter #560640:  Learning rate = 0.004077:   Batch Loss = 0.099063, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.10110784322, Accuracy = 1.0\n",
      "Iter #561280:  Learning rate = 0.004077:   Batch Loss = 0.100422, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.101061798632, Accuracy = 1.0\n",
      "Iter #561920:  Learning rate = 0.004077:   Batch Loss = 0.101506, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.100906968117, Accuracy = 1.0\n",
      "Iter #562560:  Learning rate = 0.004077:   Batch Loss = 0.099040, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.100743293762, Accuracy = 1.0\n",
      "Iter #563200:  Learning rate = 0.004077:   Batch Loss = 0.101547, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.100617401302, Accuracy = 1.0\n",
      "Iter #563840:  Learning rate = 0.004077:   Batch Loss = 0.099638, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.100521765649, Accuracy = 1.0\n",
      "Iter #564480:  Learning rate = 0.004077:   Batch Loss = 0.099932, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.100420147181, Accuracy = 1.0\n",
      "Iter #565120:  Learning rate = 0.004077:   Batch Loss = 0.099839, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.100316330791, Accuracy = 1.0\n",
      "Iter #565760:  Learning rate = 0.004077:   Batch Loss = 0.099383, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.10021841526, Accuracy = 1.0\n",
      "Iter #566400:  Learning rate = 0.004077:   Batch Loss = 0.099549, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.100119099021, Accuracy = 1.0\n",
      "Iter #567040:  Learning rate = 0.004077:   Batch Loss = 0.099531, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.100038014352, Accuracy = 1.0\n",
      "Iter #567680:  Learning rate = 0.004077:   Batch Loss = 0.100007, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0999265760183, Accuracy = 1.0\n",
      "Iter #568320:  Learning rate = 0.004077:   Batch Loss = 0.099562, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0998254269361, Accuracy = 1.0\n",
      "Iter #568960:  Learning rate = 0.004077:   Batch Loss = 0.098888, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0997067466378, Accuracy = 1.0\n",
      "Iter #569600:  Learning rate = 0.004077:   Batch Loss = 0.099892, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0995837599039, Accuracy = 1.0\n",
      "Iter #570240:  Learning rate = 0.004077:   Batch Loss = 0.098743, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0994794219732, Accuracy = 1.0\n",
      "Iter #570880:  Learning rate = 0.004077:   Batch Loss = 0.099535, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0993789732456, Accuracy = 1.0\n",
      "Iter #571520:  Learning rate = 0.004077:   Batch Loss = 0.099069, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0993121042848, Accuracy = 1.0\n",
      "Iter #572160:  Learning rate = 0.004077:   Batch Loss = 0.097640, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0992304235697, Accuracy = 1.0\n",
      "Iter #572800:  Learning rate = 0.004077:   Batch Loss = 0.097887, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.099144116044, Accuracy = 1.0\n",
      "Iter #573440:  Learning rate = 0.004077:   Batch Loss = 0.099374, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0990592986345, Accuracy = 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter #574080:  Learning rate = 0.004077:   Batch Loss = 0.098062, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0989427566528, Accuracy = 1.0\n",
      "Iter #574720:  Learning rate = 0.004077:   Batch Loss = 0.099123, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0989543199539, Accuracy = 1.0\n",
      "Iter #575360:  Learning rate = 0.004077:   Batch Loss = 0.098461, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0988366156816, Accuracy = 1.0\n",
      "Iter #576000:  Learning rate = 0.004077:   Batch Loss = 0.097808, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0986699387431, Accuracy = 1.0\n",
      "Iter #576640:  Learning rate = 0.004077:   Batch Loss = 0.098276, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0986029654741, Accuracy = 1.0\n",
      "Iter #577280:  Learning rate = 0.004077:   Batch Loss = 0.098594, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.098509401083, Accuracy = 1.0\n",
      "Iter #577920:  Learning rate = 0.004077:   Batch Loss = 0.098826, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.098377853632, Accuracy = 1.0\n",
      "Iter #578560:  Learning rate = 0.004077:   Batch Loss = 0.097757, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0983314812183, Accuracy = 1.0\n",
      "Iter #579200:  Learning rate = 0.004077:   Batch Loss = 0.098829, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0982551202178, Accuracy = 1.0\n",
      "Iter #579840:  Learning rate = 0.004077:   Batch Loss = 0.097541, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0982125401497, Accuracy = 1.0\n",
      "Iter #580480:  Learning rate = 0.004077:   Batch Loss = 0.098175, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0980825945735, Accuracy = 1.0\n",
      "Iter #581120:  Learning rate = 0.004077:   Batch Loss = 0.097070, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0980189219117, Accuracy = 1.0\n",
      "Iter #581760:  Learning rate = 0.004077:   Batch Loss = 0.097922, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0978694707155, Accuracy = 1.0\n",
      "Iter #582400:  Learning rate = 0.004077:   Batch Loss = 0.097431, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0977588519454, Accuracy = 1.0\n",
      "Iter #583040:  Learning rate = 0.004077:   Batch Loss = 0.097402, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0976873040199, Accuracy = 1.0\n",
      "Iter #583680:  Learning rate = 0.004077:   Batch Loss = 0.097205, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0976029485464, Accuracy = 1.0\n",
      "Iter #584320:  Learning rate = 0.004077:   Batch Loss = 0.096488, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0976422280073, Accuracy = 1.0\n",
      "Iter #584960:  Learning rate = 0.004077:   Batch Loss = 0.095273, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0975461006165, Accuracy = 1.0\n",
      "Iter #585600:  Learning rate = 0.004077:   Batch Loss = 0.098138, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0973521322012, Accuracy = 1.0\n",
      "Iter #586240:  Learning rate = 0.004077:   Batch Loss = 0.097435, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0972751304507, Accuracy = 1.0\n",
      "Iter #586880:  Learning rate = 0.004077:   Batch Loss = 0.096261, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0971943438053, Accuracy = 1.0\n",
      "Iter #587520:  Learning rate = 0.004077:   Batch Loss = 0.098071, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0971053168178, Accuracy = 1.0\n",
      "Iter #588160:  Learning rate = 0.004077:   Batch Loss = 0.096007, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0969978049397, Accuracy = 1.0\n",
      "Iter #588800:  Learning rate = 0.004077:   Batch Loss = 0.096290, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0968760028481, Accuracy = 1.0\n",
      "Iter #589440:  Learning rate = 0.004077:   Batch Loss = 0.096695, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0968074053526, Accuracy = 1.0\n",
      "Iter #590080:  Learning rate = 0.004077:   Batch Loss = 0.097903, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0967293828726, Accuracy = 1.0\n",
      "Iter #590720:  Learning rate = 0.004077:   Batch Loss = 0.097045, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0966710224748, Accuracy = 1.0\n",
      "Iter #591360:  Learning rate = 0.004077:   Batch Loss = 0.096190, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.096526004374, Accuracy = 1.0\n",
      "Iter #592000:  Learning rate = 0.004077:   Batch Loss = 0.095914, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0964280217886, Accuracy = 1.0\n",
      "Iter #592640:  Learning rate = 0.004077:   Batch Loss = 0.095732, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0963308289647, Accuracy = 1.0\n",
      "Iter #593280:  Learning rate = 0.004077:   Batch Loss = 0.096511, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0963717550039, Accuracy = 1.0\n",
      "Iter #593920:  Learning rate = 0.004077:   Batch Loss = 0.094993, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0962016582489, Accuracy = 1.0\n",
      "Iter #594560:  Learning rate = 0.004077:   Batch Loss = 0.095305, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0961320996284, Accuracy = 1.0\n",
      "Iter #595200:  Learning rate = 0.004077:   Batch Loss = 0.095141, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0959993600845, Accuracy = 1.0\n",
      "Iter #595840:  Learning rate = 0.004077:   Batch Loss = 0.095222, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0959464758635, Accuracy = 1.0\n",
      "Iter #596480:  Learning rate = 0.004077:   Batch Loss = 0.096735, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0959133282304, Accuracy = 1.0\n",
      "Iter #597120:  Learning rate = 0.004077:   Batch Loss = 0.094125, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0957250893116, Accuracy = 1.0\n",
      "Iter #597760:  Learning rate = 0.004077:   Batch Loss = 0.094629, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0956374779344, Accuracy = 1.0\n",
      "Iter #598400:  Learning rate = 0.004077:   Batch Loss = 0.095064, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0952066630125, Accuracy = 1.0\n",
      "Iter #599040:  Learning rate = 0.004077:   Batch Loss = 0.093758, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0941948145628, Accuracy = 1.0\n",
      "Iter #599680:  Learning rate = 0.004077:   Batch Loss = 0.092457, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0934003442526, Accuracy = 1.0\n",
      "Iter #600320:  Learning rate = 0.003914:   Batch Loss = 0.092138, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0928716138005, Accuracy = 1.0\n",
      "Iter #600960:  Learning rate = 0.003914:   Batch Loss = 0.091652, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0925799608231, Accuracy = 1.0\n",
      "Iter #601600:  Learning rate = 0.003914:   Batch Loss = 0.092243, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.092275775969, Accuracy = 1.0\n",
      "Iter #602240:  Learning rate = 0.003914:   Batch Loss = 0.092603, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0919099673629, Accuracy = 1.0\n",
      "Iter #602880:  Learning rate = 0.003914:   Batch Loss = 0.090804, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0912921726704, Accuracy = 1.0\n",
      "Iter #603520:  Learning rate = 0.003914:   Batch Loss = 0.090756, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0909168049693, Accuracy = 1.0\n",
      "Iter #604160:  Learning rate = 0.003914:   Batch Loss = 0.090438, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0905995741487, Accuracy = 1.0\n",
      "Iter #604800:  Learning rate = 0.003914:   Batch Loss = 0.090467, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0904140174389, Accuracy = 1.0\n",
      "Iter #605440:  Learning rate = 0.003914:   Batch Loss = 0.090186, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0901835411787, Accuracy = 1.0\n",
      "Iter #606080:  Learning rate = 0.003914:   Batch Loss = 0.089234, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0900439694524, Accuracy = 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter #606720:  Learning rate = 0.003914:   Batch Loss = 0.089809, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0899062529206, Accuracy = 1.0\n",
      "Iter #607360:  Learning rate = 0.003914:   Batch Loss = 0.088621, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0897152945399, Accuracy = 1.0\n",
      "Iter #608000:  Learning rate = 0.003914:   Batch Loss = 0.088907, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.089734852314, Accuracy = 1.0\n",
      "Iter #608640:  Learning rate = 0.003914:   Batch Loss = 0.088963, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.089433118701, Accuracy = 1.0\n",
      "Iter #609280:  Learning rate = 0.003914:   Batch Loss = 0.088315, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0893917903304, Accuracy = 1.0\n",
      "Iter #609920:  Learning rate = 0.003914:   Batch Loss = 0.090740, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0899778902531, Accuracy = 1.0\n",
      "Iter #610560:  Learning rate = 0.003914:   Batch Loss = 0.089425, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0893782451749, Accuracy = 1.0\n",
      "Iter #611200:  Learning rate = 0.003914:   Batch Loss = 0.088594, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.089075781405, Accuracy = 1.0\n",
      "Iter #611840:  Learning rate = 0.003914:   Batch Loss = 0.088236, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0886525213718, Accuracy = 1.0\n",
      "Iter #612480:  Learning rate = 0.003914:   Batch Loss = 0.087489, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0885706543922, Accuracy = 1.0\n",
      "Iter #613120:  Learning rate = 0.003914:   Batch Loss = 0.089133, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0884054303169, Accuracy = 1.0\n",
      "Iter #613760:  Learning rate = 0.003914:   Batch Loss = 0.087650, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0881684944034, Accuracy = 1.0\n",
      "Iter #614400:  Learning rate = 0.003914:   Batch Loss = 0.088610, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0879801139235, Accuracy = 1.0\n",
      "Iter #615040:  Learning rate = 0.003914:   Batch Loss = 0.087455, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0878826901317, Accuracy = 1.0\n",
      "Iter #615680:  Learning rate = 0.003914:   Batch Loss = 0.087536, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0877339318395, Accuracy = 1.0\n",
      "Iter #616320:  Learning rate = 0.003914:   Batch Loss = 0.086139, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0876571759582, Accuracy = 1.0\n",
      "Iter #616960:  Learning rate = 0.003914:   Batch Loss = 0.087990, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0874937325716, Accuracy = 1.0\n",
      "Iter #617600:  Learning rate = 0.003914:   Batch Loss = 0.087911, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0873501598835, Accuracy = 1.0\n",
      "Iter #618240:  Learning rate = 0.003914:   Batch Loss = 0.087033, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0871791392565, Accuracy = 1.0\n",
      "Iter #618880:  Learning rate = 0.003914:   Batch Loss = 0.087441, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0871084481478, Accuracy = 1.0\n",
      "Iter #619520:  Learning rate = 0.003914:   Batch Loss = 0.087306, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0871578752995, Accuracy = 1.0\n",
      "Iter #620160:  Learning rate = 0.003914:   Batch Loss = 0.086636, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0869503319263, Accuracy = 1.0\n",
      "Iter #620800:  Learning rate = 0.003914:   Batch Loss = 0.086802, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0867818966508, Accuracy = 1.0\n",
      "Iter #621440:  Learning rate = 0.003914:   Batch Loss = 0.087060, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0867820978165, Accuracy = 1.0\n",
      "Iter #622080:  Learning rate = 0.003914:   Batch Loss = 0.086820, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0867110937834, Accuracy = 1.0\n",
      "Iter #622720:  Learning rate = 0.003914:   Batch Loss = 0.085146, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0865110009909, Accuracy = 1.0\n",
      "Iter #623360:  Learning rate = 0.003914:   Batch Loss = 0.086295, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0865185782313, Accuracy = 1.0\n",
      "Iter #624000:  Learning rate = 0.003914:   Batch Loss = 0.086439, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0864233151078, Accuracy = 1.0\n",
      "Iter #624640:  Learning rate = 0.003914:   Batch Loss = 0.085896, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0862817615271, Accuracy = 1.0\n",
      "Iter #625280:  Learning rate = 0.003914:   Batch Loss = 0.085744, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0861727073789, Accuracy = 1.0\n",
      "Iter #625920:  Learning rate = 0.003914:   Batch Loss = 0.085552, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0860672965646, Accuracy = 1.0\n",
      "Iter #626560:  Learning rate = 0.003914:   Batch Loss = 0.086134, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0860420614481, Accuracy = 1.0\n",
      "Iter #627200:  Learning rate = 0.003914:   Batch Loss = 0.085586, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0859548822045, Accuracy = 1.0\n",
      "Iter #627840:  Learning rate = 0.003914:   Batch Loss = 0.085482, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0859477445483, Accuracy = 1.0\n",
      "Iter #628480:  Learning rate = 0.003914:   Batch Loss = 0.084961, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0858164802194, Accuracy = 1.0\n",
      "Iter #629120:  Learning rate = 0.003914:   Batch Loss = 0.085448, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0858224779367, Accuracy = 1.0\n",
      "Iter #629760:  Learning rate = 0.003914:   Batch Loss = 0.085272, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0856508612633, Accuracy = 1.0\n",
      "Iter #630400:  Learning rate = 0.003914:   Batch Loss = 0.085142, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0855823606253, Accuracy = 1.0\n",
      "Iter #631040:  Learning rate = 0.003914:   Batch Loss = 0.086012, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.085614413023, Accuracy = 1.0\n",
      "Iter #631680:  Learning rate = 0.003914:   Batch Loss = 0.084717, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0854776650667, Accuracy = 1.0\n",
      "Iter #632320:  Learning rate = 0.003914:   Batch Loss = 0.085536, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0854698270559, Accuracy = 1.0\n",
      "Iter #632960:  Learning rate = 0.003914:   Batch Loss = 0.085318, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0854345932603, Accuracy = 1.0\n",
      "Iter #633600:  Learning rate = 0.003914:   Batch Loss = 0.084605, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.085263684392, Accuracy = 1.0\n",
      "Iter #634240:  Learning rate = 0.003914:   Batch Loss = 0.085104, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0853302404284, Accuracy = 1.0\n",
      "Iter #634880:  Learning rate = 0.003914:   Batch Loss = 0.084972, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0851827412844, Accuracy = 1.0\n",
      "Iter #635520:  Learning rate = 0.003914:   Batch Loss = 0.084806, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0850658118725, Accuracy = 1.0\n",
      "Iter #636160:  Learning rate = 0.003914:   Batch Loss = 0.084888, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0850380510092, Accuracy = 1.0\n",
      "Iter #636800:  Learning rate = 0.003914:   Batch Loss = 0.085270, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0849992036819, Accuracy = 1.0\n",
      "Iter #637440:  Learning rate = 0.003914:   Batch Loss = 0.084614, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0849220082164, Accuracy = 1.0\n",
      "Iter #638080:  Learning rate = 0.003914:   Batch Loss = 0.084622, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0848748609424, Accuracy = 1.0\n",
      "Iter #638720:  Learning rate = 0.003914:   Batch Loss = 0.084449, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.084844134748, Accuracy = 1.0\n",
      "Iter #639360:  Learning rate = 0.003914:   Batch Loss = 0.083564, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0847413316369, Accuracy = 1.0\n",
      "Iter #640000:  Learning rate = 0.003914:   Batch Loss = 0.084468, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0846775770187, Accuracy = 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter #640640:  Learning rate = 0.003914:   Batch Loss = 0.084734, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0846440345049, Accuracy = 1.0\n",
      "Iter #641280:  Learning rate = 0.003914:   Batch Loss = 0.084348, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0846401229501, Accuracy = 1.0\n",
      "Iter #641920:  Learning rate = 0.003914:   Batch Loss = 0.084582, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0846004709601, Accuracy = 1.0\n",
      "Iter #642560:  Learning rate = 0.003914:   Batch Loss = 0.083888, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0844727680087, Accuracy = 1.0\n",
      "Iter #643200:  Learning rate = 0.003914:   Batch Loss = 0.083348, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0844383612275, Accuracy = 1.0\n",
      "Iter #643840:  Learning rate = 0.003914:   Batch Loss = 0.084716, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0849315449595, Accuracy = 1.0\n",
      "Iter #644480:  Learning rate = 0.003914:   Batch Loss = 0.084162, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0844155550003, Accuracy = 1.0\n",
      "Iter #645120:  Learning rate = 0.003914:   Batch Loss = 0.083446, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0847120210528, Accuracy = 1.0\n",
      "Iter #645760:  Learning rate = 0.003914:   Batch Loss = 0.084701, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0843767523766, Accuracy = 1.0\n",
      "Iter #646400:  Learning rate = 0.003914:   Batch Loss = 0.083597, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.084260083735, Accuracy = 1.0\n",
      "Iter #647040:  Learning rate = 0.003914:   Batch Loss = 0.084588, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0841903463006, Accuracy = 1.0\n",
      "Iter #647680:  Learning rate = 0.003914:   Batch Loss = 0.084024, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0841245055199, Accuracy = 1.0\n",
      "Iter #648320:  Learning rate = 0.003914:   Batch Loss = 0.083935, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0840648412704, Accuracy = 1.0\n",
      "Iter #648960:  Learning rate = 0.003914:   Batch Loss = 0.083709, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0839784145355, Accuracy = 1.0\n",
      "Iter #649600:  Learning rate = 0.003914:   Batch Loss = 0.084120, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0839468464255, Accuracy = 1.0\n",
      "Iter #650240:  Learning rate = 0.003914:   Batch Loss = 0.084120, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0839319527149, Accuracy = 1.0\n",
      "Iter #650880:  Learning rate = 0.003914:   Batch Loss = 0.083481, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0839259624481, Accuracy = 1.0\n",
      "Iter #651520:  Learning rate = 0.003914:   Batch Loss = 0.082686, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0838413983583, Accuracy = 1.0\n",
      "Iter #652160:  Learning rate = 0.003914:   Batch Loss = 0.083595, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0837897956371, Accuracy = 1.0\n",
      "Iter #652800:  Learning rate = 0.003914:   Batch Loss = 0.083033, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0837702080607, Accuracy = 1.0\n",
      "Iter #653440:  Learning rate = 0.003914:   Batch Loss = 0.082891, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0837123915553, Accuracy = 1.0\n",
      "Iter #654080:  Learning rate = 0.003914:   Batch Loss = 0.083122, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0836479365826, Accuracy = 1.0\n",
      "Iter #654720:  Learning rate = 0.003914:   Batch Loss = 0.083066, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0835958719254, Accuracy = 1.0\n",
      "Iter #655360:  Learning rate = 0.003914:   Batch Loss = 0.082894, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0835767611861, Accuracy = 1.0\n",
      "Iter #656000:  Learning rate = 0.003914:   Batch Loss = 0.083208, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0835684835911, Accuracy = 1.0\n",
      "Iter #656640:  Learning rate = 0.003914:   Batch Loss = 0.083256, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0835446491838, Accuracy = 1.0\n",
      "Iter #657280:  Learning rate = 0.003914:   Batch Loss = 0.083248, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0835065767169, Accuracy = 1.0\n",
      "Iter #657920:  Learning rate = 0.003914:   Batch Loss = 0.082944, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.083371669054, Accuracy = 1.0\n",
      "Iter #658560:  Learning rate = 0.003914:   Batch Loss = 0.082982, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.083396807313, Accuracy = 1.0\n",
      "Iter #659200:  Learning rate = 0.003914:   Batch Loss = 0.082612, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0833767056465, Accuracy = 1.0\n",
      "Iter #659840:  Learning rate = 0.003914:   Batch Loss = 0.083423, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0832976400852, Accuracy = 1.0\n",
      "Iter #660480:  Learning rate = 0.003914:   Batch Loss = 0.083534, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0832502990961, Accuracy = 1.0\n",
      "Iter #661120:  Learning rate = 0.003914:   Batch Loss = 0.083144, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0832635983825, Accuracy = 1.0\n",
      "Iter #661760:  Learning rate = 0.003914:   Batch Loss = 0.083065, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0833065062761, Accuracy = 1.0\n",
      "Iter #662400:  Learning rate = 0.003914:   Batch Loss = 0.082479, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0831909030676, Accuracy = 1.0\n",
      "Iter #663040:  Learning rate = 0.003914:   Batch Loss = 0.082150, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0830943956971, Accuracy = 1.0\n",
      "Iter #663680:  Learning rate = 0.003914:   Batch Loss = 0.082974, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0831105038524, Accuracy = 1.0\n",
      "Iter #664320:  Learning rate = 0.003914:   Batch Loss = 0.083005, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0831177830696, Accuracy = 1.0\n",
      "Iter #664960:  Learning rate = 0.003914:   Batch Loss = 0.082683, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0831241905689, Accuracy = 1.0\n",
      "Iter #665600:  Learning rate = 0.003914:   Batch Loss = 0.082189, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0829526036978, Accuracy = 1.0\n",
      "Iter #666240:  Learning rate = 0.003914:   Batch Loss = 0.081921, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0828640311956, Accuracy = 1.0\n",
      "Iter #666880:  Learning rate = 0.003914:   Batch Loss = 0.081958, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0923032462597, Accuracy = 0.998571038246\n",
      "Iter #667520:  Learning rate = 0.003914:   Batch Loss = 0.084058, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0857159793377, Accuracy = 1.0\n",
      "Iter #668160:  Learning rate = 0.003914:   Batch Loss = 0.083161, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0841114297509, Accuracy = 0.999714195728\n",
      "Iter #668800:  Learning rate = 0.003914:   Batch Loss = 0.082071, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0831011161208, Accuracy = 1.0\n",
      "Iter #669440:  Learning rate = 0.003914:   Batch Loss = 0.082916, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0834241509438, Accuracy = 0.999714195728\n",
      "Iter #670080:  Learning rate = 0.003914:   Batch Loss = 0.082088, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0843344405293, Accuracy = 0.999714195728\n",
      "Iter #670720:  Learning rate = 0.003914:   Batch Loss = 0.083965, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0869245827198, Accuracy = 0.998285233974\n",
      "Iter #671360:  Learning rate = 0.003914:   Batch Loss = 0.083710, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0885116383433, Accuracy = 0.999428391457\n",
      "Iter #672000:  Learning rate = 0.003914:   Batch Loss = 0.084495, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0860141143203, Accuracy = 1.0\n",
      "Iter #672640:  Learning rate = 0.003914:   Batch Loss = 0.085138, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0851008445024, Accuracy = 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter #673280:  Learning rate = 0.003914:   Batch Loss = 0.082465, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0840951725841, Accuracy = 0.999714195728\n",
      "Iter #673920:  Learning rate = 0.003914:   Batch Loss = 0.083292, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0830828994513, Accuracy = 1.0\n",
      "Iter #674560:  Learning rate = 0.003914:   Batch Loss = 0.082901, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0833875536919, Accuracy = 1.0\n",
      "Iter #675200:  Learning rate = 0.003914:   Batch Loss = 0.082558, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0831938013434, Accuracy = 0.999714195728\n",
      "Iter #675840:  Learning rate = 0.003914:   Batch Loss = 0.082151, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0994323045015, Accuracy = 0.993426680565\n",
      "Iter #676480:  Learning rate = 0.003914:   Batch Loss = 0.099995, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0965369045734, Accuracy = 0.999428391457\n",
      "Iter #677120:  Learning rate = 0.003914:   Batch Loss = 0.105174, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0924386754632, Accuracy = 0.999714195728\n",
      "Iter #677760:  Learning rate = 0.003914:   Batch Loss = 0.088038, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0889063104987, Accuracy = 0.999714195728\n",
      "Iter #678400:  Learning rate = 0.003914:   Batch Loss = 0.086780, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0855032652617, Accuracy = 1.0\n",
      "Iter #679040:  Learning rate = 0.003914:   Batch Loss = 0.084194, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0865618139505, Accuracy = 0.998856842518\n",
      "Iter #679680:  Learning rate = 0.003914:   Batch Loss = 0.091252, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0965474322438, Accuracy = 0.996570467949\n",
      "Iter #680320:  Learning rate = 0.003914:   Batch Loss = 0.087142, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.222231030464, Accuracy = 0.927122056484\n",
      "Iter #680960:  Learning rate = 0.003914:   Batch Loss = 0.546819, Accuracy = 0.800000011921\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.426052212715, Accuracy = 0.85281509161\n",
      "Iter #681600:  Learning rate = 0.003914:   Batch Loss = 0.333505, Accuracy = 0.949999988079\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.214899107814, Accuracy = 0.979136884212\n",
      "Iter #682240:  Learning rate = 0.003914:   Batch Loss = 0.168923, Accuracy = 0.975000023842\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.14004085958, Accuracy = 0.98942553997\n",
      "Iter #682880:  Learning rate = 0.003914:   Batch Loss = 0.115457, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.115217976272, Accuracy = 0.998856842518\n",
      "Iter #683520:  Learning rate = 0.003914:   Batch Loss = 0.130907, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.142087593675, Accuracy = 0.988282382488\n",
      "Iter #684160:  Learning rate = 0.003914:   Batch Loss = 0.107814, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.110378831625, Accuracy = 0.996570467949\n",
      "Iter #684800:  Learning rate = 0.003914:   Batch Loss = 0.095700, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0961970761418, Accuracy = 0.999428391457\n",
      "Iter #685440:  Learning rate = 0.003914:   Batch Loss = 0.096700, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.105734564364, Accuracy = 0.995713055134\n",
      "Iter #686080:  Learning rate = 0.003914:   Batch Loss = 0.093823, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.108543485403, Accuracy = 0.995427250862\n",
      "Iter #686720:  Learning rate = 0.003914:   Batch Loss = 0.089289, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0929904133081, Accuracy = 0.999142587185\n",
      "Iter #687360:  Learning rate = 0.003914:   Batch Loss = 0.089010, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.090304479003, Accuracy = 0.999428391457\n",
      "Iter #688000:  Learning rate = 0.003914:   Batch Loss = 0.087575, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0876395627856, Accuracy = 1.0\n",
      "Iter #688640:  Learning rate = 0.003914:   Batch Loss = 0.086102, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0863432064652, Accuracy = 1.0\n",
      "Iter #689280:  Learning rate = 0.003914:   Batch Loss = 0.084508, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0859245657921, Accuracy = 1.0\n",
      "Iter #689920:  Learning rate = 0.003914:   Batch Loss = 0.084817, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0851917564869, Accuracy = 1.0\n",
      "Iter #690560:  Learning rate = 0.003914:   Batch Loss = 0.083800, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0846662297845, Accuracy = 1.0\n",
      "Iter #691200:  Learning rate = 0.003914:   Batch Loss = 0.083712, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0843974947929, Accuracy = 1.0\n",
      "Iter #691840:  Learning rate = 0.003914:   Batch Loss = 0.083873, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0838478207588, Accuracy = 1.0\n",
      "Iter #692480:  Learning rate = 0.003914:   Batch Loss = 0.083195, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0837856382132, Accuracy = 1.0\n",
      "Iter #693120:  Learning rate = 0.003914:   Batch Loss = 0.083493, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0835400894284, Accuracy = 1.0\n",
      "Iter #693760:  Learning rate = 0.003914:   Batch Loss = 0.083192, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0833498835564, Accuracy = 1.0\n",
      "Iter #694400:  Learning rate = 0.003914:   Batch Loss = 0.083685, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0831543728709, Accuracy = 1.0\n",
      "Iter #695040:  Learning rate = 0.003914:   Batch Loss = 0.087547, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0830976143479, Accuracy = 1.0\n",
      "Iter #695680:  Learning rate = 0.003914:   Batch Loss = 0.082853, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.086184412241, Accuracy = 0.998285233974\n",
      "Iter #696320:  Learning rate = 0.003914:   Batch Loss = 0.081939, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0831077992916, Accuracy = 1.0\n",
      "Iter #696960:  Learning rate = 0.003914:   Batch Loss = 0.082299, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0827803686261, Accuracy = 1.0\n",
      "Iter #697600:  Learning rate = 0.003914:   Batch Loss = 0.081699, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0826409161091, Accuracy = 1.0\n",
      "Iter #698240:  Learning rate = 0.003914:   Batch Loss = 0.082348, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0826267898083, Accuracy = 1.0\n",
      "Iter #698880:  Learning rate = 0.003914:   Batch Loss = 0.081650, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0824216008186, Accuracy = 1.0\n",
      "Iter #699520:  Learning rate = 0.003914:   Batch Loss = 0.081528, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0823730677366, Accuracy = 1.0\n",
      "Iter #700160:  Learning rate = 0.003757:   Batch Loss = 0.081070, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0821395814419, Accuracy = 1.0\n",
      "Iter #700800:  Learning rate = 0.003757:   Batch Loss = 0.081731, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.082037769258, Accuracy = 1.0\n",
      "Iter #701440:  Learning rate = 0.003757:   Batch Loss = 0.081831, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0819166079164, Accuracy = 1.0\n",
      "Iter #702080:  Learning rate = 0.003757:   Batch Loss = 0.081959, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.081721663475, Accuracy = 1.0\n",
      "Iter #702720:  Learning rate = 0.003757:   Batch Loss = 0.082527, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0816386044025, Accuracy = 1.0\n",
      "Iter #703360:  Learning rate = 0.003757:   Batch Loss = 0.081188, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0815532952547, Accuracy = 1.0\n",
      "Iter #704000:  Learning rate = 0.003757:   Batch Loss = 0.081444, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0815176144242, Accuracy = 1.0\n",
      "Iter #704640:  Learning rate = 0.003757:   Batch Loss = 0.081333, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0813832804561, Accuracy = 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter #705280:  Learning rate = 0.003757:   Batch Loss = 0.080569, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0812922641635, Accuracy = 1.0\n",
      "Iter #705920:  Learning rate = 0.003757:   Batch Loss = 0.081838, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0812124758959, Accuracy = 1.0\n",
      "Iter #706560:  Learning rate = 0.003757:   Batch Loss = 0.080412, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0811270922422, Accuracy = 1.0\n",
      "Iter #707200:  Learning rate = 0.003757:   Batch Loss = 0.081013, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0810496509075, Accuracy = 1.0\n",
      "Iter #707840:  Learning rate = 0.003757:   Batch Loss = 0.081833, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0810660421848, Accuracy = 1.0\n",
      "Iter #708480:  Learning rate = 0.003757:   Batch Loss = 0.080311, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0810834467411, Accuracy = 1.0\n",
      "Iter #709120:  Learning rate = 0.003757:   Batch Loss = 0.079755, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0809455811977, Accuracy = 1.0\n",
      "Iter #709760:  Learning rate = 0.003757:   Batch Loss = 0.080751, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0807755440474, Accuracy = 1.0\n",
      "Iter #710400:  Learning rate = 0.003757:   Batch Loss = 0.080361, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0807080045342, Accuracy = 1.0\n",
      "Iter #711040:  Learning rate = 0.003757:   Batch Loss = 0.080529, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0806301534176, Accuracy = 1.0\n",
      "Iter #711680:  Learning rate = 0.003757:   Batch Loss = 0.080502, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.081317640841, Accuracy = 1.0\n",
      "Iter #712320:  Learning rate = 0.003757:   Batch Loss = 0.080206, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0805481821299, Accuracy = 1.0\n",
      "Iter #712960:  Learning rate = 0.003757:   Batch Loss = 0.081101, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0804391130805, Accuracy = 1.0\n",
      "Iter #713600:  Learning rate = 0.003757:   Batch Loss = 0.080473, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0804579481483, Accuracy = 1.0\n",
      "Iter #714240:  Learning rate = 0.003757:   Batch Loss = 0.080135, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0803530216217, Accuracy = 1.0\n",
      "Iter #714880:  Learning rate = 0.003757:   Batch Loss = 0.079944, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0802879109979, Accuracy = 1.0\n",
      "Iter #715520:  Learning rate = 0.003757:   Batch Loss = 0.079534, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0802420973778, Accuracy = 1.0\n",
      "Iter #716160:  Learning rate = 0.003757:   Batch Loss = 0.080417, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0801520720124, Accuracy = 1.0\n",
      "Iter #716800:  Learning rate = 0.003757:   Batch Loss = 0.079854, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0801081508398, Accuracy = 1.0\n",
      "Iter #717440:  Learning rate = 0.003757:   Batch Loss = 0.079961, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.079991184175, Accuracy = 1.0\n",
      "Iter #718080:  Learning rate = 0.003757:   Batch Loss = 0.079883, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0798947662115, Accuracy = 1.0\n",
      "Iter #718720:  Learning rate = 0.003757:   Batch Loss = 0.078610, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0797049999237, Accuracy = 1.0\n",
      "Iter #719360:  Learning rate = 0.003757:   Batch Loss = 0.079306, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0795343741775, Accuracy = 1.0\n",
      "Iter #720000:  Learning rate = 0.003757:   Batch Loss = 0.079299, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0792578086257, Accuracy = 1.0\n",
      "Iter #720640:  Learning rate = 0.003757:   Batch Loss = 0.079591, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0789556205273, Accuracy = 1.0\n",
      "Iter #721280:  Learning rate = 0.003757:   Batch Loss = 0.078220, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0786243751645, Accuracy = 1.0\n",
      "Iter #721920:  Learning rate = 0.003757:   Batch Loss = 0.077613, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0783720389009, Accuracy = 1.0\n",
      "Iter #722560:  Learning rate = 0.003757:   Batch Loss = 0.077393, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0782069340348, Accuracy = 1.0\n",
      "Iter #723200:  Learning rate = 0.003757:   Batch Loss = 0.077508, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0780235901475, Accuracy = 1.0\n",
      "Iter #723840:  Learning rate = 0.003757:   Batch Loss = 0.077923, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0778833627701, Accuracy = 1.0\n",
      "Iter #724480:  Learning rate = 0.003757:   Batch Loss = 0.077560, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0777763426304, Accuracy = 1.0\n",
      "Iter #725120:  Learning rate = 0.003757:   Batch Loss = 0.077246, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0776892825961, Accuracy = 1.0\n",
      "Iter #725760:  Learning rate = 0.003757:   Batch Loss = 0.077714, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0775687247515, Accuracy = 1.0\n",
      "Iter #726400:  Learning rate = 0.003757:   Batch Loss = 0.077249, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0774680301547, Accuracy = 1.0\n",
      "Iter #727040:  Learning rate = 0.003757:   Batch Loss = 0.076737, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0774237290025, Accuracy = 1.0\n",
      "Iter #727680:  Learning rate = 0.003757:   Batch Loss = 0.077347, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0773443728685, Accuracy = 1.0\n",
      "Iter #728320:  Learning rate = 0.003757:   Batch Loss = 0.078070, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0772576257586, Accuracy = 1.0\n",
      "Iter #728960:  Learning rate = 0.003757:   Batch Loss = 0.077137, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0771799683571, Accuracy = 1.0\n",
      "Iter #729600:  Learning rate = 0.003757:   Batch Loss = 0.076207, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0771390944719, Accuracy = 1.0\n",
      "Iter #730240:  Learning rate = 0.003757:   Batch Loss = 0.076776, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0771391764283, Accuracy = 1.0\n",
      "Iter #730880:  Learning rate = 0.003757:   Batch Loss = 0.076512, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0770322233438, Accuracy = 1.0\n",
      "Iter #731520:  Learning rate = 0.003757:   Batch Loss = 0.076389, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.076980188489, Accuracy = 1.0\n",
      "Iter #732160:  Learning rate = 0.003757:   Batch Loss = 0.077325, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0769207254052, Accuracy = 1.0\n",
      "Iter #732800:  Learning rate = 0.003757:   Batch Loss = 0.076275, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0768455266953, Accuracy = 1.0\n",
      "Iter #733440:  Learning rate = 0.003757:   Batch Loss = 0.076994, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0767979770899, Accuracy = 1.0\n",
      "Iter #734080:  Learning rate = 0.003757:   Batch Loss = 0.076319, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0767888873816, Accuracy = 1.0\n",
      "Iter #734720:  Learning rate = 0.003757:   Batch Loss = 0.076338, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.076713129878, Accuracy = 1.0\n",
      "Iter #735360:  Learning rate = 0.003757:   Batch Loss = 0.076190, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0767005234957, Accuracy = 1.0\n",
      "Iter #736000:  Learning rate = 0.003757:   Batch Loss = 0.075789, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0766556710005, Accuracy = 1.0\n",
      "Iter #736640:  Learning rate = 0.003757:   Batch Loss = 0.076753, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0766343176365, Accuracy = 1.0\n",
      "Iter #737280:  Learning rate = 0.003757:   Batch Loss = 0.076512, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0765497088432, Accuracy = 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter #737920:  Learning rate = 0.003757:   Batch Loss = 0.076626, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0764814987779, Accuracy = 1.0\n",
      "Iter #738560:  Learning rate = 0.003757:   Batch Loss = 0.075918, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0764637738466, Accuracy = 1.0\n",
      "Iter #739200:  Learning rate = 0.003757:   Batch Loss = 0.076121, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0764493793249, Accuracy = 1.0\n",
      "Iter #739840:  Learning rate = 0.003757:   Batch Loss = 0.075968, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0763821229339, Accuracy = 1.0\n",
      "Iter #740480:  Learning rate = 0.003757:   Batch Loss = 0.075935, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0763684809208, Accuracy = 1.0\n",
      "Iter #741120:  Learning rate = 0.003757:   Batch Loss = 0.075945, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0763773098588, Accuracy = 1.0\n",
      "Iter #741760:  Learning rate = 0.003757:   Batch Loss = 0.076298, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.076381675899, Accuracy = 1.0\n",
      "Iter #742400:  Learning rate = 0.003757:   Batch Loss = 0.076263, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0762831866741, Accuracy = 1.0\n",
      "Iter #743040:  Learning rate = 0.003757:   Batch Loss = 0.076221, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0762095823884, Accuracy = 1.0\n",
      "Iter #743680:  Learning rate = 0.003757:   Batch Loss = 0.075992, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0761748254299, Accuracy = 1.0\n",
      "Iter #744320:  Learning rate = 0.003757:   Batch Loss = 0.076030, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0761421173811, Accuracy = 1.0\n",
      "Iter #744960:  Learning rate = 0.003757:   Batch Loss = 0.074771, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0761331021786, Accuracy = 1.0\n",
      "Iter #745600:  Learning rate = 0.003757:   Batch Loss = 0.076131, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0761390551925, Accuracy = 1.0\n",
      "Iter #746240:  Learning rate = 0.003757:   Batch Loss = 0.076539, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0760745927691, Accuracy = 1.0\n",
      "Iter #746880:  Learning rate = 0.003757:   Batch Loss = 0.075655, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.07600659132, Accuracy = 1.0\n",
      "Iter #747520:  Learning rate = 0.003757:   Batch Loss = 0.075499, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0760370194912, Accuracy = 1.0\n",
      "Iter #748160:  Learning rate = 0.003757:   Batch Loss = 0.076383, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.076022259891, Accuracy = 1.0\n",
      "Iter #748800:  Learning rate = 0.003757:   Batch Loss = 0.075536, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0759730488062, Accuracy = 1.0\n",
      "Iter #749440:  Learning rate = 0.003757:   Batch Loss = 0.075278, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0759237259626, Accuracy = 1.0\n",
      "Iter #750080:  Learning rate = 0.003757:   Batch Loss = 0.076611, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0759122148156, Accuracy = 1.0\n",
      "Iter #750720:  Learning rate = 0.003757:   Batch Loss = 0.075956, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0759441107512, Accuracy = 1.0\n",
      "Iter #751360:  Learning rate = 0.003757:   Batch Loss = 0.075282, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0759017542005, Accuracy = 1.0\n",
      "Iter #752000:  Learning rate = 0.003757:   Batch Loss = 0.074803, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0758825615048, Accuracy = 1.0\n",
      "Iter #752640:  Learning rate = 0.003757:   Batch Loss = 0.075596, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0758097320795, Accuracy = 1.0\n",
      "Iter #753280:  Learning rate = 0.003757:   Batch Loss = 0.076368, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0757966712117, Accuracy = 1.0\n",
      "Iter #753920:  Learning rate = 0.003757:   Batch Loss = 0.075550, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0758145749569, Accuracy = 1.0\n",
      "Iter #754560:  Learning rate = 0.003757:   Batch Loss = 0.076023, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0759625583887, Accuracy = 1.0\n",
      "Iter #755200:  Learning rate = 0.003757:   Batch Loss = 0.075093, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0757655650377, Accuracy = 1.0\n",
      "Iter #755840:  Learning rate = 0.003757:   Batch Loss = 0.075265, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0757496133447, Accuracy = 1.0\n",
      "Iter #756480:  Learning rate = 0.003757:   Batch Loss = 0.075423, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0758415535092, Accuracy = 1.0\n",
      "Iter #757120:  Learning rate = 0.003757:   Batch Loss = 0.075890, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.075690060854, Accuracy = 1.0\n",
      "Iter #757760:  Learning rate = 0.003757:   Batch Loss = 0.075248, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0756751745939, Accuracy = 1.0\n",
      "Iter #758400:  Learning rate = 0.003757:   Batch Loss = 0.075337, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0756587833166, Accuracy = 1.0\n",
      "Iter #759040:  Learning rate = 0.003757:   Batch Loss = 0.076257, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0756276845932, Accuracy = 1.0\n",
      "Iter #759680:  Learning rate = 0.003757:   Batch Loss = 0.075483, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0756159201264, Accuracy = 1.0\n",
      "Iter #760320:  Learning rate = 0.003757:   Batch Loss = 0.076017, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0756324976683, Accuracy = 1.0\n",
      "Iter #760960:  Learning rate = 0.003757:   Batch Loss = 0.075538, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0755672752857, Accuracy = 1.0\n",
      "Iter #761600:  Learning rate = 0.003757:   Batch Loss = 0.074867, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0755739286542, Accuracy = 1.0\n",
      "Iter #762240:  Learning rate = 0.003757:   Batch Loss = 0.074838, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0755565017462, Accuracy = 1.0\n",
      "Iter #762880:  Learning rate = 0.003757:   Batch Loss = 0.075541, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0755030736327, Accuracy = 1.0\n",
      "Iter #763520:  Learning rate = 0.003757:   Batch Loss = 0.075712, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0754680633545, Accuracy = 1.0\n",
      "Iter #764160:  Learning rate = 0.003757:   Batch Loss = 0.075184, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.075471162796, Accuracy = 1.0\n",
      "Iter #764800:  Learning rate = 0.003757:   Batch Loss = 0.074542, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0754249840975, Accuracy = 1.0\n",
      "Iter #765440:  Learning rate = 0.003757:   Batch Loss = 0.075076, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0754062235355, Accuracy = 1.0\n",
      "Iter #766080:  Learning rate = 0.003757:   Batch Loss = 0.075126, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.075458355248, Accuracy = 1.0\n",
      "Iter #766720:  Learning rate = 0.003757:   Batch Loss = 0.075530, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0754982829094, Accuracy = 1.0\n",
      "Iter #767360:  Learning rate = 0.003757:   Batch Loss = 0.075878, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0754954516888, Accuracy = 1.0\n",
      "Iter #768000:  Learning rate = 0.003757:   Batch Loss = 0.075340, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0754116848111, Accuracy = 1.0\n",
      "Iter #768640:  Learning rate = 0.003757:   Batch Loss = 0.075249, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0753785669804, Accuracy = 1.0\n",
      "Iter #769280:  Learning rate = 0.003757:   Batch Loss = 0.075009, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0752995237708, Accuracy = 1.0\n",
      "Iter #769920:  Learning rate = 0.003757:   Batch Loss = 0.075725, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0753078907728, Accuracy = 1.0\n",
      "Iter #770560:  Learning rate = 0.003757:   Batch Loss = 0.075533, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0753200501204, Accuracy = 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter #771200:  Learning rate = 0.003757:   Batch Loss = 0.075172, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0752649754286, Accuracy = 1.0\n",
      "Iter #771840:  Learning rate = 0.003757:   Batch Loss = 0.074712, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0752859860659, Accuracy = 1.0\n",
      "Iter #772480:  Learning rate = 0.003757:   Batch Loss = 0.075386, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0753221958876, Accuracy = 1.0\n",
      "Iter #773120:  Learning rate = 0.003757:   Batch Loss = 0.075120, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0752669498324, Accuracy = 1.0\n",
      "Iter #773760:  Learning rate = 0.003757:   Batch Loss = 0.074453, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0752370133996, Accuracy = 1.0\n",
      "Iter #774400:  Learning rate = 0.003757:   Batch Loss = 0.074674, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.075176179409, Accuracy = 1.0\n",
      "Iter #775040:  Learning rate = 0.003757:   Batch Loss = 0.074791, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0751804336905, Accuracy = 1.0\n",
      "Iter #775680:  Learning rate = 0.003757:   Batch Loss = 0.074464, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.075165450573, Accuracy = 1.0\n",
      "Iter #776320:  Learning rate = 0.003757:   Batch Loss = 0.074894, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.075154542923, Accuracy = 1.0\n",
      "Iter #776960:  Learning rate = 0.003757:   Batch Loss = 0.074670, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0751409307122, Accuracy = 1.0\n",
      "Iter #777600:  Learning rate = 0.003757:   Batch Loss = 0.075054, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0751124173403, Accuracy = 1.0\n",
      "Iter #778240:  Learning rate = 0.003757:   Batch Loss = 0.073975, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0751033872366, Accuracy = 1.0\n",
      "Iter #778880:  Learning rate = 0.003757:   Batch Loss = 0.074542, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0751487538218, Accuracy = 1.0\n",
      "Iter #779520:  Learning rate = 0.003757:   Batch Loss = 0.074898, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0751079842448, Accuracy = 1.0\n",
      "Iter #780160:  Learning rate = 0.003757:   Batch Loss = 0.074661, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0750858336687, Accuracy = 1.0\n",
      "Iter #780800:  Learning rate = 0.003757:   Batch Loss = 0.075384, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0750321373343, Accuracy = 1.0\n",
      "Iter #781440:  Learning rate = 0.003757:   Batch Loss = 0.075700, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0750286877155, Accuracy = 1.0\n",
      "Iter #782080:  Learning rate = 0.003757:   Batch Loss = 0.074544, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.074999012053, Accuracy = 1.0\n",
      "Iter #782720:  Learning rate = 0.003757:   Batch Loss = 0.074362, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0750105306506, Accuracy = 1.0\n",
      "Iter #783360:  Learning rate = 0.003757:   Batch Loss = 0.074876, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0750572606921, Accuracy = 1.0\n",
      "Iter #784000:  Learning rate = 0.003757:   Batch Loss = 0.074558, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0750043168664, Accuracy = 1.0\n",
      "Iter #784640:  Learning rate = 0.003757:   Batch Loss = 0.074934, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0750227496028, Accuracy = 1.0\n",
      "Iter #785280:  Learning rate = 0.003757:   Batch Loss = 0.075637, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0749711543322, Accuracy = 1.0\n",
      "Iter #785920:  Learning rate = 0.003757:   Batch Loss = 0.074004, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0749327987432, Accuracy = 1.0\n",
      "Iter #786560:  Learning rate = 0.003757:   Batch Loss = 0.074591, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0749242603779, Accuracy = 1.0\n",
      "Iter #787200:  Learning rate = 0.003757:   Batch Loss = 0.074394, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0749299973249, Accuracy = 1.0\n",
      "Iter #787840:  Learning rate = 0.003757:   Batch Loss = 0.075396, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0749304294586, Accuracy = 1.0\n",
      "Iter #788480:  Learning rate = 0.003757:   Batch Loss = 0.074510, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0749208927155, Accuracy = 1.0\n",
      "Iter #789120:  Learning rate = 0.003757:   Batch Loss = 0.074696, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0749118700624, Accuracy = 1.0\n",
      "Iter #789760:  Learning rate = 0.003757:   Batch Loss = 0.074711, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0748698338866, Accuracy = 1.0\n",
      "Iter #790400:  Learning rate = 0.003757:   Batch Loss = 0.074256, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0749406069517, Accuracy = 1.0\n",
      "Iter #791040:  Learning rate = 0.003757:   Batch Loss = 0.074411, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0748615264893, Accuracy = 1.0\n",
      "Iter #791680:  Learning rate = 0.003757:   Batch Loss = 0.074426, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0748465061188, Accuracy = 1.0\n",
      "Iter #792320:  Learning rate = 0.003757:   Batch Loss = 0.075098, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0748243629932, Accuracy = 1.0\n",
      "Iter #792960:  Learning rate = 0.003757:   Batch Loss = 0.074204, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0748321041465, Accuracy = 1.0\n",
      "Iter #793600:  Learning rate = 0.003757:   Batch Loss = 0.074251, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0748511254787, Accuracy = 1.0\n",
      "Iter #794240:  Learning rate = 0.003757:   Batch Loss = 0.074493, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0748245567083, Accuracy = 1.0\n",
      "Iter #794880:  Learning rate = 0.003757:   Batch Loss = 0.074948, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0748219117522, Accuracy = 1.0\n",
      "Iter #795520:  Learning rate = 0.003757:   Batch Loss = 0.073761, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0747938230634, Accuracy = 1.0\n",
      "Iter #796160:  Learning rate = 0.003757:   Batch Loss = 0.075639, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0748415887356, Accuracy = 1.0\n",
      "Iter #796800:  Learning rate = 0.003757:   Batch Loss = 0.074545, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0747948288918, Accuracy = 1.0\n",
      "Iter #797440:  Learning rate = 0.003757:   Batch Loss = 0.074480, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0747135803103, Accuracy = 1.0\n",
      "Iter #798080:  Learning rate = 0.003757:   Batch Loss = 0.074238, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0746882334352, Accuracy = 1.0\n",
      "Iter #798720:  Learning rate = 0.003757:   Batch Loss = 0.074053, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0747081115842, Accuracy = 1.0\n",
      "Iter #799360:  Learning rate = 0.003757:   Batch Loss = 0.074089, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0746884569526, Accuracy = 1.0\n",
      "Iter #800000:  Learning rate = 0.003607:   Batch Loss = 0.074782, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0746653601527, Accuracy = 1.0\n",
      "Iter #800640:  Learning rate = 0.003607:   Batch Loss = 0.075149, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0746771991253, Accuracy = 1.0\n",
      "Iter #801280:  Learning rate = 0.003607:   Batch Loss = 0.074012, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0746239572763, Accuracy = 1.0\n",
      "Iter #801920:  Learning rate = 0.003607:   Batch Loss = 0.074860, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0746362134814, Accuracy = 1.0\n",
      "Iter #802560:  Learning rate = 0.003607:   Batch Loss = 0.074255, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0746269673109, Accuracy = 1.0\n",
      "Iter #803200:  Learning rate = 0.003607:   Batch Loss = 0.075293, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0746362656355, Accuracy = 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter #803840:  Learning rate = 0.003607:   Batch Loss = 0.073619, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0746247321367, Accuracy = 1.0\n",
      "Iter #804480:  Learning rate = 0.003607:   Batch Loss = 0.074979, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0746133476496, Accuracy = 1.0\n",
      "Iter #805120:  Learning rate = 0.003607:   Batch Loss = 0.074187, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0745729207993, Accuracy = 1.0\n",
      "Iter #805760:  Learning rate = 0.003607:   Batch Loss = 0.074351, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.074612043798, Accuracy = 1.0\n",
      "Iter #806400:  Learning rate = 0.003607:   Batch Loss = 0.075515, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0750921070576, Accuracy = 1.0\n",
      "Iter #807040:  Learning rate = 0.003607:   Batch Loss = 0.074663, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0747431889176, Accuracy = 1.0\n",
      "Iter #807680:  Learning rate = 0.003607:   Batch Loss = 0.074274, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0747697278857, Accuracy = 1.0\n",
      "Iter #808320:  Learning rate = 0.003607:   Batch Loss = 0.074402, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0747049972415, Accuracy = 1.0\n",
      "Iter #808960:  Learning rate = 0.003607:   Batch Loss = 0.075037, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0747310593724, Accuracy = 1.0\n",
      "Iter #809600:  Learning rate = 0.003607:   Batch Loss = 0.075226, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.074604049325, Accuracy = 1.0\n",
      "Iter #810240:  Learning rate = 0.003607:   Batch Loss = 0.074046, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0748699828982, Accuracy = 1.0\n",
      "Iter #810880:  Learning rate = 0.003607:   Batch Loss = 0.074015, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0746158659458, Accuracy = 1.0\n",
      "Iter #811520:  Learning rate = 0.003607:   Batch Loss = 0.074529, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0746661126614, Accuracy = 1.0\n",
      "Iter #812160:  Learning rate = 0.003607:   Batch Loss = 0.074480, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0758711099625, Accuracy = 0.999714195728\n",
      "Iter #812800:  Learning rate = 0.003607:   Batch Loss = 0.075134, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0745913684368, Accuracy = 1.0\n",
      "Iter #813440:  Learning rate = 0.003607:   Batch Loss = 0.074116, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0746044144034, Accuracy = 1.0\n",
      "Iter #814080:  Learning rate = 0.003607:   Batch Loss = 0.074433, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0745810568333, Accuracy = 1.0\n",
      "Iter #814720:  Learning rate = 0.003607:   Batch Loss = 0.074193, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.074518725276, Accuracy = 1.0\n",
      "Iter #815360:  Learning rate = 0.003607:   Batch Loss = 0.073977, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0744427517056, Accuracy = 1.0\n",
      "Iter #816000:  Learning rate = 0.003607:   Batch Loss = 0.073347, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0744328573346, Accuracy = 1.0\n",
      "Iter #816640:  Learning rate = 0.003607:   Batch Loss = 0.073895, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0744065344334, Accuracy = 1.0\n",
      "Iter #817280:  Learning rate = 0.003607:   Batch Loss = 0.073768, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0744345635176, Accuracy = 1.0\n",
      "Iter #817920:  Learning rate = 0.003607:   Batch Loss = 0.075058, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0744346231222, Accuracy = 1.0\n",
      "Iter #818560:  Learning rate = 0.003607:   Batch Loss = 0.074660, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0744351670146, Accuracy = 1.0\n",
      "Iter #819200:  Learning rate = 0.003607:   Batch Loss = 0.073752, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0743637904525, Accuracy = 1.0\n",
      "Iter #819840:  Learning rate = 0.003607:   Batch Loss = 0.074180, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0743783414364, Accuracy = 1.0\n",
      "Iter #820480:  Learning rate = 0.003607:   Batch Loss = 0.073854, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0743404179811, Accuracy = 1.0\n",
      "Iter #821120:  Learning rate = 0.003607:   Batch Loss = 0.074117, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0743946135044, Accuracy = 1.0\n",
      "Iter #821760:  Learning rate = 0.003607:   Batch Loss = 0.074730, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0743258148432, Accuracy = 1.0\n",
      "Iter #822400:  Learning rate = 0.003607:   Batch Loss = 0.074696, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0743334293365, Accuracy = 1.0\n",
      "Iter #823040:  Learning rate = 0.003607:   Batch Loss = 0.073891, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0742750614882, Accuracy = 1.0\n",
      "Iter #823680:  Learning rate = 0.003607:   Batch Loss = 0.074240, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0742637813091, Accuracy = 1.0\n",
      "Iter #824320:  Learning rate = 0.003607:   Batch Loss = 0.073854, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0742528662086, Accuracy = 1.0\n",
      "Iter #824960:  Learning rate = 0.003607:   Batch Loss = 0.074032, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0742377638817, Accuracy = 1.0\n",
      "Iter #825600:  Learning rate = 0.003607:   Batch Loss = 0.074553, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0744488537312, Accuracy = 1.0\n",
      "Iter #826240:  Learning rate = 0.003607:   Batch Loss = 0.074188, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0744107067585, Accuracy = 1.0\n",
      "Iter #826880:  Learning rate = 0.003607:   Batch Loss = 0.073784, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0743147730827, Accuracy = 1.0\n",
      "Iter #827520:  Learning rate = 0.003607:   Batch Loss = 0.073731, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0742812901735, Accuracy = 1.0\n",
      "Iter #828160:  Learning rate = 0.003607:   Batch Loss = 0.073705, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0742629170418, Accuracy = 1.0\n",
      "Iter #828800:  Learning rate = 0.003607:   Batch Loss = 0.074205, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0742470249534, Accuracy = 1.0\n",
      "Iter #829440:  Learning rate = 0.003607:   Batch Loss = 0.074493, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0742571875453, Accuracy = 1.0\n",
      "Iter #830080:  Learning rate = 0.003607:   Batch Loss = 0.073612, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0742142274976, Accuracy = 1.0\n",
      "Iter #830720:  Learning rate = 0.003607:   Batch Loss = 0.074004, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0742234811187, Accuracy = 1.0\n",
      "Iter #831360:  Learning rate = 0.003607:   Batch Loss = 0.073849, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0741648152471, Accuracy = 1.0\n",
      "Iter #832000:  Learning rate = 0.003607:   Batch Loss = 0.073857, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0742035806179, Accuracy = 1.0\n",
      "Iter #832640:  Learning rate = 0.003607:   Batch Loss = 0.073104, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0742545500398, Accuracy = 1.0\n",
      "Iter #833280:  Learning rate = 0.003607:   Batch Loss = 0.074686, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0741174519062, Accuracy = 1.0\n",
      "Iter #833920:  Learning rate = 0.003607:   Batch Loss = 0.074223, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0741181448102, Accuracy = 1.0\n",
      "Iter #834560:  Learning rate = 0.003607:   Batch Loss = 0.073686, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0741172805429, Accuracy = 1.0\n",
      "Iter #835200:  Learning rate = 0.003607:   Batch Loss = 0.073813, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0740650296211, Accuracy = 1.0\n",
      "Iter #835840:  Learning rate = 0.003607:   Batch Loss = 0.073324, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0741085186601, Accuracy = 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter #836480:  Learning rate = 0.003607:   Batch Loss = 0.074179, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0741106718779, Accuracy = 1.0\n",
      "Iter #837120:  Learning rate = 0.003607:   Batch Loss = 0.073358, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0740776360035, Accuracy = 1.0\n",
      "Iter #837760:  Learning rate = 0.003607:   Batch Loss = 0.073156, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0740168839693, Accuracy = 1.0\n",
      "Iter #838400:  Learning rate = 0.003607:   Batch Loss = 0.073572, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0740097314119, Accuracy = 1.0\n",
      "Iter #839040:  Learning rate = 0.003607:   Batch Loss = 0.073997, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0739990547299, Accuracy = 1.0\n",
      "Iter #839680:  Learning rate = 0.003607:   Batch Loss = 0.073923, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0739587694407, Accuracy = 1.0\n",
      "Iter #840320:  Learning rate = 0.003607:   Batch Loss = 0.073306, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0739671513438, Accuracy = 1.0\n",
      "Iter #840960:  Learning rate = 0.003607:   Batch Loss = 0.074094, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0739734917879, Accuracy = 1.0\n",
      "Iter #841600:  Learning rate = 0.003607:   Batch Loss = 0.073282, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0739309564233, Accuracy = 1.0\n",
      "Iter #842240:  Learning rate = 0.003607:   Batch Loss = 0.073461, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0740380436182, Accuracy = 1.0\n",
      "Iter #842880:  Learning rate = 0.003607:   Batch Loss = 0.073894, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0740134567022, Accuracy = 1.0\n",
      "Iter #843520:  Learning rate = 0.003607:   Batch Loss = 0.074010, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0739877074957, Accuracy = 1.0\n",
      "Iter #844160:  Learning rate = 0.003607:   Batch Loss = 0.074025, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0739197507501, Accuracy = 1.0\n",
      "Iter #844800:  Learning rate = 0.003607:   Batch Loss = 0.074218, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0739241689444, Accuracy = 1.0\n",
      "Iter #845440:  Learning rate = 0.003607:   Batch Loss = 0.074206, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0739249512553, Accuracy = 1.0\n",
      "Iter #846080:  Learning rate = 0.003607:   Batch Loss = 0.073692, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0739022567868, Accuracy = 1.0\n",
      "Iter #846720:  Learning rate = 0.003607:   Batch Loss = 0.073123, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0738290697336, Accuracy = 1.0\n",
      "Iter #847360:  Learning rate = 0.003607:   Batch Loss = 0.074464, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0738286599517, Accuracy = 1.0\n",
      "Iter #848000:  Learning rate = 0.003607:   Batch Loss = 0.073850, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.07385212183, Accuracy = 1.0\n",
      "Iter #848640:  Learning rate = 0.003607:   Batch Loss = 0.073839, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0739554539323, Accuracy = 1.0\n",
      "Iter #849280:  Learning rate = 0.003607:   Batch Loss = 0.074154, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0738002806902, Accuracy = 1.0\n",
      "Iter #849920:  Learning rate = 0.003607:   Batch Loss = 0.073540, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0738147497177, Accuracy = 1.0\n",
      "Iter #850560:  Learning rate = 0.003607:   Batch Loss = 0.073381, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0737426877022, Accuracy = 1.0\n",
      "Iter #851200:  Learning rate = 0.003607:   Batch Loss = 0.073614, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0738364905119, Accuracy = 1.0\n",
      "Iter #851840:  Learning rate = 0.003607:   Batch Loss = 0.073316, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.073745906353, Accuracy = 1.0\n",
      "Iter #852480:  Learning rate = 0.003607:   Batch Loss = 0.073275, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0737565979362, Accuracy = 1.0\n",
      "Iter #853120:  Learning rate = 0.003607:   Batch Loss = 0.073276, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0733770281076, Accuracy = 1.0\n",
      "Iter #853760:  Learning rate = 0.003607:   Batch Loss = 0.074943, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0732346922159, Accuracy = 1.0\n",
      "Iter #854400:  Learning rate = 0.003607:   Batch Loss = 0.074035, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.084316983819, Accuracy = 0.996856272221\n",
      "Iter #855040:  Learning rate = 0.003607:   Batch Loss = 0.077810, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.144047468901, Accuracy = 0.97227782011\n",
      "Iter #855680:  Learning rate = 0.003607:   Batch Loss = 0.376117, Accuracy = 0.875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.12446859479, Accuracy = 0.989139735699\n",
      "Iter #856320:  Learning rate = 0.003607:   Batch Loss = 0.309717, Accuracy = 0.975000023842\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.334857285023, Accuracy = 0.925693035126\n",
      "Iter #856960:  Learning rate = 0.003607:   Batch Loss = 0.128959, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.121475495398, Accuracy = 0.990854501724\n",
      "Iter #857600:  Learning rate = 0.003607:   Batch Loss = 0.128158, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.127648621798, Accuracy = 0.996856272221\n",
      "Iter #858240:  Learning rate = 0.003607:   Batch Loss = 0.105293, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.10127530247, Accuracy = 0.998856842518\n",
      "Iter #858880:  Learning rate = 0.003607:   Batch Loss = 0.088528, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0880535766482, Accuracy = 1.0\n",
      "Iter #859520:  Learning rate = 0.003607:   Batch Loss = 0.084098, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0857263579965, Accuracy = 0.999714195728\n",
      "Iter #860160:  Learning rate = 0.003607:   Batch Loss = 0.083724, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0826837047935, Accuracy = 0.999714195728\n",
      "Iter #860800:  Learning rate = 0.003607:   Batch Loss = 0.079775, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0799520164728, Accuracy = 1.0\n",
      "Iter #861440:  Learning rate = 0.003607:   Batch Loss = 0.079834, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.079301789403, Accuracy = 1.0\n",
      "Iter #862080:  Learning rate = 0.003607:   Batch Loss = 0.084297, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0788895860314, Accuracy = 1.0\n",
      "Iter #862720:  Learning rate = 0.003607:   Batch Loss = 0.078324, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.078709192574, Accuracy = 1.0\n",
      "Iter #863360:  Learning rate = 0.003607:   Batch Loss = 0.078189, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0782873630524, Accuracy = 1.0\n",
      "Iter #864000:  Learning rate = 0.003607:   Batch Loss = 0.079040, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0780281871557, Accuracy = 1.0\n",
      "Iter #864640:  Learning rate = 0.003607:   Batch Loss = 0.077579, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0776878073812, Accuracy = 1.0\n",
      "Iter #865280:  Learning rate = 0.003607:   Batch Loss = 0.076865, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0774831175804, Accuracy = 1.0\n",
      "Iter #865920:  Learning rate = 0.003607:   Batch Loss = 0.076638, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0773147568107, Accuracy = 1.0\n",
      "Iter #866560:  Learning rate = 0.003607:   Batch Loss = 0.076845, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0771682783961, Accuracy = 1.0\n",
      "Iter #867200:  Learning rate = 0.003607:   Batch Loss = 0.076916, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.077063113451, Accuracy = 1.0\n",
      "Iter #867840:  Learning rate = 0.003607:   Batch Loss = 0.076767, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0769764333963, Accuracy = 1.0\n",
      "Iter #868480:  Learning rate = 0.003607:   Batch Loss = 0.077581, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0769195109606, Accuracy = 1.0\n",
      "Iter #869120:  Learning rate = 0.003607:   Batch Loss = 0.077305, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0768423080444, Accuracy = 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter #869760:  Learning rate = 0.003607:   Batch Loss = 0.076514, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0767487362027, Accuracy = 1.0\n",
      "Iter #870400:  Learning rate = 0.003607:   Batch Loss = 0.075663, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0766901671886, Accuracy = 1.0\n",
      "Iter #871040:  Learning rate = 0.003607:   Batch Loss = 0.076559, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0766317471862, Accuracy = 1.0\n",
      "Iter #871680:  Learning rate = 0.003607:   Batch Loss = 0.076407, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0765860229731, Accuracy = 1.0\n",
      "Iter #872320:  Learning rate = 0.003607:   Batch Loss = 0.076370, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0765300989151, Accuracy = 1.0\n",
      "Iter #872960:  Learning rate = 0.003607:   Batch Loss = 0.076361, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0765328854322, Accuracy = 1.0\n",
      "Iter #873600:  Learning rate = 0.003607:   Batch Loss = 0.075994, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0764623209834, Accuracy = 1.0\n",
      "Iter #874240:  Learning rate = 0.003607:   Batch Loss = 0.075404, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0763795599341, Accuracy = 1.0\n",
      "Iter #874880:  Learning rate = 0.003607:   Batch Loss = 0.077226, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.076352275908, Accuracy = 1.0\n",
      "Iter #875520:  Learning rate = 0.003607:   Batch Loss = 0.076358, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0762965381145, Accuracy = 1.0\n",
      "Iter #876160:  Learning rate = 0.003607:   Batch Loss = 0.075911, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0762390494347, Accuracy = 1.0\n",
      "Iter #876800:  Learning rate = 0.003607:   Batch Loss = 0.076148, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0762228518724, Accuracy = 1.0\n",
      "Iter #877440:  Learning rate = 0.003607:   Batch Loss = 0.075863, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0761960148811, Accuracy = 1.0\n",
      "Iter #878080:  Learning rate = 0.003607:   Batch Loss = 0.075608, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0761916264892, Accuracy = 1.0\n",
      "Iter #878720:  Learning rate = 0.003607:   Batch Loss = 0.076036, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0761750563979, Accuracy = 1.0\n",
      "Iter #879360:  Learning rate = 0.003607:   Batch Loss = 0.075317, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0761356204748, Accuracy = 1.0\n",
      "Iter #880000:  Learning rate = 0.003607:   Batch Loss = 0.076567, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0760799795389, Accuracy = 1.0\n",
      "Iter #880640:  Learning rate = 0.003607:   Batch Loss = 0.076005, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0760371983051, Accuracy = 1.0\n",
      "Iter #881280:  Learning rate = 0.003607:   Batch Loss = 0.076347, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0760301724076, Accuracy = 1.0\n",
      "Iter #881920:  Learning rate = 0.003607:   Batch Loss = 0.075199, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0759333446622, Accuracy = 1.0\n",
      "Iter #882560:  Learning rate = 0.003607:   Batch Loss = 0.075816, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0758535191417, Accuracy = 1.0\n",
      "Iter #883200:  Learning rate = 0.003607:   Batch Loss = 0.075768, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0758103057742, Accuracy = 1.0\n",
      "Iter #883840:  Learning rate = 0.003607:   Batch Loss = 0.074940, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0756588429213, Accuracy = 1.0\n",
      "Iter #884480:  Learning rate = 0.003607:   Batch Loss = 0.074964, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.075597435236, Accuracy = 1.0\n",
      "Iter #885120:  Learning rate = 0.003607:   Batch Loss = 0.075054, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0755816102028, Accuracy = 1.0\n",
      "Iter #885760:  Learning rate = 0.003607:   Batch Loss = 0.075271, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0755847021937, Accuracy = 1.0\n",
      "Iter #886400:  Learning rate = 0.003607:   Batch Loss = 0.075287, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0755053535104, Accuracy = 1.0\n",
      "Iter #887040:  Learning rate = 0.003607:   Batch Loss = 0.075721, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0754584372044, Accuracy = 1.0\n",
      "Iter #887680:  Learning rate = 0.003607:   Batch Loss = 0.075521, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0754314959049, Accuracy = 1.0\n",
      "Iter #888320:  Learning rate = 0.003607:   Batch Loss = 0.075281, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0754048600793, Accuracy = 1.0\n",
      "Iter #888960:  Learning rate = 0.003607:   Batch Loss = 0.074786, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0753654390574, Accuracy = 1.0\n",
      "Iter #889600:  Learning rate = 0.003607:   Batch Loss = 0.074487, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0753449723125, Accuracy = 1.0\n",
      "Iter #890240:  Learning rate = 0.003607:   Batch Loss = 0.074754, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0753274932504, Accuracy = 1.0\n",
      "Iter #890880:  Learning rate = 0.003607:   Batch Loss = 0.075357, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0752822011709, Accuracy = 1.0\n",
      "Iter #891520:  Learning rate = 0.003607:   Batch Loss = 0.074353, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0752664729953, Accuracy = 1.0\n",
      "Iter #892160:  Learning rate = 0.003607:   Batch Loss = 0.075013, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0752752199769, Accuracy = 1.0\n",
      "Iter #892800:  Learning rate = 0.003607:   Batch Loss = 0.075226, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0753058344126, Accuracy = 1.0\n",
      "Iter #893440:  Learning rate = 0.003607:   Batch Loss = 0.074563, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0752424672246, Accuracy = 1.0\n",
      "Iter #894080:  Learning rate = 0.003607:   Batch Loss = 0.074998, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0751802623272, Accuracy = 1.0\n",
      "Iter #894720:  Learning rate = 0.003607:   Batch Loss = 0.075008, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0751643925905, Accuracy = 1.0\n",
      "Iter #895360:  Learning rate = 0.003607:   Batch Loss = 0.075250, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0751911327243, Accuracy = 1.0\n",
      "Iter #896000:  Learning rate = 0.003607:   Batch Loss = 0.074895, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0751500725746, Accuracy = 1.0\n",
      "Iter #896640:  Learning rate = 0.003607:   Batch Loss = 0.074313, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0751356557012, Accuracy = 1.0\n",
      "Iter #897280:  Learning rate = 0.003607:   Batch Loss = 0.075019, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0751106292009, Accuracy = 1.0\n",
      "Iter #897920:  Learning rate = 0.003607:   Batch Loss = 0.074778, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0750954896212, Accuracy = 1.0\n",
      "Iter #898560:  Learning rate = 0.003607:   Batch Loss = 0.075467, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0750741884112, Accuracy = 1.0\n",
      "Iter #899200:  Learning rate = 0.003607:   Batch Loss = 0.075152, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0750571712852, Accuracy = 1.0\n",
      "Iter #899840:  Learning rate = 0.003607:   Batch Loss = 0.074870, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0750843435526, Accuracy = 1.0\n",
      "Iter #900480:  Learning rate = 0.003463:   Batch Loss = 0.074872, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0750463455915, Accuracy = 1.0\n",
      "Iter #901120:  Learning rate = 0.003463:   Batch Loss = 0.073996, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0750089287758, Accuracy = 1.0\n",
      "Iter #901760:  Learning rate = 0.003463:   Batch Loss = 0.075518, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0749892443419, Accuracy = 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter #902400:  Learning rate = 0.003463:   Batch Loss = 0.075442, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0749654099345, Accuracy = 1.0\n",
      "Iter #903040:  Learning rate = 0.003463:   Batch Loss = 0.074573, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0748157799244, Accuracy = 1.0\n",
      "Iter #903680:  Learning rate = 0.003463:   Batch Loss = 0.073852, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0745618343353, Accuracy = 1.0\n",
      "Iter #904320:  Learning rate = 0.003463:   Batch Loss = 0.074891, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0744043290615, Accuracy = 1.0\n",
      "Iter #904960:  Learning rate = 0.003463:   Batch Loss = 0.073544, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0743753686547, Accuracy = 1.0\n",
      "Iter #905600:  Learning rate = 0.003463:   Batch Loss = 0.073830, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0743593424559, Accuracy = 1.0\n",
      "Iter #906240:  Learning rate = 0.003463:   Batch Loss = 0.074213, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0743215307593, Accuracy = 1.0\n",
      "Iter #906880:  Learning rate = 0.003463:   Batch Loss = 0.074196, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0743169486523, Accuracy = 1.0\n",
      "Iter #907520:  Learning rate = 0.003463:   Batch Loss = 0.073794, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0742858201265, Accuracy = 1.0\n",
      "Iter #908160:  Learning rate = 0.003463:   Batch Loss = 0.074310, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0742956250906, Accuracy = 1.0\n",
      "Iter #908800:  Learning rate = 0.003463:   Batch Loss = 0.073859, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0742805376649, Accuracy = 1.0\n",
      "Iter #909440:  Learning rate = 0.003463:   Batch Loss = 0.073657, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0742974281311, Accuracy = 1.0\n",
      "Iter #910080:  Learning rate = 0.003463:   Batch Loss = 0.074046, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0742892995477, Accuracy = 1.0\n",
      "Iter #910720:  Learning rate = 0.003463:   Batch Loss = 0.074010, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0742195919156, Accuracy = 1.0\n",
      "Iter #911360:  Learning rate = 0.003463:   Batch Loss = 0.073932, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0742142051458, Accuracy = 1.0\n",
      "Iter #912000:  Learning rate = 0.003463:   Batch Loss = 0.074184, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0742107704282, Accuracy = 1.0\n",
      "Iter #912640:  Learning rate = 0.003463:   Batch Loss = 0.074385, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0742228999734, Accuracy = 1.0\n",
      "Iter #913280:  Learning rate = 0.003463:   Batch Loss = 0.074151, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0742311924696, Accuracy = 1.0\n",
      "Iter #913920:  Learning rate = 0.003463:   Batch Loss = 0.074954, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0741977393627, Accuracy = 1.0\n",
      "Iter #914560:  Learning rate = 0.003463:   Batch Loss = 0.073504, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0741710886359, Accuracy = 1.0\n",
      "Iter #915200:  Learning rate = 0.003463:   Batch Loss = 0.073402, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0741910114884, Accuracy = 1.0\n",
      "Iter #915840:  Learning rate = 0.003463:   Batch Loss = 0.073868, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0741833224893, Accuracy = 1.0\n",
      "Iter #916480:  Learning rate = 0.003463:   Batch Loss = 0.073611, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0741616338491, Accuracy = 1.0\n",
      "Iter #917120:  Learning rate = 0.003463:   Batch Loss = 0.074573, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.074139803648, Accuracy = 1.0\n",
      "Iter #917760:  Learning rate = 0.003463:   Batch Loss = 0.073756, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0741246640682, Accuracy = 1.0\n",
      "Iter #918400:  Learning rate = 0.003463:   Batch Loss = 0.074915, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0741029083729, Accuracy = 1.0\n",
      "Iter #919040:  Learning rate = 0.003463:   Batch Loss = 0.074133, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.074084661901, Accuracy = 1.0\n",
      "Iter #919680:  Learning rate = 0.003463:   Batch Loss = 0.073472, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0740707665682, Accuracy = 1.0\n",
      "Iter #920320:  Learning rate = 0.003463:   Batch Loss = 0.073352, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0740707218647, Accuracy = 1.0\n",
      "Iter #920960:  Learning rate = 0.003463:   Batch Loss = 0.073794, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0740963593125, Accuracy = 1.0\n",
      "Iter #921600:  Learning rate = 0.003463:   Batch Loss = 0.073523, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0740809440613, Accuracy = 1.0\n",
      "Iter #922240:  Learning rate = 0.003463:   Batch Loss = 0.073805, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0740645378828, Accuracy = 1.0\n",
      "Iter #922880:  Learning rate = 0.003463:   Batch Loss = 0.073897, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0740284696221, Accuracy = 1.0\n",
      "Iter #923520:  Learning rate = 0.003463:   Batch Loss = 0.073791, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0740439072251, Accuracy = 1.0\n",
      "Iter #924160:  Learning rate = 0.003463:   Batch Loss = 0.073759, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0739943087101, Accuracy = 1.0\n",
      "Iter #924800:  Learning rate = 0.003463:   Batch Loss = 0.073250, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0739956796169, Accuracy = 1.0\n",
      "Iter #925440:  Learning rate = 0.003463:   Batch Loss = 0.073334, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0739993825555, Accuracy = 1.0\n",
      "Iter #926080:  Learning rate = 0.003463:   Batch Loss = 0.074136, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.074028737843, Accuracy = 1.0\n",
      "Iter #926720:  Learning rate = 0.003463:   Batch Loss = 0.073545, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0740145146847, Accuracy = 1.0\n",
      "Iter #927360:  Learning rate = 0.003463:   Batch Loss = 0.074041, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0739814043045, Accuracy = 1.0\n",
      "Iter #928000:  Learning rate = 0.003463:   Batch Loss = 0.073865, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.073957234621, Accuracy = 1.0\n",
      "Iter #928640:  Learning rate = 0.003463:   Batch Loss = 0.073435, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0739661827683, Accuracy = 1.0\n",
      "Iter #929280:  Learning rate = 0.003463:   Batch Loss = 0.073450, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0740247294307, Accuracy = 1.0\n",
      "Iter #929920:  Learning rate = 0.003463:   Batch Loss = 0.074451, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0739439278841, Accuracy = 1.0\n",
      "Iter #930560:  Learning rate = 0.003463:   Batch Loss = 0.073966, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.07396915555, Accuracy = 1.0\n",
      "Iter #931200:  Learning rate = 0.003463:   Batch Loss = 0.074658, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0739694461226, Accuracy = 1.0\n",
      "Iter #931840:  Learning rate = 0.003463:   Batch Loss = 0.074352, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0739651620388, Accuracy = 1.0\n",
      "Iter #932480:  Learning rate = 0.003463:   Batch Loss = 0.073139, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0739187896252, Accuracy = 1.0\n",
      "Iter #933120:  Learning rate = 0.003463:   Batch Loss = 0.073524, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0739009678364, Accuracy = 1.0\n",
      "Iter #933760:  Learning rate = 0.003463:   Batch Loss = 0.073671, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0738817229867, Accuracy = 1.0\n",
      "Iter #934400:  Learning rate = 0.003463:   Batch Loss = 0.074057, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0739375948906, Accuracy = 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter #935040:  Learning rate = 0.003463:   Batch Loss = 0.073785, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0739456117153, Accuracy = 1.0\n",
      "Iter #935680:  Learning rate = 0.003463:   Batch Loss = 0.073597, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0738452449441, Accuracy = 1.0\n",
      "Iter #936320:  Learning rate = 0.003463:   Batch Loss = 0.073808, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0738568380475, Accuracy = 1.0\n",
      "Iter #936960:  Learning rate = 0.003463:   Batch Loss = 0.073308, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0738533139229, Accuracy = 1.0\n",
      "Iter #937600:  Learning rate = 0.003463:   Batch Loss = 0.073555, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0738397017121, Accuracy = 1.0\n",
      "Iter #938240:  Learning rate = 0.003463:   Batch Loss = 0.073570, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0738369673491, Accuracy = 1.0\n",
      "Iter #938880:  Learning rate = 0.003463:   Batch Loss = 0.073670, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0738707110286, Accuracy = 1.0\n",
      "Iter #939520:  Learning rate = 0.003463:   Batch Loss = 0.074283, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0738696530461, Accuracy = 1.0\n",
      "Iter #940160:  Learning rate = 0.003463:   Batch Loss = 0.073805, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0738502889872, Accuracy = 1.0\n",
      "Iter #940800:  Learning rate = 0.003463:   Batch Loss = 0.073805, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0738065987825, Accuracy = 1.0\n",
      "Iter #941440:  Learning rate = 0.003463:   Batch Loss = 0.074069, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0737880468369, Accuracy = 1.0\n",
      "Iter #942080:  Learning rate = 0.003463:   Batch Loss = 0.073054, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0737898796797, Accuracy = 1.0\n",
      "Iter #942720:  Learning rate = 0.003463:   Batch Loss = 0.073659, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0737806111574, Accuracy = 1.0\n",
      "Iter #943360:  Learning rate = 0.003463:   Batch Loss = 0.073393, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0737789571285, Accuracy = 1.0\n",
      "Iter #944000:  Learning rate = 0.003463:   Batch Loss = 0.073440, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.07381080091, Accuracy = 1.0\n",
      "Iter #944640:  Learning rate = 0.003463:   Batch Loss = 0.074051, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.073792733252, Accuracy = 1.0\n",
      "Iter #945280:  Learning rate = 0.003463:   Batch Loss = 0.074140, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0737392902374, Accuracy = 1.0\n",
      "Iter #945920:  Learning rate = 0.003463:   Batch Loss = 0.073511, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0737260580063, Accuracy = 1.0\n",
      "Iter #946560:  Learning rate = 0.003463:   Batch Loss = 0.073428, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0737099796534, Accuracy = 1.0\n",
      "Iter #947200:  Learning rate = 0.003463:   Batch Loss = 0.072347, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0737701058388, Accuracy = 1.0\n",
      "Iter #947840:  Learning rate = 0.003463:   Batch Loss = 0.073823, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0737497285008, Accuracy = 1.0\n",
      "Iter #948480:  Learning rate = 0.003463:   Batch Loss = 0.073789, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0737199932337, Accuracy = 1.0\n",
      "Iter #949120:  Learning rate = 0.003463:   Batch Loss = 0.073410, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0737607479095, Accuracy = 1.0\n",
      "Iter #949760:  Learning rate = 0.003463:   Batch Loss = 0.073985, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0737220793962, Accuracy = 1.0\n",
      "Iter #950400:  Learning rate = 0.003463:   Batch Loss = 0.073055, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0737034007907, Accuracy = 1.0\n",
      "Iter #951040:  Learning rate = 0.003463:   Batch Loss = 0.073979, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0736504942179, Accuracy = 1.0\n",
      "Iter #951680:  Learning rate = 0.003463:   Batch Loss = 0.073164, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0736500993371, Accuracy = 1.0\n",
      "Iter #952320:  Learning rate = 0.003463:   Batch Loss = 0.073662, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0736548155546, Accuracy = 1.0\n",
      "Iter #952960:  Learning rate = 0.003463:   Batch Loss = 0.073823, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0736422911286, Accuracy = 1.0\n",
      "Iter #953600:  Learning rate = 0.003463:   Batch Loss = 0.073378, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0736472010612, Accuracy = 1.0\n",
      "Iter #954240:  Learning rate = 0.003463:   Batch Loss = 0.073197, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0736326351762, Accuracy = 1.0\n",
      "Iter #954880:  Learning rate = 0.003463:   Batch Loss = 0.073447, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0736461952329, Accuracy = 1.0\n",
      "Iter #955520:  Learning rate = 0.003463:   Batch Loss = 0.073429, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0736196488142, Accuracy = 1.0\n",
      "Iter #956160:  Learning rate = 0.003463:   Batch Loss = 0.073271, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0736345797777, Accuracy = 1.0\n",
      "Iter #956800:  Learning rate = 0.003463:   Batch Loss = 0.073361, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0736714079976, Accuracy = 1.0\n",
      "Iter #957440:  Learning rate = 0.003463:   Batch Loss = 0.073577, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0736257508397, Accuracy = 1.0\n",
      "Iter #958080:  Learning rate = 0.003463:   Batch Loss = 0.073216, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0735817849636, Accuracy = 1.0\n",
      "Iter #958720:  Learning rate = 0.003463:   Batch Loss = 0.074113, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0735787302256, Accuracy = 1.0\n",
      "Iter #959360:  Learning rate = 0.003463:   Batch Loss = 0.073108, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0735566914082, Accuracy = 1.0\n",
      "Iter #960000:  Learning rate = 0.003463:   Batch Loss = 0.074204, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0735748186707, Accuracy = 1.0\n",
      "Iter #960640:  Learning rate = 0.003463:   Batch Loss = 0.073198, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0735400617123, Accuracy = 1.0\n",
      "Iter #961280:  Learning rate = 0.003463:   Batch Loss = 0.073791, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0735128074884, Accuracy = 1.0\n",
      "Iter #961920:  Learning rate = 0.003463:   Batch Loss = 0.073672, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0735309869051, Accuracy = 1.0\n",
      "Iter #962560:  Learning rate = 0.003463:   Batch Loss = 0.073340, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.073551222682, Accuracy = 1.0\n",
      "Iter #963200:  Learning rate = 0.003463:   Batch Loss = 0.073577, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0735607668757, Accuracy = 1.0\n",
      "Iter #963840:  Learning rate = 0.003463:   Batch Loss = 0.072857, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0735326856375, Accuracy = 1.0\n",
      "Iter #964480:  Learning rate = 0.003463:   Batch Loss = 0.073354, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0735456347466, Accuracy = 1.0\n",
      "Iter #965120:  Learning rate = 0.003463:   Batch Loss = 0.072854, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0735577568412, Accuracy = 1.0\n",
      "Iter #965760:  Learning rate = 0.003463:   Batch Loss = 0.073172, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0735260248184, Accuracy = 1.0\n",
      "Iter #966400:  Learning rate = 0.003463:   Batch Loss = 0.072995, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0735231563449, Accuracy = 1.0\n",
      "Iter #967040:  Learning rate = 0.003463:   Batch Loss = 0.073831, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.073534168303, Accuracy = 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter #967680:  Learning rate = 0.003463:   Batch Loss = 0.073412, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.073570638895, Accuracy = 1.0\n",
      "Iter #968320:  Learning rate = 0.003463:   Batch Loss = 0.073181, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0735296830535, Accuracy = 1.0\n",
      "Iter #968960:  Learning rate = 0.003463:   Batch Loss = 0.072824, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0735131874681, Accuracy = 1.0\n",
      "Iter #969600:  Learning rate = 0.003463:   Batch Loss = 0.072787, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0734928995371, Accuracy = 1.0\n",
      "Iter #970240:  Learning rate = 0.003463:   Batch Loss = 0.072961, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0734819099307, Accuracy = 1.0\n",
      "Iter #970880:  Learning rate = 0.003463:   Batch Loss = 0.072682, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0734499916434, Accuracy = 1.0\n",
      "Iter #971520:  Learning rate = 0.003463:   Batch Loss = 0.072599, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0734567120671, Accuracy = 1.0\n",
      "Iter #972160:  Learning rate = 0.003463:   Batch Loss = 0.072507, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0734841153026, Accuracy = 1.0\n",
      "Iter #972800:  Learning rate = 0.003463:   Batch Loss = 0.073005, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.073438577354, Accuracy = 1.0\n",
      "Iter #973440:  Learning rate = 0.003463:   Batch Loss = 0.073522, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0734432861209, Accuracy = 1.0\n",
      "Iter #974080:  Learning rate = 0.003463:   Batch Loss = 0.072597, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0734657198191, Accuracy = 1.0\n",
      "Iter #974720:  Learning rate = 0.003463:   Batch Loss = 0.073158, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0734920799732, Accuracy = 1.0\n",
      "Iter #975360:  Learning rate = 0.003463:   Batch Loss = 0.072750, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0734303295612, Accuracy = 1.0\n",
      "Iter #976000:  Learning rate = 0.003463:   Batch Loss = 0.072573, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0734203830361, Accuracy = 1.0\n",
      "Iter #976640:  Learning rate = 0.003463:   Batch Loss = 0.073339, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0734159871936, Accuracy = 1.0\n",
      "Iter #977280:  Learning rate = 0.003463:   Batch Loss = 0.073617, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0734181106091, Accuracy = 1.0\n",
      "Iter #977920:  Learning rate = 0.003463:   Batch Loss = 0.073401, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0734259784222, Accuracy = 1.0\n",
      "Iter #978560:  Learning rate = 0.003463:   Batch Loss = 0.073557, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0734137371182, Accuracy = 1.0\n",
      "Iter #979200:  Learning rate = 0.003463:   Batch Loss = 0.073045, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0733805447817, Accuracy = 1.0\n",
      "Iter #979840:  Learning rate = 0.003463:   Batch Loss = 0.073309, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0733665078878, Accuracy = 1.0\n",
      "Iter #980480:  Learning rate = 0.003463:   Batch Loss = 0.073059, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0734101384878, Accuracy = 1.0\n",
      "Iter #981120:  Learning rate = 0.003463:   Batch Loss = 0.073357, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0733854100108, Accuracy = 1.0\n",
      "Iter #981760:  Learning rate = 0.003463:   Batch Loss = 0.073785, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0733731463552, Accuracy = 1.0\n",
      "Iter #982400:  Learning rate = 0.003463:   Batch Loss = 0.073528, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0733319893479, Accuracy = 1.0\n",
      "Iter #983040:  Learning rate = 0.003463:   Batch Loss = 0.072816, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0733746215701, Accuracy = 1.0\n",
      "Iter #983680:  Learning rate = 0.003463:   Batch Loss = 0.072638, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0733436346054, Accuracy = 1.0\n",
      "Iter #984320:  Learning rate = 0.003463:   Batch Loss = 0.073013, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0733756050467, Accuracy = 1.0\n",
      "Iter #984960:  Learning rate = 0.003463:   Batch Loss = 0.073185, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0734196901321, Accuracy = 1.0\n",
      "Iter #985600:  Learning rate = 0.003463:   Batch Loss = 0.073034, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.07363691926, Accuracy = 1.0\n",
      "Iter #986240:  Learning rate = 0.003463:   Batch Loss = 0.073622, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0733872801065, Accuracy = 1.0\n",
      "Iter #986880:  Learning rate = 0.003463:   Batch Loss = 0.073099, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.073344424367, Accuracy = 1.0\n",
      "Iter #987520:  Learning rate = 0.003463:   Batch Loss = 0.073382, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0733765214682, Accuracy = 1.0\n",
      "Iter #988160:  Learning rate = 0.003463:   Batch Loss = 0.072896, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0732964873314, Accuracy = 1.0\n",
      "Iter #988800:  Learning rate = 0.003463:   Batch Loss = 0.073067, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0733355283737, Accuracy = 1.0\n",
      "Iter #989440:  Learning rate = 0.003463:   Batch Loss = 0.072819, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.073269456625, Accuracy = 1.0\n",
      "Iter #990080:  Learning rate = 0.003463:   Batch Loss = 0.072836, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0732847303152, Accuracy = 1.0\n",
      "Iter #990720:  Learning rate = 0.003463:   Batch Loss = 0.072775, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0733261853456, Accuracy = 1.0\n",
      "Iter #991360:  Learning rate = 0.003463:   Batch Loss = 0.073005, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.073337815702, Accuracy = 1.0\n",
      "Iter #992000:  Learning rate = 0.003463:   Batch Loss = 0.072463, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.073301948607, Accuracy = 1.0\n",
      "Iter #992640:  Learning rate = 0.003463:   Batch Loss = 0.072424, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0733057260513, Accuracy = 1.0\n",
      "Iter #993280:  Learning rate = 0.003463:   Batch Loss = 0.073460, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0732960104942, Accuracy = 1.0\n",
      "Iter #993920:  Learning rate = 0.003463:   Batch Loss = 0.072996, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0733289048076, Accuracy = 1.0\n",
      "Iter #994560:  Learning rate = 0.003463:   Batch Loss = 0.074306, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0732476487756, Accuracy = 1.0\n",
      "Iter #995200:  Learning rate = 0.003463:   Batch Loss = 0.073639, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.073318593204, Accuracy = 1.0\n",
      "Iter #995840:  Learning rate = 0.003463:   Batch Loss = 0.073076, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0746238529682, Accuracy = 0.999714195728\n",
      "Iter #996480:  Learning rate = 0.003463:   Batch Loss = 0.072601, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0741725414991, Accuracy = 1.0\n",
      "Iter #997120:  Learning rate = 0.003463:   Batch Loss = 0.072956, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0737005695701, Accuracy = 1.0\n",
      "Iter #997760:  Learning rate = 0.003463:   Batch Loss = 0.072944, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0735862180591, Accuracy = 1.0\n",
      "Iter #998400:  Learning rate = 0.003463:   Batch Loss = 0.073992, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0735157132149, Accuracy = 1.0\n",
      "Iter #999040:  Learning rate = 0.003463:   Batch Loss = 0.073121, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0734369754791, Accuracy = 1.0\n",
      "Iter #999680:  Learning rate = 0.003463:   Batch Loss = 0.073861, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0734310820699, Accuracy = 1.0\n",
      "Iter #1000320:  Learning rate = 0.003324:   Batch Loss = 0.073415, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0732855573297, Accuracy = 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter #1000960:  Learning rate = 0.003324:   Batch Loss = 0.073998, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0732459574938, Accuracy = 1.0\n",
      "Iter #1001600:  Learning rate = 0.003324:   Batch Loss = 0.072991, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0733257681131, Accuracy = 1.0\n",
      "Iter #1002240:  Learning rate = 0.003324:   Batch Loss = 0.073412, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0732117444277, Accuracy = 1.0\n",
      "Iter #1002880:  Learning rate = 0.003324:   Batch Loss = 0.072921, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0732045248151, Accuracy = 1.0\n",
      "Iter #1003520:  Learning rate = 0.003324:   Batch Loss = 0.073057, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0732019841671, Accuracy = 1.0\n",
      "Iter #1004160:  Learning rate = 0.003324:   Batch Loss = 0.072496, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0731764063239, Accuracy = 1.0\n",
      "Iter #1004800:  Learning rate = 0.003324:   Batch Loss = 0.072711, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0732205063105, Accuracy = 1.0\n",
      "Iter #1005440:  Learning rate = 0.003324:   Batch Loss = 0.073233, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.0732787176967, Accuracy = 1.0\n",
      "Optimization Finished!\n",
      "FINAL RESULT: Batch Loss = 0.0732739642262, Accuracy = 1.0\n",
      "TOTAL TIME:  205.262686968\n"
     ]
    }
   ],
   "source": [
    "test_losses = []\n",
    "test_accuracies = []\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "sess = tf.InteractiveSession(config=tf.ConfigProto(log_device_placement=True))\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "# Perform Training steps with \"batch_size\" amount of data at each loop. \n",
    "# Elements of each batch are chosen randomly, without replacement, from X_train, \n",
    "# restarting when remaining datapoints < batch_size\n",
    "step = 1\n",
    "time_start = time.time()\n",
    "unsampled_indices = list(range(0,len(X_train)))\n",
    "\n",
    "while step * batch_size <= training_iters:\n",
    "    #print (sess.run(learning_rate)) #decaying learning rate\n",
    "    #print (sess.run(global_step)) # global number of iterations\n",
    "    if len(unsampled_indices) < batch_size:\n",
    "        unsampled_indices = list(range(0,len(X_train)))\n",
    "    batch_xs, raw_labels, unsampled_indicies = extract_batch_size(X_train, y_train, unsampled_indices, batch_size)\n",
    "    batch_ys = one_hot(raw_labels)\n",
    "#     batch_ys=tf.expand_dims(batch_ys,0)\n",
    "    # check that encoded output is same length as num_classes, if not, pad it \n",
    "    if len(batch_ys[0]) < n_classes:\n",
    "        temp_ys = np.zeros((batch_size, n_classes))\n",
    "        temp_ys[:batch_ys.shape[0],:batch_ys.shape[1]] = batch_ys\n",
    "        batch_ys = temp_ys\n",
    "       \n",
    "\n",
    "    # Fit training using batch data\n",
    "    _, loss, acc = sess.run(\n",
    "        [optimizer, cost, accuracy],\n",
    "        feed_dict={\n",
    "            x: batch_xs, \n",
    "            y: batch_ys\n",
    "        }\n",
    "    )\n",
    "    train_losses.append(loss)\n",
    "    train_accuracies.append(acc)\n",
    "    \n",
    "    # Evaluate network only at some steps for faster training: \n",
    "    if (step*batch_size % display_iter == 0) or (step == 1) or (step * batch_size > training_iters):\n",
    "        \n",
    "        # To not spam console, show training accuracy/loss in this \"if\"\n",
    "        print(\"Iter #\" + str(step*batch_size) + \\\n",
    "              \":  Learning rate = \" + \"{:.6f}\".format(sess.run(learning_rate)) + \\\n",
    "              \":   Batch Loss = \" + \"{:.6f}\".format(loss) + \\\n",
    "              \", Accuracy = {}\".format(acc))\n",
    "        \n",
    "        # Evaluation on the test set (no learning made here - just evaluation for diagnosis)\n",
    "        loss, acc = sess.run(\n",
    "            [cost, accuracy], \n",
    "            feed_dict={\n",
    "                x: X_test,\n",
    "                y: one_hot(y_test)\n",
    "            }\n",
    "        )\n",
    "        test_losses.append(loss)\n",
    "        test_accuracies.append(acc)\n",
    "        print(\"PERFORMANCE ON TEST SET:             \" + \\\n",
    "              \"Batch Loss = {}\".format(loss) + \\\n",
    "              \", Accuracy = {}\".format(acc))\n",
    "\n",
    "    step += 1\n",
    "\n",
    "print(\"Optimization Finished!\")\n",
    "\n",
    "# Accuracy for test data\n",
    "\n",
    "one_hot_predictions, accuracy, final_loss = sess.run(\n",
    "    [pred, accuracy, cost],\n",
    "    feed_dict={\n",
    "        x: X_test,\n",
    "        y: one_hot(y_test)\n",
    "    }\n",
    ")\n",
    "\n",
    "test_losses.append(final_loss)\n",
    "test_accuracies.append(accuracy)\n",
    "\n",
    "print(\"FINAL RESULT: \" + \\\n",
    "      \"Batch Loss = {}\".format(final_loss) + \\\n",
    "      \", Accuracy = {}\".format(accuracy))\n",
    "time_stop = time.time()\n",
    "print(\"TOTAL TIME:  {}\".format(time_stop - time_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1573\n",
      "12571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pirl/anaconda3/lib/python2.7/site-packages/matplotlib/font_manager.py:1331: UserWarning: findfont: Font family [u'Bitstream Vera Sans'] not found. Falling back to DejaVu Sans\n",
      "  (prop.get_family(), self.defaultFamily[fontext]))\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x864 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 100.0%\n",
      "\n",
      "Precision: 100.0%\n",
      "Recall: 100.0%\n",
      "f1_score: 100.0%\n",
      "\n",
      "Confusion Matrix:\n",
      "Created using test set of 3499 datapoints, normalised to % of each class in the test dataset\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x864 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# (Inline plots: )\n",
    "%matplotlib inline\n",
    "\n",
    "font = {\n",
    "    'family' : 'Bitstream Vera Sans',\n",
    "    'weight' : 'bold',\n",
    "    'size'   : 18\n",
    "}\n",
    "matplotlib.rc('font', **font)\n",
    "\n",
    "width = 12\n",
    "height = 12\n",
    "plt.figure(figsize=(width, height))\n",
    "\n",
    "indep_train_axis = np.array(range(batch_size, (len(train_losses)+1)*batch_size, batch_size))\n",
    "#plt.plot(indep_train_axis, np.array(train_losses),     \"b--\", label=\"Train losses\")\n",
    "plt.plot(indep_train_axis, np.array(train_accuracies), \"g--\", label=\"Train accuracies\")\n",
    "\n",
    "indep_test_axis = np.append(\n",
    "    np.array(range(batch_size, len(test_losses)*display_iter, display_iter)[:-1]),\n",
    "    [training_iters]\n",
    ")\n",
    "#plt.plot(indep_test_axis, np.array(test_losses), \"b-\", linewidth=2.0, label=\"Test losses\")\n",
    "plt.plot(indep_test_axis, np.array(test_accuracies), \"b-\", linewidth=2.0, label=\"Test accuracies\")\n",
    "print(len(test_accuracies))\n",
    "print(len(train_accuracies))\n",
    "\n",
    "plt.title(\"Training session's Accuracy over Iterations\")\n",
    "plt.legend(loc='lower right', shadow=True)\n",
    "plt.ylabel('Training Accuracy')\n",
    "plt.xlabel('Training Iteration')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Results\n",
    "\n",
    "predictions = one_hot_predictions.argmax(1)\n",
    "\n",
    "print(\"Testing Accuracy: {}%\".format(100*accuracy))\n",
    "\n",
    "print(\"\")\n",
    "print(\"Precision: {}%\".format(100*metrics.precision_score(y_test, predictions, average=\"weighted\")))\n",
    "print(\"Recall: {}%\".format(100*metrics.recall_score(y_test, predictions, average=\"weighted\")))\n",
    "print(\"f1_score: {}%\".format(100*metrics.f1_score(y_test, predictions, average=\"weighted\")))\n",
    "\n",
    "print(\"\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(\"Created using test set of {} datapoints, normalised to % of each class in the test dataset\".format(len(y_test)))\n",
    "confusion_matrix = metrics.confusion_matrix(y_test, predictions)\n",
    "\n",
    "\n",
    "#print(confusion_matrix)\n",
    "normalised_confusion_matrix = np.array(confusion_matrix, dtype=np.float32)/np.sum(confusion_matrix)*100\n",
    "\n",
    "\n",
    "# Plot Results: \n",
    "width = 12\n",
    "height = 12\n",
    "plt.figure(figsize=(width, height))\n",
    "plt.imshow(\n",
    "    normalised_confusion_matrix, \n",
    "    interpolation='nearest', \n",
    "    cmap=plt.cm.Blues\n",
    ")\n",
    "plt.title(\"Confusion matrix \\n(normalised to % of total test data)\")\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(n_classes)\n",
    "plt.xticks(tick_marks, LABELS, rotation=90)\n",
    "plt.yticks(tick_marks, LABELS)\n",
    "plt.tight_layout()\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.13546728, 0.49099743, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.41811946, 0.67219204, 0.67219204, 0.67219204, 0.4721349, 0.591026, 0.67219204, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.6684767, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.2869391, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.38468134, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.38468134, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.28722492, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.2043441, 0.2869391, 0.2869391, 0.2869391, 0.2043441, 0.2043441, 0.2869391, 0.2869391, 0.2043441, 0.2869391, 0.2869391, 0.2869391, 0.2043441, 0.2043441, 0.2043441, 0.2869391, 0.2869391, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2869391, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2869391, 0.2869391, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2869391, 0.2043441, 0.2869391, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2869391, 0.2043441, 0.2043441, 0.2043441, 0.2869391, 0.2043441, 0.2869391, 0.2043441, 0.2043441, 0.2043441, 0.2869391, 0.2869391, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2869391, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2869391, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2869391, 0.2869391, 0.2869391, 0.2869391, 0.2043441, 0.2043441, 0.2043441, 0.2869391, 0.2869391, 0.2869391, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2869391, 0.2043441, 0.2043441, 0.2043441, 0.2869391, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2869391, 0.2869391, 0.2043441, 0.2043441, 0.2869391, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2869391, 0.2869391, 0.2869391, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2869391, 0.2869391, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2869391, 0.2869391, 0.2869391, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2869391, 0.2869391, 0.2869391, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2869391, 0.2869391, 0.2043441, 0.2043441, 0.2043441, 0.2869391, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2869391, 0.2869391, 0.2043441, 0.2043441, 0.2043441, 0.2869391, 0.2869391, 0.2043441, 0.2869391, 0.2869391, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2869391, 0.2869391, 0.2869391, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2869391, 0.2869391, 0.2869391, 0.2869391, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2869391, 0.2869391, 0.2869391, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2869391, 0.2869391, 0.2869391, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2869391, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2869391, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.2043441, 0.49128324, 0.49128324, 0.49128324, 0.49128324, 0.5101458, 0.44869962, 0.6716205, 0.6710489, 0.67219204, 0.6707631, 0.6710489, 0.66504717, 0.6710489, 0.6716205, 0.67219204, 0.67219204, 0.67219204, 0.67219204, 0.67219204, 0.67219204, 0.67219204, 0.67219204, 0.67219204, 0.67219204, 0.67219204, 0.67219204, 0.67219204, 0.67219204, 0.67219204, 0.67219204, 0.67219204, 0.67219204, 0.67219204, 0.67219204, 0.67219204, 0.67219204, 0.67219204, 0.67219204, 0.67219204, 0.67219204, 0.67219204, 0.67219204, 0.67219204, 0.67219204, 0.67219204, 0.67219204, 0.67219204, 0.67219204, 0.67219204, 0.67219204, 0.67219204, 0.67219204, 0.67219204, 0.67219204, 0.67219204, 0.67219204, 0.67219204, 0.67219204, 0.67219204, 0.67219204, 0.67219204, 0.67219204, 0.67219204, 0.67219204, 0.67219204, 0.67219204, 0.67219204, 0.67219204, 0.67219204, 0.67219204, 0.67219204, 0.67219204, 0.67219204, 0.67219204, 0.67219204, 0.67219204, 0.67219204, 0.67219204, 0.67219204, 0.67219204, 0.67219204, 0.67219204, 0.67219204, 0.67219204, 0.67219204, 0.67219204, 0.67219204, 0.67219204, 0.67219204, 0.67219204, 0.67219204, 0.67219204, 0.67219204, 0.67219204, 0.67219204, 0.67219204, 0.67219204, 0.67219204, 0.67219204, 0.67219204, 0.67219204, 0.67219204, 0.67219204, 0.67219204, 0.67219204, 0.67219204, 0.67219204, 0.67219204, 0.67219204, 0.6710489, 0.6687625, 0.844527, 0.7893684, 0.7965133, 0.6630466, 0.8036582, 0.6747642, 0.83452415, 0.8462418, 0.844527, 0.8442412, 0.8442412, 0.8442412, 0.8448128, 0.8382395, 0.81337523, 0.8525293, 0.82023436, 0.8525293, 0.95598745, 0.98685336, 0.98685336, 0.970563, 0.9394113, 0.9365533, 0.9768505, 0.8525293, 0.90483, 0.9368391, 0.9714204, 0.9948557, 0.99657047, 0.9948557, 0.9991426, 0.9922835, 0.9862818, 0.9977136, 0.9979994, 0.9991426, 0.9994284, 0.99657047, 0.9994284, 0.99828523, 0.99828523, 0.9994284, 0.9994284, 0.9994284, 0.9994284, 0.99857104, 0.9945699, 0.9991426, 0.9997142, 0.9991426, 0.9977136, 0.9774221, 0.8962561, 0.9051157, 0.99828523, 0.9939983, 0.9785653, 0.9945699, 0.9922835, 0.9979994, 0.99628466, 0.9994284, 0.9991426, 0.9974278, 0.9994284, 0.9997142, 0.9994284, 0.9994284, 0.9997142, 0.9994284, 0.9994284, 0.9994284, 0.9997142, 0.9997142, 0.9991426, 0.9994284, 0.9994284, 0.9994284, 0.9994284, 0.9994284, 0.9997142, 0.99571306, 0.9991426, 0.9994284, 0.9979994, 0.9997142, 0.9997142, 0.9997142, 0.9997142, 0.9997142, 0.9977136, 1.0, 0.9994284, 0.9994284, 0.9997142, 1.0, 1.0, 1.0, 1.0, 0.9997142, 1.0, 1.0, 0.9997142, 0.9991426, 1.0, 0.9994284, 0.9991426, 0.9994284, 0.9979994, 0.99885684, 1.0, 1.0, 0.9991426, 0.9991426, 0.9994284, 1.0, 0.9991426, 0.9997142, 0.99857104, 0.9997142, 0.9997142, 0.9997142, 0.9997142, 0.9994284, 0.99657047, 0.9974278, 0.9911403, 0.9997142, 0.99885684, 0.9997142, 0.9968563, 0.9994284, 0.9997142, 0.997142, 1.0, 0.99857104, 1.0, 1.0, 1.0, 1.0, 0.9994284, 1.0, 0.9994284, 0.9997142, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99857104, 1.0, 0.9997142, 1.0, 0.9997142, 0.9997142, 0.99828523, 0.9994284, 1.0, 1.0, 0.9997142, 1.0, 1.0, 0.9997142, 0.9934267, 0.9994284, 0.9997142, 0.9997142, 1.0, 0.99885684, 0.99657047, 0.92712206, 0.8528151, 0.9791369, 0.98942554, 0.99885684, 0.9882824, 0.99657047, 0.9994284, 0.99571306, 0.99542725, 0.9991426, 0.9994284, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99828523, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9997142, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9968563, 0.9722778, 0.98913974, 0.92569304, 0.9908545, 0.9968563, 0.99885684, 1.0, 0.9997142, 0.9997142, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9997142, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "print(test_accuracies)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
